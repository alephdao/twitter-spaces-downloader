

--- Segment 1 (Starting at 00:00) ---

[0:00 - 0:05] (Silence)
[0:05] Speaker 1: Alright.
[0:06 - 0:09] (Silence)
[0:09] Speaker 1: Um, you know, there are now like five,
[0:21 - 0:24] (Silence)
[0:42 - 0:47] (Silence)
[0:47] Speaker 1: Okay.
[0:53 - 0:57] (Silence)
[0:57] Speaker 1: I'm putting out an experience on that.
[1:40 - 1:45] (Silence)
[1:52 - 2:05] (Silence)
[2:17 - 2:23] (Silence)
[2:28] Speaker 1: water and thing?
[4:03 - 4:24] (Silence)
[4:24] Speaker 1: Oh, uh, now it's a good time to grab a drink, right?
[4:33 - 4:53] (Silence)
[4:59 - 5:04] (Silence)
[5:40 - 5:46] (Silence)

--- Segment 2 (Starting at 10:00) ---

[10:00] Speaker 1: Benji.
[10:01] Speaker 2: I mean, I'll, I'll move out to the center, but I'll, I'll be kind of glued here.
[10:05] Speaker 1: Here.
[10:05] Speaker 2: Let's see, this, I don't think, have you tested it with a phone?
[10:09] Speaker 1: Okay.
[10:09] Speaker 2: USB C?
[10:11] Speaker 1: Yep.
[10:14] Speaker 2: Okay, it'll show up there once it connects, if it connects.
[10:19] Speaker 1: I can run down and get another dongle.
[10:21] Speaker 2: Okay.
[10:21] Speaker 1: Oh, I saw it.
[10:22] Speaker 2: Oh, you saw it?
[10:24] Speaker 1: Do you have to configure it somewhere in here?
[10:31] Speaker 2: Is it showing up?
[10:32] Speaker 1: Says dock connector.
[10:33] Speaker 2: Dock connector. So maybe let's try to mirror.
[10:35] Speaker 1: Oh, there it is.
[10:36] Speaker 2: Okay.
[10:36] Speaker 1: Oh, sweet.
[10:38] Speaker 2: Cool.
[10:39] Speaker 1: No audio, right?
[10:41] Speaker 2: No audio.
[10:42] Speaker 1: Okay.
[10:42] Speaker 2: You're gonna be glued here stuck with the phone, right?
[10:44] Speaker 1: So, you're gonna.
[10:44] Speaker 2: Uh, can I use this?
[10:45] Speaker 1: Yeah, yep.
[10:45] Speaker 2: Okay.
[10:46] Speaker 1: Cool.
[10:47] Speaker 2: I'll use this one.
[10:49] Speaker 1: Okay.
[10:49] Speaker 2: So you're gonna use this but then roam the stage.
[10:52] Speaker 1: Maybe a little bit.
[10:53] Speaker 2: Yeah, as much as I can.
[10:53] Speaker 1: You can't move the.
[10:54] Speaker 2: I don't want this to get disconnected.
[10:55] Speaker 1: Right.
[10:56] Speaker 2: Yeah, I'll be careful.
[10:57] Speaker 1: Can you leave it on?
[10:57] Speaker 2: I just feel safer that's not gonna.
[10:58] Speaker 1: Yeah, all right.
[10:59] Speaker 2: Just leave it on the.
[11:00] Speaker 1: Yeah.
[11:00] Speaker 2: Okay.
[11:01] Speaker 1: Cool.
[11:01] Speaker 2: Yeah, you want to use this one?
[11:02] Speaker 1: Cool.
[11:02] Speaker 2: Just grab it and do it here.
[11:03] Speaker 1: Okay.
[11:03] Speaker 2: Okay, thanks Benji.
[11:04] Speaker 1: Right now?
[11:05] Speaker 2: Yeah, yeah.

[11:20 - 11:26] (Silence)

[11:26] Speaker 1: Benji, also just, just gonna have a tell everybody.
[11:27] Speaker 2: Yeah.
[11:27] Speaker 1: Right.
[11:27] Speaker 2: You weren't going to test it on the screen or anything?
[11:28] Speaker 1: Yeah.
[11:29] Speaker 2: Okay, great.
[11:29] Speaker 1: I'm just having this old, holding slide.
[11:31] Speaker 2: Yeah, yep.
[11:35] Speaker 1: Uh, sure.
[11:36] Speaker 2: Cool.
[11:36] Speaker 1: So we're good.
[11:38] Speaker 2: Thanks.
[11:38] Speaker 1: Thanks.
[11:38] Speaker 2: Yep.

[11:45 - 11:50] (Silence)

[11:50] Speaker 2: All right.
[11:51] Speaker 1: Uh can you hear me?
[11:53] Speaker 2: You're good.
[11:53] Speaker 1: Awesome.
[11:54] Speaker 2: Uh welcome everybody to Gauntlet AI demo day.
[11:57] Speaker 1: I know there's still a bunch of people, uh, excuse me, so a bunch of people coming in, but, uh, we are so excited to have you all and those of you that are still joining and those of you that are, uh, on the live stream.
[12:09] Speaker 2: Hello to you too.
[12:09] Speaker 1: Um, we are very, very, very excited to show you some of what we've got today.
[12:15] Speaker 2: Um, for those of you that don't know what Gauntlet AI is, uh, we're a 12 week program for engineers where essentially on the outside, what we do is take very smart people.
[12:28] Speaker 1: Um, and week one, we shock the system by having them build apps, but not allowing them to write any code.
[12:37] Speaker 2: So our entire goal at Gauntlet AI is to get everybody on the cutting edge of AI, uh, because that slope is pretty vertical.
[12:47] Speaker 1: We think, uh, and I'll just say for, for myself, when, when we started Gauntlet AI, on day one, I was pretty excited about the potential of what could happen if you have people building with AI, uh, trying to adopt the newest models, the newest generative AI, everything as fast as you can.
[13:12] Speaker 2: But I've been pretty blown away and I think that what you will all see in a couple of different ways today is pretty wild as well.
[13:20] Speaker 1: So, uh, to give you an idea, nobody had ever heard of Gauntlet AI about four months ago.
[13:29] Speaker 2: Every single student that you'll see today, uh, could have possibly heard about Gauntlet AI three weeks or three months in about two weeks ago.
[13:40] Speaker 1: Um, so pretty much everybody who is here saw that this was going to happen, dropped everything in their lives, jumped right on board and two weeks later they were enrolled and building and uh, doing all of that as fast as they can.
[13:55] Speaker 2: Um, we're very selective.
[13:57] Speaker 1: Uh, we picked about two or 3% of the applicants.
[14:04] Speaker 2: Um, and then Gauntlet AI, the name comes from our somewhat sinker swim environment.
[14:11] Speaker 1: So every week there's a new challenge.
[14:13] Speaker 2: If anybody doesn't make it acceptably through that week, um, and I set those standards, uh, then you don't move on to the next week.
[14:23] Speaker 1: So the people that you see today, they made it all the way through the Gauntlet and that's pretty exciting for us.
[14:30] Speaker 2: Um, but I want to zoom out just a little bit and talk about why any of this exists in the first place.
[14:37] Speaker 1: Um, there are a lot of different training programs out there, there's a lot of different ways that you can learn different things.
[14:44] Speaker 2: Um, but we have a couple singular focuses in mind, some of which are not obvious, uh, from the outside world.
[14:55] Speaker 1: So the first is, for me, transparently, uh, I want to build a credential that is a stronger hiring signal than anything like a Stanford CS degree or an MIT CS degree.
[15:09] Speaker 2: That's our, that's my main overarching objective.
[15:14] Speaker 1: Now, how do we get there?
[15:15] Speaker 2: That's going to be a long journey.
[15:17] Speaker 1: Um, there's a lot that goes into it.
[15:19] Speaker 2: Um, but for these students, what you're going to see is that they can build things really, that they can utilize the cutting edge techniques in AI.
[15:31] Speaker 1: They all have very unique workflows.
[15:33] Speaker 2: They all have, you know, they've been building, uh, a lot.
[15:37] Speaker 1: You will see that they are very smart.
[15:39] Speaker 2: Uh, we select for that very intentionally and you'll see that they are very hardworking.
[15:46] Speaker 1: Um, the generally speaking, uh, people show up in this office here at 8:00 a.m., uh, 8:00 a.m. Central every day and they're here until midnight, seven days a week, the entire time.
[16:02] Speaker 2: Uh, Sunday starts a little bit later for church, but that's the only, uh, only exception that we've got.
[16:08] Speaker 1: Um, so people have been building 80, 100 plus hours a week and, you know, people being really smart and working really hard, creates a little bit of magic.
[16:20] Speaker 2: But I think perhaps the unspoken thing that all of the the Gauntletters or the gauntlet students or the challengers or whatever everybody has decided to call them, um, the unspoken part of that that I think is the most important is how quickly they learn.
[16:41] Speaker 1: Everybody who is in Gauntlet AI is spending hours and hours each week, learning about and keeping a record of the new things that they're encountering.
[16:53] Speaker 2: They're figuring out how to jump into new spaces.
[16:57] Speaker 1: Um, they're building workflows for learning.
[17:01] Speaker 2: They are, uh, rapidly acquiring expertise in whatever they're building, um, and they're staying on the cutting edge of AI.
[17:10] Speaker 1: Our our thesis is that the the rate of change of AI is basically going vertical.

--- Segment 3 (Starting at 20:00) ---

[20:00] Speaker 1: Um, we're not necessarily, you know, pioneering new methods of AI. We're not doing, you know, fundamental research. Our goal is just to get to the edge of what AI is capable of and ride that wave.
Um, we think doing that is possibly the biggest force multiplier in human history.

And even versus what I expected, it's it is very, very, very powerful.
Um, I've been running schools, code schools for a very long time. Uh, now what I what I see, when I see a project, I have a, you know, kind of background mental process running of approximately how long I think it would take to to do any project, right?
Devil's always in the details, but um, there have been a number of companies that have come in and worked with us where their mental model says that something should take six months, maybe a year, and Gauntlet Challengers are knocking it out in literally 48 hours, 72 hours.

I know that seems crazy and people are going to keep calling me on it and I'm going to keep showing evidence of that.
Um, but roughly, you know, now I have to tweak my thinking so that what I'm used to anticipating taking about a month, uh, is somewhere between 24 and 48 hours.

And we see that regularly.
Um, as one piece of evidence to that effect, um, we don't have enough room on this floor for all of the students to be here.
Um, so I posted a tweet or a post or a Z on X, um, about half an hour ago and the students who are not up here presenting are downstairs on our other floor, and they're going to be building.

Um, and they're going to be building whatever they find the most impressive from the replies to that tweet.
So from right now, they have about five hours to build whatever it is that you're requesting and they're going to come back in five hours and show us all what they've built.
Um, this is a demo day in a couple different ways.
Um, the other thing that they're going to do today and the bulk of where our time will be spent is walk us through the demos of what they've built.
Now, they've been at Gauntlet for about three months, a little less than three months.
Um, the first month is online, the last couple months have been here in person in Austin.
Um, but they started their projects that they're going to show to you about three weeks ago, and most of them had two or three projects running simultaneously.
So when you see something up here, adjust your mental model for that's taken them basically a couple of weeks to build.
Um, it's pretty, pretty wild.
Um, so we're very excited to have you all here.
Uh, for those that are joining the stream, we're excited to uh have you all wherever you are virtually.
Um, and I just want to say thank you to all of the the companies who've made this possible.
Um, I mean, millions and millions of dollars went into making this happen.
Uh, the the students pay nothing.
Uh, the students were all flown here from all over the world.
Uh, they're put up in housing, they have offices, we have buses that run to and from the apartments, food, all sorts of events.
Um, that's only made possible by the hiring companies that hire these folks and fund this all.
So thank you to the hiring companies.
Thank you to the sponsoring companies.
Um, and thank you to the the Challengers.
Um, I think it's a little bit understated how crazy it is for some some people, and especially, you know, nobody had heard of this program three weeks before it started.
We hadn't announced it.
It wasn't possible to have heard about it.
Um, from when we announced it, there are a couple hundred people in those, you know, two or three weeks, who dropped everything, decided to take a risk, uh, decided that it was worth going a hundred hours a week after something, um, at a pace that's mind-boggling and sometimes unsustainable.
Um, but that's, that's not an easy thing to do.
They put a lot of uh hope and a lot of trust in me, um, in the companies that are hiring, and uh they they took a big risk and worked extremely hard.
So, um, I want to say thank you to all those of you who who did that and who did this crazy thing.
And I think all of us to a person are very, very pleased with what the results have been.
Um, so with that, I am going to uh a couple items of business.
First is, uh you can find my tweet.
I've pinned it.
Um, I'm Austin on X.
AUSTEN is my username.
Um, so please reply to that with recommendations or suggestions or requests for what the Challengers downstairs are going to build, and they're going to show you what they can build in five hours.
Um, and now I'm really, really excited to get to the demos.
Uh, so I'm going to have Ash, who's one of our program managers here and's been working with the the Challengers day in and day out.
Uh, he's going to go over some logistics and introduce you to some stuff that in very Gauntlet nature, uh, was built in the last 24 hours or so, um, to to make it a little bit easier to follow along.
So, uh, Ash.

[29:22] Speaker 2: All right.
Hey guys, and welcome again to the 1,000 people watching us on X right now.
So I just want to go through some of the logistical details and some of the things that we have available to you to sort of meet our graduates and understand what they've created.
So at demod day.gauntletai.com, you'll be able to create an account, and you'll be able to see each of the graduates and all the projects that they have.
At the same time, as the graduates get up here, you'll be able to see who's presenting.
Each of the graduates are going to be presenting for three to four minutes.
They'll be going through their applications and then walking through a little bit of their AI first coding framework.
They've been preparing for this for about two to three days now, and the goal of these presentations is to make sure that they can showcase what they can build in just a few weeks, but at the same time, how quickly they can build with AI and build a viable product that they can launch.
So you'll see some people trying to uh launch a product and get users.
You'll see some people showcasing a product that they've built super quickly, and uh how quickly they're able to build with AI and add to the sort of ecosystem and domain.
So, without further ado, we're going to get started with our first Gauntlet Challenger, and we'll be going one by one throughout the entire day.
And as we go through these different presentations, if you want to rate the presentation, you can do so on the website, and if you have any comments or suggestions or feedback for the presenter, you can leave them there as well.
So I want to welcome our first Challenger, Benjamin Visik.

[31:25] Speaker 3: All right.
Good morning, everybody.
Just make sure we're getting hooked up to the monitor here.
All right, we're good to go.
All right, good morning, everybody.
Uh, this is a special day.
You're going to see a lot of cool presentations, you'll meet a lot of great people, and you'll probably forget most of it.
And I would argue that most days something special happens.
Something you want to remember, and these days are even more difficult to remember.
We have some solutions for this.
Maybe people write things down in a notebook, which is very difficult to query.
Maybe you use Apple notes or voice memos, which all have their own drawbacks.
They're built for a lot of things, but not one specific thing.
So my name is Benji, and today I'll be presenting Momo, where we reimagine voice memos from the ground up with AI.
I've got my app set to the action button, so you can open it anytime, and with one tap, you can start recording.
We have some tags up here.
In your settings, you can decide which tags show up on the home screen from somebody you met, a great idea you had, whether it happened now or maybe a dream that you just woke up from.
And of course, our app is localized in Japanese.
But we'll keep it in English for this audience.
And uh let's record something.
Presenting right now in front of a beautiful group of people.
One thing about voice memos is Apple's audio transcription is not so great.
So we use Whisper AI, Whisper API, which allows you to transcribe in any language, and using AI, it's way more effective.
And so let's check it out.
We'll make this a little bigger.
And as you can see, I've already been recording what I've been doing all day, tagging it.
And if I want to go to the filter, we can see who I met.
Awesome.
I realized Nathan was from Tampa, where I used to live.
And uh can't remember where I was talking to Nathan.
Perfect.
We can see it was right here in the office, of course.
So some of us will have a lot of our moments in the same place.
We go to the office every day or we're at home.
But imagine you're

--- Segment 4 (Starting at 30:00) ---

[30:00] Speaker 1: you're traveling. Imagine you're on a business trip.
You want to remember where you were at and when you were at.
The beauty of Momo is that it takes where you were and when you were, so you can always remember everything about that moment.
And the best part is, it's all done in iCloud, which means I can't see your data, uh if you believe Apple, they can't see your data. So it's completely private and with one tap, you can remember everything you've been doing all day as soon as you want to.
So, thank you. I'm Benji. This was Momo and uh thanks for coming today.

[31:17 - 31:35] (Music)

[31:35] Speaker 2: All right. Next up, we have the team Tinyvec. We have Tyler, Matt, and Spencer. Now, Tinyvec has become a popular library across the entire cohort to power local vector databases and RAG based solutions. That's welcome team Tinyvec.

[32:15 - 32:26] (Music)

[32:45] Speaker 3: Hi, my name is Tyler. This is Matt, and this is Spencer.
I set out to create Tinyvec, an open source vector database that doesn't suck.
Vector databases power the core of almost every single AI application you see out there today to provide relevant context to bridge the gap of LLMs and their training data sets.
Tinyvec is unique in the fact that it does not require an internet connection or large server setup to run.
It's loved by developers, especially here at Gauntlet. It's extremely fast and simple to use.

So, why should you use Tinyvec?
As you can see from this bar graph, it's up to five times faster than other vector databases available today.
We think that any serious AI application is going to be required to use Tinyvec.
It offers the fastest search time, the lowest latency, therefore, your customers are going to have the best end user experience possible.

Tinyvec runs on any device, no matter how constrained the resources are.
It can run on iPhones, tablets, desktop computers. Anything that can use a file, Tinyvec can run.
While Tinyvec has only been out for a couple of weeks now, we've already seen some great market adoption with about 3,000 downloads in the Nodejs and Python ecosystem.
This tells us that developers and AI application owners love incorporating Tinyvec into their AI applications.
In fact, here at Gauntlet Demo Day, you're going to be seeing a handful of applications that are using Tinyvec to supercharge their AI applications.
Now, I want to pass it to Matt, who's going to talk about some of the awesome innovations we've been able to see for mobile applications.

[34:51] Speaker 4: Open source models are increasingly competitive with closed source ones.
There is no moat.
The key will be user experience and personalization.
And an important part of that is privacy.

Especially given recent regulations that restrict the movement of user data across borders.
Tinyvec solves these problems.
It runs anywhere, it's so fast, the resource requirements are tiny, and the user's data never needs to leave their device.

Existing local vector databases are slow, complicated to use, and resource intensive.
Tinyvec on iPhone is 20 times faster than the leading alternatives.
Sorry.
Tinyvec runs anywhere.

But sometimes, Tinyvec is the leading option for applications that want to use AI on device, but sometimes you need to expand beyond the user's device, which is why today we're excited to announce that Tinyvec is expanding to the cloud.

[35:57] Speaker 5: Tinyvec cloud is the Vercel for AI deployment for the next million vibe coders. Uh it's production scale with zero hassle, very simple pricing. And essentially, we want to take your and development environment and very quickly make it available on the cloud. With a single line of code change, following the same Tinyvec API, we want to keep things as easy as possible. You set your public URI, badabing badaboom, you have your vector search working for you. To move from Tinyvec, we've been developing locally. It's as easy as drag and dropping a couple files. This means you have production vector search in a matter of seconds.
This completely wipes out a lot of the competitors in the space and with a full platform like this, we're able to do some really neat things. For both technical and non-technical people. We have this clustering and visualization, so you can see what embeddings are closer to each other, where your documents lie on the grand scheme of the black box that is um AI.
And so this pricing advantage is absolutely insane. A lot of these vector databases will charge you for nickel and dime you left, right and center. Tinyvec uh-uh. It's very lean under the hood, so the pricing is very lean on top of it. So up to 10 times cheaper and this is where Tinyvec can go. I mean there's millions of ideas where to take an incredible idea and scale it even further. Um over time, the goal is to become a cornerstone in AI development, especially for this new age of how quickly we need to continue to build things with how great of an API Tinyvec really is.
So that's Tinyvec. I uh appreciate you all coming out. Uh this is Tyler, Matt, and I'm Spencer.

[37:47 - 37:48] (Applause)
[37:49 - 37:50] (Silence)
[37:50] Speaker 2: I'll get this shit up.

[37:54] Speaker 6: Next up, we have a challenger who is using AI assistant to see if we can change radio signals coming across from different stations across Austin. We have Stephen Newman and Gig Valm.

[38:13 - 38:16] (Silence)

[39:24] Speaker 7: Okay, we're ready to go.
So I'm actually working on uh a gifted and talented education platform that does incorporate software defined radio as one of its modules.
Gifted and talented education is probably one of the most important parts of our society but it often gets overlooked. Countries like China, Singapore and Finland, they don't overlook it. They're pouring a lot of funding into these programs and we should probably follow suit if we don't want to get left behind.
So, these kids are capable of learning new technologies. Not just uh a curriculum that's advanced, but exposing them to advanced technology as well.
So, what I've done here is made available some modules, learning modules in radio technology, quantum computing, material science, computer security, robotics, sensors and circuits. And these are available for rental for individual students or entire classrooms.
And the cool thing about them is that they don't require an instructor. They're all AI guided, first principles learning from the very ground up. And here's a an example of the first module here, which is the software defined radio module. And you'll see we've got our signals here on the left and on the right is integrated uh an AI tutor that will walk the students through everything from learning what a a wave is, what a radio wave is and building all the way up to intercepting satellite signals, doing general signals analysis and even launching their own radio station, choosing their own programming and music. So not only is it very educational but that's a lot of fun. This is the kind of thing kids are going to want to participate in. It'll be available for middle school students, high school students and the AI tutor automatically works with whatever level they're already at. So doesn't doesn't get boring, they jump right in.
And I've got a QR code right here, so if anyone knows any students who want to launch their own radio station, then go ahead and uh we'll put them on the list and have a great time. Okay, thank you.

[41:56] (Applause)

--- Segment 5 (Starting at 40:00) ---

[40:00 - 40:04] (Silence)
[40:04] Speaker 1: All right, next up, we have AI powered video generation with splify.me, Scott, Chris and Kevin.

[40:21 - 40:25] (Silence)
[40:25] Speaker 2: All right, how's everyone doing?

[40:28] Speaker 2: Perfect. All right, I'll start while they're getting set up.
[40:31] Speaker 2: Hi everyone.
[40:32] Speaker 2: My name is Scott Gubau.
[40:33] Speaker 2: I'm here with Chris Vitich and Kevin Ma.
[40:37] Speaker 2: We are focusing on short form video content.
[40:40] Speaker 2: So short form video content is a $100 billion dollar industry.
[40:46] Speaker 2: And as this industry has grown, several companies have made video editors specialized for short form content creators.
[40:53] Speaker 2: Uh, but these editors are often difficult to learn, expensive, and quite frankly boring to use.
[40:59] Speaker 2: Um the truth is most short form video content creators aren't trying to make, you know, this super high polished content.
[41:06] Speaker 2: They're trying to make funny, ridiculous, viral memes to share with their friends.
[41:12] Speaker 2: So that's why we created, hopefully they'll get this plugged in a second so we can do the big reveal.

[41:24] Speaker 2: All right, well that's why we created splify.me, a fun, ridiculous, and ridiculously fun and ridiculously easy to use viral content generator.
[41:38] Speaker 2: So we're going to do a live demo today.
[41:40] Speaker 2: So this is the current uh landing screen.
[41:44] Speaker 2: And when you the user logs in, they're taken to a dashboard where they can navigate to the different features of the website.
[41:53] Speaker 2: Now, uh the main feature is the video maker, which Chris will walk through in a second, but they can also access their projects, the media library, which contains a library of commonly used memes that anyone can access and like quickly load into their videos, as well as they can upload their own media.
[42:09] Speaker 2: So now Chris is going to do a walk through of the video maker.
[42:13] Speaker 3: Awesome.
[42:14] Speaker 3: Thanks Scott.
[42:15] Speaker 3: I'll walk through some of the video maker uh capabilities.
[42:19] Speaker 3: We try to emphasize simplicity over complexity uh for this for this uh feature right here.
[42:24] Speaker 3: So I can actually put in an existing uh media resource.
[42:28] Speaker 3: We'll have this video uh explaining mitochondria.
[42:33] Speaker 3: Uh we have a few different options.
[42:35] Speaker 3: We can use modify the video format as well as the split screen uh that's sort of common in a lot of meme formats you might see nowadays.
[42:42] Speaker 3: So we'll actually try that.
[42:43] Speaker 3: We'll put in a popular uh Subway Surfers uh clip.
[42:47] Speaker 3: We have the ability to control the simultaneous audio.
[42:52] Speaker 3: Uh there's YouTube to MP3 transcript, uh but we'll actually do our own text to speech transcript using some of the compelling voices that we have actually available on the application.
[43:01] Speaker 3: So we'll try Donald Trump uh just to try to create a compelling video.
[43:06] Speaker 3: This is a pretty quick process.
[43:08] Speaker 3: We're using Spechify to create the MP3 file and then opening AI whisper to transcribe the VTT.
[43:15] Speaker 3: So we'll get the audio clip.
[43:17] Speaker 3: Uh we'll add that to the video and then we'll add some subtitles as well.
[43:23] Speaker 3: So we'll try the full uh feature right here.
[43:26] Speaker 3: the volume.

[43:30] Speaker 4: So the basic unit of all.
[43:32] Speaker 3: by.
[43:33] Speaker 3: I'll restart just to emphasize.
[43:35] Speaker 4: have a double membrane structure and use aerobic respiration to generate adenosine triphosphate.
[43:40] Speaker 4: Here is a sign.
[43:41] Speaker 3: So you can hear the simultaneous audio as well as the newly generated text to speech.
[43:45] Speaker 3: We have a large number of other features that we want to implement or we couldn't present uh today including AI clipping um and some other image overlays and dynamic AI image overlays as well.
[43:55] Speaker 3: Uh but I'll uh lead this off to Kevin who'll explain our marketing strategy and approach.
[44:01] Speaker 5: Thanks Chris.
[44:02] Speaker 5: So we have a weight list live at splify.me on the internet and we're planning on launching within the next few weeks.
[44:08] Speaker 5: And so during the next few weeks, we're going to be using our platform as our only uh marketing tool.
[44:13] Speaker 5: So we're going to be generating all our own or advertisement using our tool and then uh posting it to the internet and then we'll be launching soon.
[44:21] Speaker 5: And then we're going to be launching with two tiers, uh one core plan with the uh core functionality that you've seen here today.
[44:27] Speaker 5: It's going to be $9.99 a month.
[44:29] Speaker 5: and we're also going to have an additional premium tier with access to video generation models um an iOS app, uh automatic script writing and a lot of other features.
[44:39] Speaker 5: So uh sign up for the weight list now at splify.me and get slopping.

[44:56 - 45:00] (Silence)
[45:00] Speaker 1: All right.
[45:01] Speaker 1: Next up, we have CCO Vibe with Roger, Pat, Roger, Pat, Adam, and Rosston.

[45:30 - 45:34] (Silence)
[45:34] Speaker 6: Okay.
[45:34] Speaker 6: Okay, we are CCO Vibe.
[45:36] Speaker 6: Um our website is CCOvibe.com and you can see our presentation by clicking right here.
[45:43] Speaker 6: Um we are a platform to uh train and uh validate CCO Viber because the future of coding and development is going to be with Viber.
[45:57] Speaker 6: Viber.
[45:58] Speaker 6: Viber.
[45:59] Speaker 6: Viber.
[46:00] Speaker 6: Relax.
[46:01] Speaker 6: Relax.
[46:02] Speaker 6: I'll take over from here.
[46:03] Speaker 7: Earlier this past Monday, we were on a call with a potential client.
[46:09] Speaker 7: For us, selling software is second nature.
[46:13] Speaker 7: We've done it for years.
[46:17] Speaker 7: Confidently, we told him, look, what used to take us three months, we can now achieve as AI engineers in just one week.
[46:28] Speaker 7: Initially, it seemed like he believed in us, but soon we sensed hesitation and skepticism.
[46:36] Speaker 7: The problem was clear.
[46:37] Speaker 7: He didn't trust that high-quality production grade software could really be delivered in just one week.
[46:45] Speaker 7: And honestly, we get it.
[46:48] Speaker 7: From the outside, this speed sounds almost unbelievable.
[46:54] Speaker 7: Well, it's clear to us inside our bubble, the wider world remains skeptical.
[46:59] Speaker 7: This is our big challenge, selling AI first engineering to a market accustomed to traditional timelines.
[47:08] Speaker 7: How do we bridge that gap?
[47:10] Speaker 7: That's exactly why we built CCO Vibe.
[47:14] Speaker 6: Um users can come to our platform and they go through a series of learning paths which are then reviewed by uh their peers.
[47:25] Speaker 6: So that they can then go to folks out in the world who don't necessarily trust vibe coders and say, look, I've built all these things and we're validated by my local peers.
[47:34] Speaker 6: Um they can also make pull requests to the platform because it's open source.
[47:38] Speaker 6: So with that package, they can then go on and say to people like, look, I've built all this stuff.
[47:43] Speaker 6: You can see it, it's great.
[47:45] Speaker 6: Um you can see it all on GitHub and then they become part of this community, which then helps them continue to sell.
[47:52] Speaker 8: We also built a mobile app, which acts as a virtual water cooler environment for all vibe coders.
[47:58] Speaker 8: It's a place where you can show off your work, get feedback, chat with other developers and manage all of your projects.
[48:08] Speaker 8: It also acts as a central marketplace for all developers.
[48:13] Speaker 8: There are a lot of executives out there uh with lazy engineering teams reporting to them who are excited by this new wave of vibe coders who can build things quickly and they're jealous.
[48:26] Speaker 8: They have big backlogs of work they want to get done and they're looking for new avenues to distribute it.
[48:33] Speaker 8: We want our marketplace to be that distribution channel.
[48:37] Speaker 8: Users can log in, view the leads in the platform and pick up work based on their personal expertise.
[48:46] Speaker 8: They can also update their master checklist in the mobile app.
[48:50] Speaker 8: So whether you're vibing at your desk or you're out touching grass, you can keep all of your projects up to date all the time.
[48:58] Speaker 9: And so now that I've received validation from the community, I'm starting to get clients.
[49:02] Speaker 9: I now need to start projects for my clients.
[49:04] Speaker 9: We've integrated GitHub Oauth.
[49:07] Speaker 9: So with one click, you can import your current project or start a new one.
[49:12] Speaker 9: We have an agent which scans your GitHub repository and creates all your documentation from your project requirements stock to your universal checklist based off your current architecture and setup.
[49:22] Speaker 9: To make sure you spend your time vibing rather than having meetings with clients and stakeholders, we've implemented what we call a CCO, Chief cognitive officer, which loads your platform experiences, learning paths and spiky POVs into a LLM.
[49:39] Speaker 9: Clients can

--- Segment 6 (Starting at 50:00) ---

[50:00] Speaker 1: and prospects can now or can give your CCO feedback and feature requests, which triggers an update to your project's universal checklist.
[50:14] Speaker 1: This way you keep the vibes high and the stress low, so you can make more contributions to the community.

[50:23] Speaker 1: We've had a lot of contributions already from the Gauntlet IO group including Norman's Monster Hunter, so you can play that on our platform.
[50:30] Speaker 1: We use form locker and AGA Slot Starters.
[50:34] Speaker 1: Uh we're going to be presenting at the Morpheus Agents Builder event uh in Austin on Tuesday April 22nd and you can scan this QR code to register for that to see what we've come up with by then.

[50:55] (Clapping)

[51:08] (Silence)
[51:12] Speaker 2: All right.
[51:13] Speaker 2: Next up, we have Jacob and Robert.
[51:17] Speaker 2: We're going to be talking about Vision BI and their solution for natural language to SQL.
[51:34] (Silence)
[51:58] Speaker 2: All righty guys.
[52:00] Speaker 2: Um welcome today.
[52:02] Speaker 2: We're here to present to you Vision BI.
[52:06] Speaker 2: Um this is meant to be a little bit beyond uh just a text to SQL.
[52:13] Speaker 2: We are hoping to build a fully integrated AI data analyst.
[52:21] Speaker 2: Um some of the problems that we have uh that I've experienced uh businesses are growing more and more uh data sources come from different platforms, the large volume of data that's hard to manage.
[52:38] Speaker 2: Current BI solutions, they take a lot of time to set up.
[52:41] Speaker 2: You've got require massive teams, it requires a lot of infrastructure.
[52:49] Speaker 2: And one of the big shortcomings that a lot of those tools have is they're very descriptive and not very prescriptive.
[52:55] Speaker 2: So honestly that's where I would come in uh my career as a data analyst, you know, you send out a report, you get it back and or I get back all the questions that come in at the end.
[53:08] Speaker 2: Um so what we have kind of architected uh planned some solutions.
[53:14] Speaker 3: I'm sure.
[53:15] Speaker 3: So, one of the great things that we do here at Vision BI is that you can take data from multiple sources, plug it in and we standardize it in a way that we can store the metadata regarding your data sources and then pull what we need whenever we query the AI agent and cache it in memory and then be able to spit out results based on it.
[53:52] Speaker 3: It makes it very intuitive for users who aren't very tech savvy and it basically replaces everything that you would want a data analyst to do.
[53:59] Speaker 3: So if you want to make charts, graphs, reports, etcetera, um just ask what your monthly sales are.
[54:07] Speaker 3: Um you can ask our AI data analyst and it'll pop it out for you.
[54:13] Speaker 2: We'll take you through a quick demo video real quick the little voiceover.
[54:18] Speaker 2: Um kind of walking through the process and how the platform is set up.
[54:26] Speaker 2: Um so as you log in, you're greeted and it's easy enough to add data sources.
[54:31] Speaker 2: Right now we've built out Google Sheets integration.
[54:34] Speaker 2: We've got uh other database and other type of connections as well.
[54:42] Speaker 2: So whenever you connect data into uh into our platform, it's we're trying to make it as seamless and easy as possible so that anybody can do it.
[54:50] Speaker 2: Um and then as you connect your data, we have AI running that will go in, analyze the structure of the data, the types of uh the data types within each columns uh as well as uh a summary of what's in the table, what's in each column.
[55:11] Speaker 2: So we can see here as we inserted our marketing data, we've got it's analyzed each of the columns that are in there to give uh give context to any later uh access to that data.
[55:28] Speaker 2: And then, as we move into the Imagine screen where the users can ask anything of their data,
[55:39] Speaker 3: so we're asking about our sales.
[55:43] Speaker 2: Um we can see as the agents thinking using a variety of tools and it's able to um find out relatively quickly what the data is.
[55:56] Speaker 3: So here we've asked what our top sales month is and quickly it runs through the tools.
[56:01] Speaker 3: It gets this data sources, it returns the tables regarding those data sources and it runs a query against it that spits out the in natural language the data regarding it.
[56:16] Speaker 3: And then here, we've asked the state in which um the most sales occurred, which requires a multi-table join, brings data in from multiple sources and we spit out natural language regarding that.
[56:31] Speaker 3: And next, we'll ask to generate a table as you would a data analyst in any company.
[56:38] Speaker 3: Um, and we can see how that pans out.
[56:41] Speaker 2: There's voice support as well, which I guess you can't really hear over the demo but so in the case where, you know, we have it on mobile or if you're on the go, it's easy enough to uh get anything spit out.
[56:54] Speaker 2: And so we've got charts thrown out immediately in the chat.
[56:59] Speaker 2: Um and we can iterate on that and you can keep asking more and more questions.
[57:04] Speaker 3: So thank you for checking out Vision BI.
[57:07] Speaker 3: Um we've got a QR code that you can scan to go to our website.
[57:13] Speaker 2: Well, you get the whole, the whole Google sheet, yeah.
[57:15] Speaker 2: So we've got uh more information available online at Vision BI.
[57:19] Speaker 2: You can scan the QR code, um and there's form there to contact us if you have any more questions.
[57:23] Speaker 2: Thank you for your time.
[57:25] Speaker 3: Appreciate that.
[57:25] Speaker 2: Take care.

[57:34] (Silence)
[57:39] Speaker 2: All right, so at Gauntlet AI, we have a tradition of cloning applications with AI and then making them our own and adding our own features.
[57:50] Speaker 2: So next up is Refall, who's taken an app he uses every day, but matched it to his own workflow.
[58:02] (Silence)

[58:17] Speaker 4: All right.
[58:18] Speaker 4: Hello, my name is Rafael.
[58:22] Speaker 4: Um, yeah, we can start.
[58:24] Speaker 4: Um, so at Goly AI, we spend the first part of our uh first half of the program, cloning several great apps in a week and adding increasingly complex AI features.
[58:38] Speaker 4: Um the next week, we cloned Slack, Zendesk and TikTok.
[58:50] Speaker 4: So after over 1,000 hours spent on AI first development, I thought it would be very fitting to close this experience by using cursor to make a clone of cursor.
[59:04] Speaker 4: Let me show you Derser.
[59:08] Speaker 4: It's a uh Visual Studio Code extension with a built-in AI agent to help you write code.
[59:17] Speaker 4: The general workflow is very familiar.
[59:22] Speaker 4: It's uh we have a chat where we can select files for context and type instructions for the agent to follow.
[59:32] Speaker 4: But while it is a cursor clone, I wanted to make two changes to better reflect the workflow I would develop over the past three months.
[59:42] Speaker 4: Um the first is thinking process.
[59:47] Speaker 4: So the prevailing uh sentiment over our cohort is that LLMs work best with really detailed prompts.
[59:56] Speaker 4: Um highlighting where and which uh in which files uh what and which files should be changed, essentially trading cognitive loads spent on research for better code output.
[01:00:10] Speaker 4: We can write this by hand or have a um solution planning step that's completely separate from code generation and that's what happening in Derser.
[01:00:22] Speaker 4: Second is uh checkpointing system.
[01:00:29] Speaker 4: More experienced developers um naturally separate concerns that we're working on to a different LM chats and maintain a relatively clean and reasonable Git history.
[01:00:45] Speaker 4: Derser forces developers to put this problem first by having each chat to be a separate Git branch and ending every agent response with a commit.
[01:00:59] Speaker 4: When all changes related to a single problem are done, user can accept a chat, which is then squashed and merged to a parent branch, making all changes related to a single problem end up in a single commit.
[01:01:14] Speaker 4: Let me show you how it works.
[01:01:23] Speaker 4: First, the uh analysis is done.
[01:01:32] Speaker 4: Then the code is generated, which is which will then be applied to our files.
[01:01:52] Speaker 4: Once the changes are done, Derser checks for errors, then writes a summary and makes a commit.

--- Segment 7 (Starting at 60:00) ---

[61:00] Speaker 1: Let's see it again.
[61:03] Speaker 1: Let me, uh, swipe there, and, um, yeah, so we have a research.
[61:12] Speaker 1: Then we have code changes.
[61:21] Speaker 1: And then they are applied to the files.
[61:25] Speaker 1: Thank you.

[61:32] Speaker 2: All right.
[61:33] Speaker 2: The next challenger thinks that the job search process is completely broken.
[61:38] Speaker 2: So he made an AI to fix it entirely.
[61:41] Speaker 2: Next up is Neils, with Nexus AI.

[62:09 - 62:14] (Silence)
[62:14] Speaker 3: All right.
[62:15] Speaker 3: So I'm not sure if all of you have been looking for a job recently.
[62:19] Speaker 3: In case you haven't, I'm here to remind you that job search sucks.
[62:27] Speaker 3: Okay?
[62:27] Speaker 3: If you need me to be a little bit more specific about that, I can.
[62:35] Speaker 3: So I asked my friends who are currently looking for jobs, and they are complaining that they see exactly the same jobs every day, even though they already decided they don't like them.
[62:49] Speaker 3: And there are hundreds of people applying all for the same jobs.
[62:53] Speaker 3: Everyone is asking for AI skills.
[62:59] Speaker 3: And I needed to see this disaster for myself.
[63:01] Speaker 3: So I went on my LinkedIn, looked at my top recommended job, and it was an unpaid internship, even though I have three years experience in the field.
[63:12] Speaker 3: So clearly there's something wrong there, right?
[63:15] Speaker 3: And I think what's fundamentally wrong is that job boards try to predict your future from your past without knowing what's going on in your head right now.
[63:29] Speaker 3: And the solution for that is an integrated career companion.
[63:37] Speaker 3: So we have AI career coaching, interview coaching, which develops a relationship with the user and grows the context about the user, knows what's in your mind, what plans you have for your career at any given time, and it gives you job recommendations based on that.
[63:59] Speaker 3: So you will only see jobs that you actually want, that are attractive to you.
[64:05] Speaker 3: At the same time, it's also a filter, so there's not 500 people applying to every single job.
[64:13] Speaker 3: Um, so the platform is up and running right now.
[64:17] Speaker 3: And the main elements that we have is the career coach, which is a voice AI agent, which was developed in collaboration with a real career coach, so the scripts are all up to the current standards of career coaching.
[64:36] Speaker 3: Then we have a growing context window that summarizes all the sessions that you've done, all the documents that you've shared.
[64:45] Speaker 3: And then you get job recommendations based on all that context, which I'm basically at this point scraping off the web from hundreds of career pages of major companies.
[65:01] Speaker 3: The cherry on top of that is an application agent that turns any job site on the internet into one click apply.
[65:12] Speaker 3: So once I've taken the job off the career's page, put it into my app, I have this agent running in the back end, because it knows everything about you and it has all your documents, it can just go to the job and apply for you.
[65:24] Speaker 3: And that's it.
[65:26] Speaker 3: It's all in one for basically anything you could want in your career journey.
[65:36] Speaker 3: Um, so there's a couple people working on this right now, but I think I have the best solution on the market, because this is going to scale with intelligence, right?
[65:50] Speaker 3: I'm using AI models at all the crucial parts of my application, and as they get smarter, my application will get better.
[65:59] Speaker 3: It's going to be hyper personalized as well, and I'm owning the context window, right?
[66:06] Speaker 3: The more time you spend with my AI agent, the less likely you're going to be to switch, because I have the data that leads me to give you the best job recommendations.
[66:19] Speaker 3: So why would you?
[66:22] Speaker 3: Um, so the app is live right now, it's at NexusAI.careers.
[66:27] Speaker 3: I have about 80 users signed up right now, and actually my phone just vibrated, which means I got another one.
[66:34] Speaker 3: Um, yeah.
[66:35] Speaker 3: So I would be super excited to to get a lot of feedback.
[66:40] Speaker 3: I'm already getting feedback from users every day.
[66:43] Speaker 3: Um so if you want to sign up, let me know how you what your experience with my agents is, that would be amazing.
[66:51] Speaker 3: Thank you.

[66:57 - 67:06] (Silence)
[67:06] Speaker 2: Now, have we ever traveled to a city, wanted to see the sites, but we didn't know where to go, or the history behind what we were looking at?
[67:13] Speaker 2: Well, the next solution is meant just for that.
[67:16] Speaker 2: We have Michael with audio guide.

[67:23 - 67:30] (Silence)
[67:30 - 67:41] (Silence)
[67:41 - 67:50] (Silence)
[67:50 - 68:02] (Silence)
[68:02] Speaker 4: Hi.
[68:02] Speaker 4: Imagine you are traveling to Rome for your friend's wedding.
[68:07] Speaker 4: You are staying at the Sofitel Hotel and have a few hours to kill before the festivities start at a venue called Masa Rooftop.
[68:16] Speaker 4: You want to explore the city, but didn't do any research beforehand, nor you have time to book a tour.
[68:22] Speaker 4: This app was built for you.
[68:26] Speaker 4: With a simple prompt Sofitel Rome, Masa Rooftop.
[68:38] Speaker 4: We tell you, you say whether you're interested in history and architecture, let's say, and you have two to three hours to tour, and you're willing to walk within two kilometers.
[68:48] Speaker 4: Google Places API will fetch 15 of the best recommendation location for you.
[68:54] Speaker 4: You want to explore Trefontaine, Coliseum, and Pantheon.
[69:01] Speaker 4: We generate the tour and create an itinerary based on what you're wanting to see.
[69:08] Speaker 4: We save the tour and what happens in the background is we fetch Wikipedia data and Wiki Voyage, send it to Open AI for transcript and then generate text to speech.
[69:28] Speaker 4: So here your audio files are generated, uh in three versions, brief overview.
[69:35] Speaker 5: Sound of gentle water flowing.
[69:38] Speaker 5: Welcome to by Giuseppe.
[69:41] Speaker 4: All the way to in depth.
[69:44] Speaker 5: of gentle water horses.
[69:48] Speaker 5: One For every single location you chose.
[70:00] Speaker 4: Um, also you can look at your itinerary in Google Maps, if you so desire.
[70:09] Speaker 4: And a cool feature is you can change your language preference if you'd rather listen to it in French.
[70:15] Speaker 4: Now, all your other files are going to be in French.
[70:19] Speaker 4: Go back to your tour.
[70:23 - 70:27] (Silence)
[70:27] Speaker 4: Start again.

[70:37] Speaker 4: And now it is in French..
[70:40] Speaker 5: coule doucement.
[70:44] Speaker 5: Bienvenue de nouveau Ã .
[70:46] Speaker 4: Another feature we have is a live agent that you can talk to, right now only in English.
[71:03] Speaker 4: Hi, how high is the Trevi fountain?
[71:06] Speaker 5: Ah, you mean the Trevi fountain.
[71:09] Speaker 5: It stands at an impressive 26.3 meters, which is about 86 feet.
[71:15] Speaker 4: Let's do it for the Pantheon as well.
[71:22] Speaker 4: Hi, can you tell me in what year was built the Pantheon?
[71:27] Speaker 5: Ah, the Pantheon.
[71:30] Speaker 5: It's a fascinating structure.
[71:33] Speaker 5: The Pantheon as we see it today was built around 126 AD by Emperor Hadrian.
[71:40] Speaker 5: However, there was an earlier temple on the same site.
[71:43] Speaker 4: So right now, this live is a website and a PWA, so you can have it in your phone.
[71:51] Speaker 4: Um, some features I want to build in the future is offline mode, so if you don't have connectivity in the new local your arts, you can uh download it beforehand.
[72:03] Speaker 4: And I also want to have the live agent in different languages as well.
[72:08] Speaker 4: Thank you.

[72:18] Speaker 2: All right.
[72:19] Speaker 2: Now we have access to so much healthcare data in our pockets.
[72:24] Speaker 2: What if we could analyze that data and understand the health of ourselves and our families?
[72:31] Speaker 2: With this problem in mind, we have Max, Nicholas, and Ryan with Tiger Care.

--- Segment 8 (Starting at 70:00) ---

[70:00 - 70:03] (Silence)
[70:03] Speaker 1: Imagine, you and your family just moved to a new city.
[70:10] Speaker 1: Your spouse and you both work full-time, have kids, you're taking care of elderly parents that uh have multiple conditions.
[70:25] Speaker 1: You have to get the new patient paperwork, you have dozens of medications, you have to manage who is taking what medications.
[70:36] Speaker 1: How do you and your family bring all the data together and how do you know the whole picture?
[70:45] Speaker 1: So the issue today is that the landscape of electronic health records is evolving.
[70:53] Speaker 1: Um the 21st century Act data standards are moving towards greater data portability.
[71:02] Speaker 1: But for individuals and families, key barriers still exist.
[71:06] Speaker 1: You know, privacy concerns make data access complicated.
[71:12] Speaker 1: Uh there's fragmentation that leaves records scattered across different systems and the lack of interoperability means these systems don't always communicate with each other.
[71:24] Speaker 1: So chronic so what does the data show?
[71:30] Speaker 1: The data shows that chronic diseases rates are up.
[71:35] Speaker 1: Um the CDC data shows that 60% of US adults have at least one condition and trust in institutions are down.
[71:45] Speaker 1: People feel that healthcare is reactive rather than proactive.
[71:52] Speaker 1: So they take matters into their own hands, and this has led to the rise of the health conscious consumer, because health insights offer promise.
[72:02] Speaker 1: What you know might just save you or at least make you feel like it could.
[72:07] Speaker 1: But more data leads to more confusion.
[72:11] Speaker 1: How do you interpret it all?
[72:14] Speaker 2: So what do families really want with their healthcare data?
[72:20] Speaker 2: They don't just want to they just don't just want access to all their data, they want to understand it, they want meaning to it.
[72:28] Speaker 2: So they also want the whole picture, secure access to all their healthcare data all in one place.
[72:38] Speaker 2: But now having all this knowledge, what do they do with that?
[72:41] Speaker 2: They get if if they don't have some way to interpret all this data, they get very overwhelmed.
[72:48] Speaker 2: This is where we harness AI to understand your family's healthcare data.
[72:54] Speaker 2: Advanced warnings and screenings for for your healthcare data and warnings before things actually become serious can invigorate people to take action before anything is gets to an emergency level.
[73:17] Speaker 2: So that's where we come in, introducing Tigercare.
[73:23] Speaker 2: The family doctor in your pocket, but not really because saying that would be illegal.
[73:33] Speaker 2: We give you a a full hub of everyone all your families and your own personal data as well as incorporating an AI for natural language query which you could ask any questions related to yours or your family's health data.
[73:54] Speaker 3: So let's talk about how we built this thing.
[73:58] Speaker 3: We started with a family.
[74:02] Speaker 3: You and your family uh have all your records in one place now.
[74:06] Speaker 3: We sync up with all of your iOS and Android compatible devices, my fitness pal, uh nutrition, um wearables.
[74:18] Speaker 3: Uh.
[74:20] Speaker 3: And then uh we host it on the Azure Cloud.
[74:24] Speaker 3: World class security keeps your data secure and we leverage the power of open AI with the latest data models to do advanced screenings on every new data point coming in and to let you chat in real time with an LLM that has access to all your healthcare records.
[74:40] Speaker 3: Uh we integrate with Fast and Connect, which has connections with over 40,000 different healthcare institutions in the United States um to bring all those records in securely.
[74:56] Speaker 3: So, in conclusion, we're fixing security, portability and interoperability of your medical records, getting everything in one place and then giving you the power with AI generated insights that will ultimately give you and your family improved healthcare outcomes.
[75:20] Speaker 3: Uh sign up on our uh email weight list at tiger.care.
[75:26] Speaker 3: Uh I'm Max, this is Nick and that's Ryan and we're Tiger Care.
[75:30] Speaker 3: Thank you.
[75:44 - 75:50] (Silence)
[75:50] Speaker 1: All right, so in this Gantlet cohort, we have a lot of gamers and the next solution is your table top AI assistant and its name is Fry.
[76:04] Speaker 1: Next up is Marcus.
[76:43 - 76:53] (Silence)
[76:53] Speaker 4: All right.
[76:57] Speaker 4: Hello everybody.
[77:00] Speaker 4: How many of you play or know about D and D?
[77:06] Speaker 4: All right, that's a number of you.
[77:08] Speaker 4: So, how many people worldwide?
[77:12] Speaker 4: Around 50 million.
[77:15] Speaker 4: That's a lot of people.
[77:17] Speaker 4: And what are people overwhelmingly looking for when playing this game?
[77:27] Speaker 4: Roleplay and exploration.
[77:32] Speaker 4: But there's a problem with this.
[77:36] Speaker 4: You looked at the core rule books for like D and D Pathfinder, most of it's taken up by combat rules.
[77:45] Speaker 4: When you play a session, a lot of it's just, you know, combat.
[77:49] Speaker 4: New players may not know what the rules are.
[77:54] Speaker 4: They may show up to session, don't know how to build, level up their character, how to take a turn and that wastes time.
[78:04] Speaker 4: It slows down the game, that's not fun.
[78:09] Speaker 4: So, what's our solution?
[78:16] Speaker 4: Fry, your or tabletop AI assistant.
[78:23] Speaker 4: So, who is Fry?
[78:32] Speaker 4: Fry is your assistant that will help you level up your character, take your combat turns and explain rules to you.
[78:43] Speaker 4: And if you're waiting, you can just chat with him.
[78:46] Speaker 4: So, let's start with a demo.
[78:49] Speaker 4: He sits straight into the foundry.
[78:52] Speaker 4: I am going to level up my character right now.
[78:57] Speaker 4: So what's going to happen now?
[78:58] Speaker 4: Fry is going to start thinking and he's going to tell us, hey, I saw you just leveled up.
[79:06] Speaker 4: Here's some suggestions.
[79:10] Speaker 4: So that'll pop up shortly.
[79:12] Speaker 4: All right, let's see what he has to say.
[79:16] Speaker 4: Skill increase.
[79:19] Speaker 4: Um hmm.
[79:23] Speaker 4: All right, what's his recommendations?
[79:28] Speaker 4: Acrobatics or nature.
[79:30] Speaker 4: Okay, I like that.
[79:34] Speaker 4: Let's have him do it.
[79:58] Speaker 4: All right, so you can tell him things to do and he'll just modify your character sheet uh so that you know, you don't have to do it yourself.
[80:10] Speaker 4: You can just say, hey, level up my character for you uh for me.
[80:16] Speaker 4: All right, seems like he's done that.
[80:19] Speaker 4: Now, oh, shoot, our combat turn's coming up and we don't know what we're supposed to do.
[80:28] Speaker 4: Well, Fry is here to help you with that too.
[80:32] Speaker 4: So, he's thinking and he will tell us uh give us suggestions so we don't have to dither around for a little bit.
[80:41] Speaker 4: So, all right, Fry, what should we do on our turn?
[81:02] Speaker 4: So, it takes a little bit to think, but the main aspect is uh it's combat leveling up, it takes time and it slows down the game.
[81:19] Speaker 4: So instead of worrying about that, just kind of wasting time and slowing it down.
[81:25] Speaker 4: Remember that D and D is fun and it's for everyone.
[81:31] Speaker 4: Let's get back to having fun.
[81:50] Speaker 1: All right.
[81:52] Speaker 1: Now we have over 4,000 people watching, so I have to say this.
[81:57] Speaker 1: Demoday.gauntletai.com.
[82:01] Speaker 1: Get on our website, see all the presenters, see the challengers, what are they presenting and see if you want to give them feedback or any criticism as to what you're seeing.
[82:12] Speaker 1: Next up, we have the App Halos, the team of Leway, Bjorn, Sude and Terry and they're going to take AI filters to the next level by mapping them to your face.

--- Segment 9 (Starting at 80:00) ---

[80:00 - 80:05] (Silence)
[80:05] Speaker 1: Hello everyone.
[80:07] Speaker 2: Hello.
[80:08] Speaker 1: Introduce uh we're team Hailos, and today we're going to be introducing to you a new way of doing digital filters.
[80:16] Speaker 1: I'm sure most of us have used digital filters.
[80:19] Speaker 1: It's a growing industry where everyone just wants to look like their best self online um in the digital world.
[80:28] Speaker 1: But the problem is when you do try to use these filters, often times it makes you look just like everyone else.
[80:34] Speaker 1: They're not personal.
[80:35] Speaker 1: They are sort of fake.
[80:38] Speaker 1: You can tell at a glance that someone has applied a filter, which honestly defeats the point of a filter, because the idea is that you should look like yourself at your best every day.
[80:47] Speaker 1: Not like you at your best with the filter on.
[80:50] Speaker 1: So, I'm going to hand it off to Terry to talk about the industry.
[80:55] Speaker 2: Uh yeah, so um the existing photo um editing market is huge.
[81:02] Speaker 2: Um this is uh the numbers for uh facetune, which isn't even the biggest company, it's only the second biggest.
[81:09] Speaker 2: And it's bringing in 100 million a year.
[81:11] Speaker 2: Um everyone uses this um these kinds of apps.
[81:17] Speaker 2: But um at the same time they're just not good.
[81:20] Speaker 2: They shove your face into like identical little like pointy chin, bug-eyed, um faces, they're really generic, um and uncanny, um and uh they have all these fiddly settings that consumers just don't want to deal with.
[81:39] Speaker 1: Yeah, so what do we do differently?
[81:43] Speaker 1: Um our process basically is that we take the user's face, we analyze it, and we carefully transform it according to um a proprietary process based in aesthetic science.
[82:01] Speaker 1: Once we have the transformation, we create a personalized 3D face mesh that we store and we can apply to any image, any video from any angle.
[82:21] Speaker 1: And yeah, I don't know if you guys can tell a difference because the screen is not bright.
[82:25] Speaker 1: Uh it's my face.
[82:27] Speaker 1: On the left is the unfiter face, and on the right is the filter face.
[82:32] Speaker 1: It's a little hard to tell, but like we deliberately made it exaggerated so that you can see that the nose is smaller on the right side.
[82:39] Speaker 1: And it blends pretty seamlessly into the face, so if you don't know, like you you just can't tell, which is the whole point of our app.
[82:50] Speaker 1: And yeah, I mean, imagine a world where like you just upload one photo of your face, and instantly our app has a personalized 3D imaging of your skeletal structure, your facial features, and from that point on, we can do any minor tweaks to your face and any photos that you upload in the future, we can apply the exact same changes to those photos at any angle because it's modeled in 3D.
[83:08] Speaker 1: So with that tech we there are so many use cases for we could do.
[83:11] Speaker 1: It's not just like a beauty digital filter, it's also a foundational tech.
[83:15] Speaker 2: Yeah, so now that we have this infrastructure, uh we can just build any sort of app on it, like you know a bunch of ideas like real-time analysis, uh custom color pallets.
[83:25] Speaker 2: Um because now that, you know, we have this uh mesh system, we can just uh map it onto anything.
[83:34] Speaker 2: Um yeah, and a bunch of different consumer app ideas, uh listed all here, yeah.
[83:41] Speaker 2: We got uh yeah, a bunch of things.
[83:45] Speaker 1: Another really interesting potential application is in plastic surgery.
[83:50] Speaker 1: So the plastic surgery market in the US is about $27 billion and that's in the US alone.
[83:58] Speaker 1: So this application or this technology can easily be extended to um 3D modeling and visualization of plastic surgery transformations both for the users or the patients as well as for the providers in treatment planning.
[84:21] Speaker 1: And that's it.
[84:22] Speaker 1: Join the Hailos revolution.
[84:26] Speaker 1: So yeah, hopefully you guys can see that the right side is filter, the left side is not, but the right side looks so natural that's like the no filter filter, which is our tagline.
[84:36] Speaker 1: Thank you guys for your time.
[84:58] Speaker 3: So at Gauntlet AI, we have a few students working at the foundational level too.
[85:03] Speaker 3: This is Yoris, who built a competition for a benchmark that he thinks he can make better with agents.
[85:23] Speaker 4: Hey, I'm uh Yoris and in a lot of our projects, a big problem many of us had was making LLMs work was making LLMs work well with long context.
[85:39] Speaker 4: So when you have like a small prompt, uh a small exercise and you want to make the LLM work, it actually works fairly well, but for an entire book it just gets lost and confused.
[85:55] Speaker 4: And there are some benchmarks which try to do that, but they are only evaluated on foundation models.
[86:05] Speaker 4: So you see a leaderboard on an agent like Longbench, which tries to where you see different foundation models like Gemini and um GPT4 perf performing, but in fact, the right way to approach this problem in a real world setup is to build some sort of agentic system.
[86:30] Speaker 4: So you build some rag or um read in chunks and it is not it is not obvious which model is the best for that.
[86:41] Speaker 4: So for that, I'm sort of set up a competition, a framework where people can submit their own agentic frameworks with the same model and compare the performance on this Long Bench 2 benchmark.
[86:58] Speaker 4: Um showing how easy it is to um participate in that and set up and change your agent and uh upload it in this video.
[87:11] Speaker 4: So you just have to clone a repository, you have a very simple you have a scaffolding and you have to basically just change one function to change a prompt.
[87:23] Speaker 4: And then you up then this automatically gets the token usage gets tracked, the results get uploaded to Hugging Face and if you have a good result, you can get a you you can just submit a pull request and get into the leaderboard.
[87:42] Speaker 4: The original authors of the competition have seen have indicated that they're interested in these results.
[87:49] Speaker 4: So there's a good chance that they will be parts of this leader world and I believe that if you build this sort of ontology of different agents, you will um be able it will be helpful for not just one particular app, but many different uh people trying to build this thing and you get your five um minutes of fame.
[88:22] Speaker 4: So, let's see what's happening here.
[88:24] Speaker 4: We we have basically we have we just have very quickly changed some prompt and we see um the result.
[88:34] Speaker 4: The um agent is being evaluated with some concurrency and um logging and tracking and the results are being processed.
[88:48] Speaker 4: And then on Hugging Face, you see associated with the commit ID, the performance of the uh the the exact results and can analyze them further.
[89:03] Speaker 4: Um yes, that's the that's my pitch.
[89:09] Speaker 4: I have um we can we can see we I I have here a QR code that you can scan to get to this repository and I'd be happy if some people reading this like watching this from online are interested and um submit something.
[89:38] Speaker 3: All right, it's March and we all know what March is about, March Magnus.
[89:43] Speaker 3: And if you don't know what bracketology is, you're about to find out.
[89:47] Speaker 3: This is your platform for your NCAA predictions with John Boyle.
[90:20] Speaker 5: Hey everyone, I'm uh excited to present to you guys today my demo day project, Bracket Lab.
[90:32] Speaker 5: So first, I want to start by answering the question, what is bracketology?
[90:38] Speaker 5: So I'm sure a lot of you guys are familiar with uh the March Maness tournament that happens every year.
[90:44] Speaker 5: I'm sure a lot of you guys filled out brackets last week that were busted about two hours into the tournament.
[90:53] Speaker 5: Um but bracketology is

--- Segment 10 (Starting at 90:00) ---

[90:00 - 90:05] (Silence)
[90:05] Speaker 1: a different process.
It's the process that comes before Selection Sunday, when all the teams are selected.
And to really put it succinctly, it's the science of predicting the 68 teams that will end up making the tournament field.
Um if we take a uh look at the current state of bracketology, um it's not very accessible to the casual fan.
There's a lot of technical barriers.
A lot of uh sharing predictions consists of just posting uh text predictions on people's websites and blogs.
There's no platform at all for users to create brackets.
There's no uh good all-in-one platform for tools in order to make these kinds of predictions.
So my aim with Bracket Lab was to solve all of that.
I wanted to build a platform where it had all the tools you would need to make the predictions.
Uh it would provide a nice user interface for you to build the bracket and view your predictions.
And then also a platform for sharing predictions with uh with other people.

So, without further ado, let me just do a quick demo of uh what creating a bracket looks like in Bracket Lab.
So, the user will first see this kind of create bracket screen.
They'll be met with a team list consisting of all the teams that can make the tournament, as well as an empty seed list on the right side, which is basically a ranking of all the teams uh that for the user to fill out in order to make the tournament.
Um I got a bunch of different tools in here like filtering on different stats, sorting, and then I also have some other buttons which are a little more advanced features for uh for more experienced bracketologists.
Uh for simplicity's sake, I just selected AutoFill bracket here and uh that just used the net as default.
It's customizable, but it just used the net to rank all the teams.
Um so just go ahead and submitting the bracket, it takes us to the bracket preview page.
And so what I did was development algorithm that's very similar to the actual process the NCAA selection committee uses uh in order to bracket the teams.
And so that's basically what's we see here.
And then I give uh the user also some chance for some last-minute customizability if they want to, you know, switch around pod locations or teams.
Then once they're happy, they can go ahead and submit their bracket and boom, they've created their first bracket and they can call themselves a bracketologist.

Um basically to wrap it up, I just wanted to go over some of the AI tools and methodologies I use to build the program.
Uh one of the best things for me is speech to text coding.
Just allows me to develop so much faster when I don't have to type out uh each prompt.
I can be very verbose in what I say and uh I can just allows me to develop a lot more rapidly.
I'm also not a very uh UI oriented guy, so using AI uh UI tools really helps me make the design look clean without being too proficient myself in that manner.
Uh and uh yeah, code suggestions, data processing and that's uh that's basically it.
Thank you.

[95:52] Speaker 2: All right, we have 4.9k individuals watching us live on X.
Just 100 away from that 5K mark.
If you haven't gone to demo day.gauntletai.com, go there right now, see who's presenting and check out our projects.
Next up, we have a group who believes that software is being democratized.
Building apps is going to be really easy, but getting customers is going to be the hard part.
It's Leadify with Malachi, Jesse and Reese.

[97:34] Speaker 3: Is this working?

[97:36] Speaker 4: Am I not?
We got one mic.
That works, right?

[97:42] Speaker 3: Is this coming through?

[97:44] Speaker 4: It is.
I'll bend down.
Test, test, test.

[97:50] Speaker 5: Yeah, what's up guys?
Do my best Tony Robbins.
Let's get up, guys!
Get some energy in here.
Let's go!
Do we go full screen?
Sweet.
All yours.
Go for it.

[98:20] Speaker 4: All right, guys.
Welcome to Leadify.
We're an AI agent that crawls the web to find you customers and reach out to them completely autonomously on your behalf.
So, if you look at the AI landscape, one thing is pretty clear.
Production is becoming democratized.
What that means is everybody can build apps now.
You don't have to be a developer.
You can just use cursor or lovable to create and ship products.
And so, if you fast forward three, five years when everyone is shipping apps, what is the next giant opportunity?
It's going to be finding customers.
Distribution will still remain a bottleneck and so that's what we are laser focused on.
So, in the past, everyone would have maybe a dozen ideas and only build one of them.
Now with AI, everyone has a dozen ideas and builds a hundred products instead.
And so building is fun as we all discovered at Gauntlet AI, but what's not fun is going out and marketing your idea and finding customers for all of them.
Luckily, we think the future is one where you can simply push a button and get a customer.
And so we're stoked to build in that direction.
So, it's an incredibly simple three-step process.
It'll literally take you two minutes of work and we'll autonomously reach out to every platform on the internet to find you customers.
You give us your website, we scrape it, learn what you do and generate a proposal for what type of customers we think you want.
You can go through, correct it, make any fine tunings you want, and then boom, that's literally all you do.
We crawl the web, go through Twitter, go through Reddit, go through websites, we find you leads and draft tailored messages to each and every one of them from you.
And so the vision is to do this for the entire internet, to have agents that crawl the entire web.
But for MVP, we focused on just two platforms, Reddit and Twitter.
So here, you might see that um you can put in your website for your product and a description and we'll figure out what your product does, what your target audience is going to be.
Um any problems, any major problems that your product solves, and then we're going to match that to an appropriate subreddit.
So for each subreddit, we've uh scraped and figured out what the most popular problems that people talk about in that subreddit, what they um what problems that are unsolved and then we'll use this uh customer profile um location and automate the process to find your customer.

[101:55] Speaker 5: Yeah, we do.
So, for this example, we went to Gauntlet AI.
Who needs Gauntlet?
People who need jobs.
They're complaining.
They're like, dude, there's no CS jobs out there.
You've seen those people on Reddit.
Like pessimists who are always down on the world.
We literally go out there, we find these posts and we comment on them to be like, dude, there are CS jobs out there.
In fact, we can guarantee you a 200k CS job if you join Gauntlet, make it through the 12 weeks and then do a demo day pitch like this.
Fantastic.
So that's how you can use Reddit to find leads.
And there's another giant treasure trove of potential customers and that is Twitter, which we all know.
And so after you search through Reddit, we can also search through Twitter.
And how it works is after you enter in who your ideal customer profile is, our agent is going to go through Twitter.
So this is not me doing it.
This is the agent putting in the work and it's going to be looking for people that might be buying a Docusign alternative.
So let's say I'm building a Docusign clone, the agent will have just found any tweets that might relate to that and here it actually found, this is a real tweet.
And so if I were building a Docusign clone, I could totally promote my product right there and potentially close a customer.
And so a lot of exciting things are happening right now in this direction.
Yeah, so like literally everyone I talked to, their face lights up.
They're like, dude, I want more customers.
We've a massive weight list of people interested.
So we've onboarded five beta clients.
For those clients, we found them over a thousand qualified leads and to date we've sent 500 tailored comments.
We are super excited by this direction.
Thank you guys so much for the time.
If you want to stay tuned, feel free to check out our weight list through that QR code and much more is coming.
Thank you guys so much.
Quick bonus I want to throw in there.
Sign up to the QR code and I'll get you 10 leads for free.
Thank you, guys.

I know what you're thinking.
How do I hire these people?
If you go to gauntletai.com,

--- Segment 11 (Starting at 100:00) ---

[101:54] Speaker 1:  AI.com and click apply to be a hiring partner or if you want to join the next gauntlet.
[102:02] Speaker 1:  You can apply to be a challenger.
[102:06] Speaker 1:  gauntlet ai.com.
[102:11] Speaker 1:  Next up, we have Gary, and he's obsessed with the silver tsunami.
[102:16] Speaker 1:  He'll tell you what that is.
[102:22 - 102:41] (Silence)
[102:42] Speaker 2:  Please work.
[102:43] Speaker 2:  There we go.
[102:47 - 102:52] (Silence)
[102:52] Speaker 2:  All right, so I want you all to meet Mark, one of my friends, grandpas.
[103:00] Speaker 2:  Mark is a NASA veteran, 80 years old, and all that he wants to do is make sure the lessons that he's learned in his illustrious career are passed down to his grandkids and the next generation.
[103:14] Speaker 2:  By the way, one of his grandchildren just had a baby last week.
[103:19] Speaker 2:  So this is incredibly relevant.
[103:23] Speaker 2:  And so one thing I started doing was, hmm, I'm not sure what I want to automate.
[103:31] Speaker 2:  So I want us manually start emailing him from this autobiography agent Junie, right?
[103:39] Speaker 2:  And I it was a pretty manual process, but I figured out how to make it really engaging.
[103:45] Speaker 2:  And then once I figured that out, I basically automated that process by creating a platform called talktojunie.com.
[103:55] Speaker 2:  In this particular case, I'm onboarding Dallas, who wants to tell his story.
[104:02] Speaker 2:  Um, I basically provide context on Dallas, and then I say create subject.
[104:17] Speaker 2:  And then what happens is that Dallas now gets an email from Junie, his personal autobiographer.
[104:28] Speaker 2:  And here, Dallas is typing out a a a response to the starting questions.
[104:45] Speaker 2:  Good job, Dallas.
[105:01] Speaker 2:  And then once he's done, all he does is he presses send, and then what I get because I selected that I want to manually review the automated generated follow-up questions because I just want to make sure there's no hallucinations, but it just makes it so easy for me, right?
[105:21] Speaker 2:  And so when I'm ready, I say send the subject, and then boom, Dallas gets another email from Junie for a follow-up.
[105:36] Speaker 2:  And and and and what what is beautiful is that I can see the whole process.
[105:46] Speaker 2:  I can see that the AI is developing a a greater and greater understanding of Dallas.
[105:56] Speaker 2:  And it is honestly trivial at this stage to be able to then put that into a printed book, turn that into an animated, you know, short video.
[106:08] Speaker 2:  So we really automated the process of collecting, um, biography.
[106:16] Speaker 2:  And it was through many, many iterations of building lots of different kinds of interfaces that we found that especially elderly people really love telling their story via email, right?
[106:29] Speaker 2:  It's the interface that they learned, they don't want to relearn something else, but we can still add AI automations behind the scenes.
[106:38] Speaker 2:  Um, if you want to be an alpha user of talktojunie.com, um, please DM me on Twitter at @garyshang, g a r y s h e n g.
[106:53] Speaker 2:  Um, if you have a a a a loved one that just retired, you have a grandparent you know that has an amazing story.
[107:02] Speaker 2:  Um, I'd love to have Junie talk to them and then we can uh preserve their legacy forever.
[107:11] Speaker 2:  Thank you.
[107:15 - 107:23] (Silence)
[107:23 - 107:38] (Music Playing)
[107:39] Speaker 1:  All right, next up, we have Donald with D3.
[107:44] Speaker 1:  If we're going to make coding accessible to everyone, then we have to move away from code syntax and move completely over to natural language in English.
[107:54] Speaker 1:  So, he's coming up next with his coding solution just using documents.
[108:01 - 108:18] (Silence)
[108:18] Speaker 3:  I don't see it.
[108:19 - 108:25] (Silence)
[108:25] Speaker 3:  Testing, testing.
[108:29] Speaker 3:  Uh okay, my name is Donald and I've been working on D3 for the last couple weeks.
[108:36] Speaker 3:  So, about a year ago Open Open AI released GPT 40 and with that release, uh serious code generation became a reality.
[108:50] Speaker 3:  Google and Anthropic were not far behind.
[108:54] Speaker 3:  So, Andre Carpathy saw this coming about two years ago already and more recently, earlier this month, YC reported that a quarter of their startups were using AI to generate 95% of their code bases.
[109:11] Speaker 3:  So, I started to think about the limitations of the chat-based interface.
[109:17] Speaker 3:  Um writing software like having a conversation isn't quite how it works usually.
[109:25] Speaker 3:  Arguably writing software is a lot more like writing down laws.
[109:32] Speaker 3:  Uh, so we skirt around this problem using at rules and cursor, we go outside of our IDEs to generate to generate PRDs and specifications otherwise.
[109:48] Speaker 3:  Uh, so for me this felt inherently like a work around and I thought that documentation is code made a lot more sense.
[109:56] Speaker 3:  So, with this, we can see the whole system at once.
[110:02] Speaker 3:  We can clarify and improve our intent over time and we can keep track of things properly with chat.
[110:11] Speaker 3:  We chat back and forth and then the chat is gone with uh documentation, it's a living document that evolves over time.
[110:19] Speaker 3:  And in a way we're kind of doing this uh with these long prompts that we're writing, we're bringing in external documents, trying to get it all in one place.
[110:33] Speaker 3:  So, I'll show you what I've been building.
[110:42] Speaker 3:  Uh, so the initial process is very much like VZero or lovable if you're familiar, you input your project description.
[110:54] Speaker 3:  The different starts here where the users asked a number of clarifying questions.
[111:01] Speaker 3:  So this uh allows us to gather the user user's intent and with that intent we can generate a list of specifications.
[111:28] Speaker 3:  So, that pops us into this new interface here and on the left we have all of our specifications.
[111:40] Speaker 3:  And with those specs, we can build the product.
[111:46] Speaker 3:  So, this actually generates the entire app all in one shot.
[111:51] Speaker 3:  This is the front end and the back end.
[111:59] Speaker 3:  You'll see that the uh AI agent had some fun with the uh address, Amsterdam, New York.
[112:14 - 112:16] (Silence)
[112:16] Speaker 3:  And uh, so this is the front end and you'll see that there's a an API in the back end here too.
[112:32] Speaker 3:  So, the idea is that the specifications on the left map to the preview on the right and then you can just update the specifications and rebuild the app.
[112:51] Speaker 3:  All right, my name is Donald.
[112:52] Speaker 3:  Thank you for listening.
[112:56 - 112:59] (Silence)
[112:59] Speaker 1:  Next up, we have Devin with Align AI.
[113:04] Speaker 1:  If a curriculum builder wants to align with national and state standards, they can do that in an instant with this application.
[113:13 - 113:47] (Silence)
[113:47] Speaker 4:  Can you guys hear me okay?
[113:49] Speaker 4:  Cool.
[113:51] Speaker 4:  Hi, um, I'm Devin, and this is Align AI.
[113:58] Speaker 4:  And we help curriculum builders align with state and national standards instantly.
[114:04] Speaker 4:  Um, here's what's crazy about my presentation today.
[114:09] Speaker 4:  Everything I'm about to show you today came together in about 24 hours, and this included going from no code to an MVP to a paying customer of $6,000.
[114:22] Speaker 4:  Uh, so here's what happened.
[114:27] Speaker 4:  Four days ago, my original client came to me and said, hey, we can't show our demo on stage.
[114:34] Speaker 4:  Um and I was a little bit screwed uh because Gauntlet AI gave us about three weeks to do this demonstration.
[114:43] Speaker 4:  So, I go home and I'm talking to my housemate and she lays this gem on me where she says, hey, I think I have an interesting problem at work you might like.
[114:54] Speaker 4:  And the idea was so good that the next morning she and I were on a call with her CEO.
[114:59] Speaker 4:  Let me summarize that meeting.
[115:05] Speaker 4:  So when you were a kid in Ja in gym class and you were jumping rope,

--- Segment 12 (Starting at 110:00) ---

[110:00] Speaker 1: that activity aligned with a very specific government standard.
And curriculum builders in the United States need to align with hundreds and thousands of local state and national standards.
And this process is extremely tedious, slow, and takes full-time specialists to do.
So I had out-of-body experience at that moment and I asked the CEO, what if I built you an AI agent that does this in hours for a tenth of the cost?
And he was intrigued.

So I went back home, and about 5 hours I built my first working prototype.
And I begged my roommate to get another meeting with the CEO the same day.
She was not thrilled about the request, but she gave in and this was what I presented to that CEO.
A full working end-to-end prototype of this solution.
It site scrapes thousands of standards, puts in a database and then uses computer vector alignments to compute the exact course alignments.

The CEO was stunned, and lo and behold I had my first customer.
I went ahead and did a little bit more market research after this, and I found out that there's 300 companies, 13,000 school districts all doing curriculum building.
This is a market of about $6 billion every year in the US.
Everybody needs to do this process.
With my product they can save millions of dollars in labor costs.

Here's my current team.
[112:23] (Laughter)
Speaker 1: I also went ahead and posted about this on X.com and thanks to Austin and Gauntlet AI, this post has received about 71,000 views and the initial positive feedback has been great.
Ladies and gentlemen, I need your help to turn this into a rocket ship.
Uh, and I hope you will reach out.

Thank you.
[112:58 - 113:05] (Silence)
[113:05] Speaker 2: All right, to the 5.5 thousand people joining us online and to everyone here live at the HQ.
We're going to be taking a five minute break.
Just stretch your legs, get some coffee, get some water and we'll be back with more presentations in just 5 minutes.
If you haven't checked out demo day.gauntletai.com and made an account, met all of our challengers and all the projects that are working on, you should do that right now.
And if you want to be a hiring partner, check out Gauntlet ai.com where you can apply to be a hiring partner or apply to be a challenger and meet me and Austin later this year.
Thank you guys.
See you in 5 minutes.
[113:59 - 117:46] (Silence)
[117:46] Speaker 3: All right.
All right, everybody both here in virtual.
Thanks for uh hanging out with us for a little bit.
Uh for those of you that are just joining, uh welcome to Gauntlet AI Demo Day.
Um we're through the first section.
I've got another little section and then we're going to take a break and we'll actually restart the live stream uh after lunch because X only lets you go for so long before it forces you to stop.
Um but we're super excited to to have you all here.
Um for those of you that are unfamiliar with Gauntlet, uh it's a 12-week program.
We take people from all over the world.
Uh we are very, very selective for uh intelligence and coding ability.
Um and then the reason it is named Gauntlet is we put you through a series of challenges.
Each week is a new challenge in order to move from one week to the next.

--- Segment 13 (Starting at 120:00) ---

[120:05] Speaker 2: you have to successfully complete the challenge. So we started out with around 230 students who were accepted from more than 3,000 applicants uh almost three months ago. Uh now we're down to right around 110. Um so that's that's going to be the number that graduates with us. Uh if you're interested in being a Challenger in the future, it is completely free, all expenses paid. We cover your flights, we cover your housing, we cover your food, we cover the events, um all of that stuff.
[120:51] Speaker 2: Um if you're interested in applying to be a hiring partner, uh you can do that at gauntletai.com for subsequent cohorts.
[121:04] Speaker 2: Um there is no guarantee that if you want to hire, you are able to hire. We have to be selective on both sides. Um with that said, uh we're going to kick off the uh next set of demos and we'll go from there. Thank you.

[121:45] Speaker 3: All right. Next up, we have David, who's going to bring you Dexter. It's an investigation management platform for law enforcement where he uses AI to supercharge their workflow.

[122:32] Speaker 4: Hey, so I'm David and this is Dexter. Uh an investigation management platform. So consider that uh most crimes go unsolved uh reported crimes. Uh and this is not because they're particularly difficult to solve, but uh just sheer volume. Uh and even worse, uh often rushing can lead to uh mistaken convictions from mishandling evidence.
[123:02] Speaker 4: Uh so Dexter accelerates the investigation process while also making it more robust. So, let's take a look. Uh so you can manage your cases here uh and inside a case you can add uh all of the case files that you care about. Uh and once you add them, it uh automatically analyzes them, uh extracts transcripts and uh relevant information. So you can see uh for scanned documents, uh we get all of the text side by side, uh so you can look through that. Um and for longer documents, it uh keeps pace of the content. So, you know, you you want to match the page that you're actually on with the text that you're looking at. Uh and this makes it easy to copy out or uh do search and things like that.
[123:54] Speaker 4: Um we describe pictures, uh so we get a detailed description of like crime scenes and things like that. Uh and through all of this, we can search. Uh so we do semantic search uh which gives both uh good sensitivity uh for everything we might care about as well as good specificity uh so that we can tell precisely what we're looking for all together in one context. And uh we can also uh transcribe audio and out here. Uh transcribe audio and uh get location awareness. So this is my favorite feature. We can extract all of the uh locations of interest in a document uh and then from that context uh find precise locations on the map uh to look into.

[124:50] Speaker 4: We can also do manual annotations uh that mirror handwritten notes uh in the traditional workflow. Uh so we can uh annotate specific locations on the documents uh so that when we uh go and uh try to search for those annotations later, uh we can actually find them directly in context by jumping back to the page and the location that it's on.
[125:16] Speaker 4: Um so this is a lot better than managing a big stack of papers uh and pinboards. They really do use pinboards. And uh throughout all of this, uh you know, we're working with uh many different teams in law enforcement. So we have uh sharing uh cases uh at the file granularity. Uh and this allows us to uh keep our files accessible while also keeping them uh secure uh because this is really important for you don't want to leak evidence. So, uh that's the uh the state of the demo right now. This started as a last week as a two-day sprint uh for Othram. Uh and uh they they requested this and I'm going to continue working through it uh to bring it to market uh in the next week. And so many thanks to the original team that worked with me on this uh to get a lot of the original features. Thank you.

[126:33] Speaker 3: Next, we're going to go to the area of augmented reality and AI with Dan. He has this app called Tono, which helps you learn Chinese using augmented reality and AI.

[127:33] Speaker 5: All right. How many people here want to learn a foreign language? Anybody? And how many people here have ever tried to learn a foreign language and have given up? So, I think it's pretty common. uh as someone who's been learning Chinese for the past 15 years, it's quite common um to to fail because learning a foreign language is hard and Chinese is especially hard. Um as everyone knows, China's economy is booming. A lot of people want to learn Chinese for that reason, but it's a hard language to learn and all languages um are hard if you learn them in the wrong way.
[128:15] Speaker 5: And a lot of students do learn languages in the wrong way. So in schools, we use textbooks, but they're often outdated and kind of boring and you can't really learn to speak a language by reading a book. Um and some apps like Duolingo, for example, you get some amount of speaking, but it's not really personalized and a lot of people just kind of end up giving up. So, how can we solve this? I think AI can help. So, my solution is to use AI to make language learning more about interacting with the real environment around you as opposed to just looking in a book or something like that. And so what I AI can do, first, it can interpret the environment. Second, it can personalize the learning, teaching you the things that you care about that you want to learn. And third, it can test your output. You can speak to the AI and it can actually um understand what you're saying and tell you if it's right or wrong.
[129:10] Speaker 5: So, let me show you guys a demo of how this works. So, the app is going to look at our environment. It's going to recognize all the objects in the environment. Then we can click on the objects.

[129:22] Speaker 5: æ°´èï¼èæã æ¢¨å­ï¼è è¿ï¼åã ç¤ç®±ï¼æ¤å­ï¼é¤æ¡ã å°ç®±ãå°ç®±ã Then all these objects are going to be saved to our collection and we can go ahead and repeat the pronunciationsã
[129:45] Speaker 5: é¤æ¡ï¼æ¤å­ï¼ç¤ç®±ã Learn the pronunciations. We can check out our collection of all the objects we saved.

[129:55] Speaker 5: å°ç®±ã Practice speaking them trying to engrain them in our memory.

[130:04] Speaker 5: æ°´èã If you guys can remember those two, bing means refrigerator and means sync. Now we're going to go ahead and practice. So what is the Chinese word for refrigerator? Can anybody tell me?

[130:18] Speaker 5: æ¤å­ã å°ç®± Excellentã å°ç®±ã Okay? So, we already learned a word. What's the word for a dining table?

[130:26] Speaker 5: é¤æ¡ã é¤æ¡ã
[130:28] Speaker 5: Okay? So it's going to go ahead and quiz us and it's going to use spaced repetition to, you know, show us these cards just as we're about to forget them. In addition, we can go backwards, we can go from the Chinese to the English. We get the Chinese. What does bing mean? I knew I remember refrigerator, right?

[130:48] Speaker 5: What is Sanjo é¤æ¡ã Dining table. Great. Okay? So, we've already learned two words in Chinese and these are, you know, words that we chose, words that we're interested in. Um and finally, possibly most importantly, we can practice our pronunciation to make sure we're getting it right.

[131:04] Speaker 5: å°ç®±ã So, we speak into the app.
[131:07] Speaker 5: å°ç®±ã and it's going to quiz us. Give us a score, tell us how good our pronunciation is and we can get the feedback based on this. Okay? So, um, I have not released this app in the App Store yet, but I did release it on WeChat, which is a new you guys may know is like China's social network. A lot of people gave me some great feedback and they also said, when is an English version of this app coming out? So, I think this could work for Chinese, it could really work for any language, and it's going to be a way to make language learning more interesting, more fun, and, um, more useful in the real world. Thank you.

[131:59] Speaker 2: If you're just joining us and haven't gone to demoday.gauntletai.com, use this QR code to check out our challengers, the projects they're working on, and to give them real feedback on their applications. Next up, we have Dallas. Dallas can take any TikTok creator and make them into an agent. What am I saying? Well, he's about to just show you.

--- Segment 14 (Starting at 130:00) ---

[130:00] (Silence)
[130:06] Speaker 2: Okay, so where do you get your information from?
For me, 99% of it comes from online.
So I thought, what if we could take our experts and turn to turn them into agents we can interact with?

And so, I'm going to start with Tik Tok.
I connected with a Tik Tok influencer with over 50 million views on teen dating advice.
And she was saying that the struggle for teens is that they might be afraid or embarrassed to talk with their parents or friends, so they need somebody that they can get real advice from privately.
And so, I think the best way to illustrate this would just be to let her talk for herself here.
So, you can come in, give it a call, and get on the phone with her.

And I'm going to get the sound on.

All righty.
So you come into the app, click call.
[132:04] Speaker 3: Hello, it's L.
How's it going?
[132:07] Speaker 4: Hey, can you help me reach out to somebody?
I'm afraid when I ask them something it's going to sound rehearsed.
[132:15] Speaker 3: Of course, it's totally normal to feel that way.
Let's make it more natural.
What do you want to ask them?
We could brainstorm some casual ways to phrase it together.
[132:24] Speaker 4: Um, she's on the soccer team.
[132:27] Speaker 3: That's cool.
Do you want to ask her about soccer or are you thinking about something more personal?
Like hanging out or getting to know her better?
[132:34] Speaker 4: Uh probably the soccer team.
[132:36] Speaker 3: Got it.
You could start with something like, hey, how's the season going for you?
Or I saw you guys had a game last week.
How did it go?
These sound casual and show you're interested in what she does.
What do you think?
[132:50] Speaker 4: Yeah, that's probably good.
Then what do I say after?
[132:53] Speaker 3: After she responds, you can keep the conversation going by asking follow-up questions.
For example, if she mentions how the game went, you could ask what her favorite part of the game was or if she has any memorable moments from the season so far.
You could also share something related, like if you play sports or enjoy watching soccer.
Just keep it light and fun.
[133:26] Speaker 2: And so, we can also meet you where you're at.
So if they're texting, we can go into IMessage.
You can take a screenshot of the question in your messages, pull up that screenshot all within the message app, send it to AI and get a response where you can easily insert the response into the chat and send it, helping you overcome anxiety and make new relationships.
Thank you.
Yeah.
[134:27] (Silence)
[134:30] Speaker 5: All right.
Next up, we have Cyrus with two applications, Logcow and Bud.
Logcow lets you catalog your memories and your events and Bud is your plant-based companion within your mobile.
Let's welcome Cyrus.
[135:19] (Silence)
[135:53] Speaker 6: Okay.
Uh hello.
Uh I'm Cyrus.
I have two apps to present, Log Cal and Bud.

Uh Log Cow is a social diary.
Uh scan this QR to join uh an app web app to join an entry for this event.
Uh this is how most people join Logcow.
If you never join an entry with other people, it's a purely private journal, but when you join an entry with someone else, they are considered your friend and you can now add each other to hangouts by username uh instead of always scanning or using a uh invite link.
Uh from there you'll be able to see uh common entries with them, but you'll only ever see hangouts that you're in.
Um, so it's uh as opposed to a more open social media, it's uh a social diary.

Uh scientifically, journaling is good for you.
Uh second point.
A lot of people don't like current social media.
Who even uses Twitter anymore?
No one.
It's called X now.
Uh three, in person social interaction has been on the decline for years, not even counting Covid.
Scientifically, uh loneliness has skyrocketed.
So, what about a social journal that gets you to have more social experiences, not less?
Um, I'll just explain some of the views here.
So this is the like landing.
Uh you have a sort of calendar mosaic of your hangouts.
Uh and this is at least for me, like I built this app for myself, uh trying to fill this up actually gets me to hang out with people uh with people more than I would.
Um, then we have uh log entry uh and log view uh and there are more features than just that, but I will move on to Bud.
I have another QR for this.
Uh scan that one and you can claim this little workout bud.
Um, I'm not giving you as much time for this one.
Uh I think that's that's probably fine.
Uh, Bud consists of two parts, a desktop web app for building your bud and a mobile app for carrying a bud with you uh to chat with about your goals and good habits.
Um, the editor allows you a high degree of freedom when building your bud.
Plant parts cost stars.
Uh you can place parts wherever, however, uh whichever color uh so on.
Uh the plant chat is equipped with conversation history, memory and goal tracking to push you further in your life while earning stars to build more buds.
Uh as you grow, your plant collection grows.
You can even gift buds to your friends.
Uh Bud won't solve your life problems.
You will.

And that's it.
I mean, uh the video does show uh the actual carrying of one of the buds that you edit into the mobile companion app.
Um, which is a very simple app.
only allows you to uh chat with your bud and um uh collect stars to then go back and create more plants if you want.
Yeah.
That's it.
Thank you.
[139:31] (Silence)
[139:35] Speaker 5: All right, next up, we have the Mortenson Brothers, Christian and Benjamin.
Why is it that the most intelligent minds in our country have to go through so much rigor mortis to get funding?
What if you could just apply for grants at the push of a button?
That's what Christian and Benjamin are solving.
[140:06] (Silence)
[140:09] Speaker 7: All right, I'm Christian.
[140:10] Speaker 8: And I'm Ben.
[140:11] Speaker 7: And this is our product Turbo Grant.
So every year in the US, we're spending about $1.2 billion dollars on grant applications and not even research, just purely the applications.
On an individual level, it takes about 170 hours and $8,000 per application.
And from hearing from researchers in the field, this is a massive pain point and not a good use of their time.
So we took a deep dive into all the complexities of the grant application process and identified several steps where uh AI can assist at a minimum or entirely automate a step.

So first, we start with three main inputs.
The first is a research description, which is a one to two uh page outline of at a high level the research that they intend to conduct.
Then we have scientific figures, which is images of tables, charts, data that supports their research, which we send to an image model, so we can get a text representation of it and we can plug that in at different points in the uh application process.
The last is a 30 to 60 minute chalk talk where researchers can um in free form, uh describe all the research that they intend to conduct rather than dealing with all the formatting complexities and just simply get their thoughts out.
And from there,
[141:50] Speaker 8: And then we have our funding matching agent.
So it's going to take those inputs and it's going to extract it.

--- Segment 15 (Starting at 140:00) ---

[140:00] Speaker 2: answers and then ask the researcher follow up questions to best match them up with funding opportunities for their research.

[140:33] Speaker 3: And then once we know have a good idea of the type of funding opportunities that they're actually eligible for, what we've done is we've taken all of the NIH and NSF grants and we've uh scraped all the web pages and use LLMs to turn them into structured data, so that you can filter quickly for various things like are you doing animal trials?
[141:00] Speaker 3: Is there a certain deadline you need to hit, um, or award amounts.
[141:07] Speaker 3: Um, you can also search through and score, um, different uh, funding opportunities based on uh what you're looking for.
[141:19] Speaker 3: And then even once you get onto the page, we have a chat interface.
[141:24] Speaker 3: So if you need to ask more fine grain questions, you can quickly get answers and as soon as you select it, we'll start generating your outputs.

[141:36] Speaker 2: And then we have our research agents.
[141:40] Speaker 2: So it's going to take the um Emerald Cloud Labs uh equipment catalog and it's going to find the most relevant pieces of equipment for the researchers experiments.

[142:01] Speaker 3: And then the second agent that we've built is a uh sources agent.
[142:07] Speaker 3: So we look through your inputs and see the claims that you're making and the research that you're going to conduct and it finds uh questions that could use support.
[142:18] Speaker 3: And then it will go out using the perplexity API and pull in uh cited sources that you can use later on in your research.

[142:34] Speaker 3: And then the next is the AI writing co-pilot.
[142:39] Speaker 3: So there's a bunch of documents that you need to create for your uh research application and what we've done is combined the inputs that you've given us earlier along with the context of what the documents purpose is and we generate a first draft for you.
[143:00] Speaker 3: And then from there, we have a built-in AI writing co-pilot where you can come up with edits and say things like fix all the spelling errors, condense these paragraphs and you collaborate with it until you're uh happy with the result and it uh simplifies that time considerably.

[143:32] Speaker 2: And then we have our requirements agent.
[143:35] Speaker 2: So this is going to allow you to skip the 300 plus page agency requirement documentation.
[143:44] Speaker 2: It's going to generate a bunch of questions for the researcher and extract the required documents for their specific funding opportunity and their research.

[144:06] Speaker 3: And now that you've got your full list of requirements and you've completed all your documents and you have your output documents ready to go, uh you're now ready to submit in just a fraction of the time.
[144:17] Speaker 3: Thank you.

[144:22 - 144:40] (Silence)
[144:40] Speaker 4: What if we could take product management and bring it to everybody?
[144:47] Speaker 4: This is an app that supercharges product management, Sonata PM, with Chris and Eli.
[144:55] Speaker 4: Let's give it up.

[145:00 - 145:33] (Silence)
[145:33] Speaker 5: All right.
[145:35] Speaker 5: So, building products is easier than ever.

[145:44] Speaker 5: But building good product is still really hard.
[145:51] Speaker 5: You need to know your market, your competitors, your customers, inside and out, front to back.

[146:06] Speaker 5: Pricing, positioning, messaging, and obviously your product, design, technology, the team, what they're working on and when and why.

[146:32] Speaker 5: And to do all that and to know all that, you're working in a dozen different systems at a minimum, and you're having a dozen conversations and writing a dozen documents, doing a dozen edits on it, and it's a lot, it's hard.
[146:51] Speaker 5: And so we're building Sonata PM to make it easy for founders, product managers, for anyone, an integrated hub that reads from and writes from to all the systems you're already working with and equips you with a team of product specialists, AI agents that do research, competitive analysis, documentation, customer insights, prioritization and planning, the whole works.

[147:28] Speaker 5: Um maybe not the whole works just yet, we started with research and documentation.
[147:36] Speaker 5: And to show you what we've built so far, here's Eli.

[147:41] Speaker 6: Thanks.
[147:44] Speaker 6: So, to show you what um our flow looks like right now.
[147:51] Speaker 6: We starting?

[147:55] Speaker 6: There we go.
[147:57] Speaker 6: So, um to get you started with your project planning process, uh we provide a number of templates for documents that are commonly used in project management like user persona.
[148:10] Speaker 6: When you go to fill this out, you're just having a conversation with Arya, our document edit editing specialist, describing your ideal user.
[148:26] Speaker 6: She'll take from the conversation the key points and create the user persona and you can go in and modify it yourself if you want.
[148:37] Speaker 6: It's a collaborative process.
[148:40] Speaker 6: For documents that uh are a little more technical, you can make use of of Reed, our research agent to go and do competitive analysis in the background while you continue to edit the document with Arya.

[149:00] Speaker 6: Uh, this particular project requirements stock is for um a new team member.
[149:06] Speaker 6: We'll be rolling out in a few weeks called Echo that will be taking uh user feedback across multiple sources and aggregating it into one place.
[149:23] Speaker 6: And it looks like our uh research report is done.
[149:29] Speaker 6: So, um we can go in there, grab the relevant points and bring that into our project requirement doc.

[149:50] Speaker 6: We also can integrate into whatever ticketing management software you're using like GitHub or Jira.
[150:02] Speaker 6: So, when you think that your project requirement doc is good enough, you're ready to move on and start building, uh we will automatically generate for you uh a set of user stories.
[150:25] Speaker 6: Once these are generated, you can go over, make any final tweaks, and then from within Sonata PM, send out those tickets to whatever platform you have integrated with.
[150:44] Speaker 6: Here we have GitHub, so click a button and we have a GitHub issue.

[150:54] Speaker 5: Thanks Eli.
[150:56] Speaker 5: So that's Eli Guting, I'm Chris Sutton with Sonata PM.
[151:00] Speaker 5: Thanks.

[151:02 - 151:16] (Silence)
[151:16] Speaker 4: Next up, we have Campbell, who'll be presenting a solution that is linear for event planning.
[151:23] Speaker 4: He's taking Gauntlet AI and seeing if you can manage the ops of all the events end to end.

[151:29 - 151:43] (Silence)
[151:43] Speaker 4: Woo!

[151:51 - 152:46] (Silence)
[152:46] Speaker 7: Gauntlet Ops is the solution to a problem posed by our by our very own Gauntlet Ops team.
[152:58] Speaker 7: I asked them, what one app would help you manage our 120 students and staff the most?
[153:08] Speaker 7: What resulted was a streamlined all-in-one event planning app that focuses on a clinically sharp UI, a focus on performance, and most importantly, a magical feel when navigating through it and overall using it.
[153:36] Speaker 7: The main problem with event planning apps or event planning in general is that all of the apps are fragmented.
[153:52] Speaker 7: So they use Google Docs, Google, I mean, they use Excel, Google Docs, party full and other forms for invites, and they also use Airtable to manage all their data.

[154:15] Speaker 7: So my solution streamlines all of that and I'll show you through a demo.
[154:22] Speaker 7: So we click on Demo Day.
[154:28] Speaker 7: We have the event Command Center.
[154:34] Speaker 7: You see a high level overview of everything you need.

[154:43] Speaker 7: You have favorited time blocks, which we'll get to soon.
[154:48] Speaker 7: We have a fully fledged task manager.
[154:55] Speaker 7: We have vendors, and of course, we have a way to manage our participants.
[155:08] Speaker 7: Last but not least, we also have a way to manage our budget.
[155:13] Speaker 7: Let me go through with the timeline first.
[155:17] Speaker 7: We have a beautiful timeline view of the entire event.

[155:27] Speaker 7: It's color coded by the actual part, not the priority, but the status of the event.
[155:34] Speaker 7: So we have pending, in progress, and complete.
[155:41] Speaker 7: You can click in on it, get all the details you need, add it to your favorites, so you can see it on your event Command Center.
[155:52] Speaker 7: And you can also assign it to a task.

[156:04] Speaker 7: Next, we have the actual fully fledged task manager, color coded by priority.

[156:20] Speaker 7: You can assign it to a logistics team member to deal with.

--- Segment 16 (Starting at 150:00) ---

[150:00] Speaker 2: you can assign a budget item to it. So you can make a task, assign a budget item, and deal with that on a different page.
You can also chat with it with a real time chat with your team members and of course you can make the budget item straight from this page.

Next is a critical page, basically the financial and the budget page. The important thing here is that we have a total budget but we have planned expenses and actual expenses. This actual expense and this budget health are the critical things that stakeholders need to know.
Basically you need to know an easy way to track if you're on over or right on budget.

Also from this page you can add budget items straight from this page.
You can also see what vendors those budget items are connected to. Last but not least, we have a Rolodex of vendors. Over like these last eight weeks, of course there's going to be way more vendors from these from our event planners.
So we have an we have an easy way to filter sort them.
We have all the info we need here and they're also all intricately connected to the budget items.
And last but not least, you can upload any docs for that vendor.

So let's say you have receipts, you have menus, you have any emails from that vendor, you can connect it here.
Essentially you have an end to end event planning app that's ready for iteration based on any feedback.
Thank you.

[153:42] (Silence)
[153:49] Speaker 3: Next up, we have Cameron who tried to use AI to do graphics programming.
He's going to show you what he was able to render and how he went about it.

[154:27] (Silence)
[154:30] Speaker 4: Thank you Ash.

[155:06] (Silence)
[155:33] Speaker 4: Morning, I am Cameron McWilliams and for the past three weeks I have been working on a Ray tracing render.
Um, so in traditional rendering, uh the only thing you get is the projection of the scene onto the camera plane and this gives relatively little information uh to work with for actually shading the scene.
Here instead we are casting rays out into the scene and by simulating the paths of light reflecting off materials, um, we can use very accurate reflections to get um, specular surfaces like this metal ball in the front.
Um, we can use uh fuzz and randomized reflections to get um rougher surfaces are all of these diffuse balls and we don't even have to have reflections.
We can have uh rays um go through objects and um that's how we get um physically accurate glass refraction.
Um, this is a solved problem, so why did I make another renderer?
Um, the problem that I had with currently existing renders is that um they tend to um have some shortcomings for prototyping rendering techniques and procedurally generated assets.
Renders generally fall into two categories with respect to this.
You have renders like um Renderman and Cycles, which in principle can be used for this kind of work, but they are very opinionated um renderers because they are designed for production use by artists, so they're not super ideal.

The other um kind of render you have is renders like Adobe Substance render which is ostensibly for this purpose, but they can be a non-starter for a lot of projects because their model is proprietary.
Um, so you don't fully know what you're doing.
Um, during these three weeks I focused on um texture generation.
Um, I'd like to say that I reached uh future parity with substance designer in three weeks.
Um that's not true, but um it is uh got a suitable array of uh features for making and rendering textures.
Um, so we do uh AI first building here.
So a question you might have is, how does AI do on graphics programming?
And uh the answer is, um, at least for writing code, not great.
Um I've got mixed results with it.
But AI still has a lot of uses in uh graphics programming.
You can cruise through boilerplate.
There's like 2000 lines of Vulkan boilerplate that I did not have to write most of because I can just tap through it all.
That's very nice.
Um, you uh don't need to use Google anymore because uh LLMs can give you uh synthesized info and contextualize it in terms of the code you've already written, which keeps you in the flow and keeps you iterating faster.
And finally, uh you have the world's smartest rubber duck to consult.

So while uh LLMs are not super great at writing code um for graphics, they are still surprisingly good at reading and understanding it.
Um and Claude often was able to find bugs that I made faster than I could.
Um, so does AI have a place in graphics programming?
Um the answer is a resounding yes.
Um, if you're interested in uh someone who thinks this is a fun use of three weeks fiddling with uh rays and vectors, um, please get in touch.
Um, and thank you for your time.

[159:37] (Silence)
[159:44] Speaker 3: All right, next up we're going to have Azaldin, who's going to be presenting a minimalist extensible reservation system, and he's calling it Romeo.

[160:15] (Silence)
[160:30] Speaker 5: Hello everyone. Uh Romeo is an extensible abstract minimalist reservation system. Uh the system is comprised of two main components, uh reservables and reservations.
The abstract uh nature of the systems allows for extensibility.
Uh the following reservable has been created for demonstration purposes. Uh reservables may be associated with child reservables as follows.
Uh an end user may create reservations as follows. Um, add notes and confirm reservation.
The power of the system lies in the um constraints and validators that may be associated with reservables.
Uh constraints and validators uh uh uh may be used to uh constrain the reservations that are uh created on a reservable.

For demonstration purposes, um the a single validator will be tested.
Uh the uh an end user may specify a simple description in uh natural language.
For example, give me a 15 minute break after every reservation. Um, the system will take the description and um generate a corresponding uh uh uh validator uh that will be applied against every reservation creators for the reservable.
Uh we'll go and create the uh validator.

Uh after the validator is created, we can test the validator by attempting to create another reservation.
Uh uh attempting to confirm the uh reservation uh returns the following error.
This demonstrates the uh extensibility of the system.
Uh uh infinitely many uh validators or constraints can be applied against the reservation system.
Uh the system is very minimalist. Uh um if interested, uh um uh um one may reach out to uh to me personally to gain access to the system.
Uh this concludes the presentation. Uh thank you very much.

--- Segment 17 (Starting at 160:00) ---

[160:00] Next up, we have Ayush, who's going to bring you Vibe Check. It's going to be day emotions and how you're feeling over time.

[161:17] Alright.
[161:18] Speaker 2: Alright.
[161:19] Speaker 2: Hello everyone. Uh, my name is Ayush Shah. and, uh, today I'm going to be presenting you guys, uh, an app called Vibe Check. And this app addresses a problem that I personally had throughout my first few years of a college, where I felt like I was living life on a sort of like a autopilot mode, where I would wake up, I would go to the same classes, I would interact with the same people and do the same things day in and day out, oftentimes for several weeks to months. And a lot of these times I wouldn't be feeling too well. I, I wouldn't really know if this is what I want to be doing, and I wouldn't really be able to make any changes to it because I was just so like used to the same environment. And what I wanted to do is come up with a solution to help people break out of that habit and allow them to have a way for someone to actually tell them what they can do to better improve their wellbeing.

[162:49] Speaker 2: So, Vibe Check is the most simple way to track, understand and improve your overall wellbeing. And the problem that we're solving, kind of that we're addressing is the high stress rate amongst adults. Uh, obviously, they can uh, adults really struggle to identify different happiness factors that they're having. And also, they have reportedly, um, stated that they would be willing to take guidance and uh, take action if they were given proper feedback on what to do to improve this.

[163:35] Speaker 2: So, this Vibe Check allows you to input your mood and different voice notes throughout the day, and it's a single action app. And so all this will be in your specific individualized data store. And over time, you'll be able to accumulate enough data to talk to your AI voice assistant and be able to get proper feedback to you and tell you if you need to do something that maybe can potentially improve your overall wellbeing. And with this, I'll take you guys to the live demo and I'll show you, um, how I'm using this app.

[164:55] Speaker 2: So as you can see, um, here we'll just input the mood, how how the user is feeling. Um, they'll have a very easy to do a way to do that. They can add voice notes here, as you can see. Um, they can save how they feel. Uh, they'll have a chance to go through different uh places such as their timeline, view their past history. Uh, they can make changes, uh, very easily and we want to really emphasize the user experience. Uh, this is the voice assistant, we'll come back to that. Uh, here you can see your overall insights has the how they have changed over time. Uh, you have a easy way to reference all of this and go back and just see overall how you're doing over certain periods of time.

[166:38 - 167:42] (Silence)
[167:42] Speaker 2: I've been looking over your mood data, and I can see that your overall mood has been declining over the past few weeks. Let's take a closer look together and see if we can figure out why. One pattern I've noticed is that your mood tends to dip on Mondays and Thursdays, especially after your CS theory classes. For example, last Thursday, your mood dropped to a 310 after your class and your note mentioned feeling frustrated and lost. It seems like these classes might be a significant source of stress for you.
[168:16] Speaker 3: Yeah, I think so. Um, I don't really like these classes. Um, I think I might be a change of major. Um, could you maybe help me, um, point me in the right direction maybe with, uh, how I can change my major potentially to something maybe I'll enjoy?
[168:29] Speaker 2: Got it, Alex. It sounds like you're seriously considering a change, and that's a big step. I'm here to help you think it through. Looking at your mood data, there are some interesting patterns that might help guide you. For example, one, on March 16th, you logged a 710 mood during a discussion about psychology with a friend. You noted feeling curious and engaged. That suggests psychology might be a subject you find stimulating and enjoyable. Two, your mood consistently improves during creative or social activities. For instance, on March 19th, during an art history class, you rated your mood a 710 and mentioned enjoying the storytelling aspect of art. This shows you might thrive in fields that involve creativity or connecting with people.
[169:28] Speaker 3: All right. Yeah. So, um, that is the voice demo. Uh, sorry we couldn't get on the first try, but, uh, connecting this and as you can see, it's able to get personalized feedback from the assistant and it's able to identify different patterns, patterns in their day-to-day uh changes and voice notes and give them very applicable and actionable advice. So, thank you so much.

[170:20 - 170:50] (Silence)
[170:50] Speaker 2: Next up, we have Asher, who's going to be showcasing Trace, the next way to journal with AI.

[172:14] Speaker 4: Hey guys, I'm Asher, and today I'm going to be presenting Trace, which is the new way to journal. So, what I'm doing is because many people want to journal, right? But the problem is that some people don't have time or don't want to spend the time just taking 30 minutes out of their day and sitting in the computer, staring at the screen for 20 minutes to write down your day, what happened, format everything. So what I've built is Trace, which lets you just chat with AI and you can it'll format your entry, you just talk to about your day, go back and forth, tell it what you did, uh how you're feeling, and then it'll format that, store it for you and then you can view it, you can go in edit it, change, whatever you want. Um, talk with it more, change. It's just to make it really easy to journal and just do that.

[173:45] Speaker 4: So, I can show you a demo right here. Um, let's see. As you can see, it has UI, you can enter things you want, but then the main thing is the voice assistant. So, I'm talking to it, I'm like, oh, hey, I went climbing today. I'm feeling pretty tired after that. Uh, I finished my new favorite book, which is Project Hail Mary. You'll see it'll pop up there after it thinks for a while. Um, and it formats it for you. And let's see, after it's thinking it will show you a little preview, so you can make sure what it's going to enter is what you want. If it's not, you can just accept it and then edit it yourself. There you go, that's a preview. That's what it got from my little talk with it. Um, you can accept it. There you go. You can see all the data entered, formats everything nicely. And, um, another thing we did is some places don't let you have your own data very well, like it's all stored through the cloud, you can't really access it. So what we did is we made it all stored locally, you can access your data, it's formatted in markdown. You can just view it and open it in whatever you want and look at it there.

[175:24] Speaker 4: So, that's Trace, and thank you guys for your time.

[176:32] Alright.
[176:33] Speaker 2: Next,

--- Segment 18 (Starting at 170:00) ---

[170:00] Speaker 2: Next up, we have Ali with live voice translation and his app Vocal.

[170:28 - 171:40] (Silence)
[171:40] Speaker 3: All right guys, hope you're doing well.
Um a common problem that I think we all run into is especially if you're traveling, but even if you're not traveling, just in the last two months that I've been here in Austin, Texas, sometimes I want to have uh voice translation.
Despite having over 600 days in a row on Duolingo, my Spanish has not improved to a level that I can communicate with people very easily.
Just in the last week, uh I had a delivery driver who couldn't speak English, and I was trying to get Rebecca's food actually up here, but he wouldn't give it to me.
And so she had to go downstairs herself.
Another time, I was in an Uber, I wanted to get dropped off on the other side of the street, and I had the same problem.
So, we have a lot of solutions for this, um and I've built a better one.
Uh I'm not alone in this.
One of the top tech influencers on Twitter, Levels.io, he speaks Dutch, English, and some Portuguese.
He was on a rant a few weeks ago talking about how he needs a better voice translation app.
And so a lot of people had attempts in their replies, and he ended up detailing exactly what he wanted.
And so these are all the specifications that he was talking about.
The industry leaders right now are Google Translate and Microsoft Translate.
Apple also has a translation app.
Uh but they just don't get the job done, and you can see why.
So, on this side, we have Google Translate.
I was speaking with my friend Drew Sutterquist.
He was speaking in English and I was speaking in Spanish.

And so, you can see that you have to wait for the person to be done talking, you have to press this button every time in a lot of languages that aren't supported in conversational mode, and then you hear the translation.

Uh we're supposed to at least okay.
Okay.

On this side, we have Microsoft Translate.
So, the UI there is a little bit better.
Um you can see that, you know, like there's a split screen view, but the same problems kind of exist there.
Now moving on to vocal.io.
So let's have a volunteer from the audience, perhaps this handsome gentleman right here.
And uh, we'll go to the live translate page, and we'll have a conversation.
I have forgotten how to speak English, so I'll speak in Urdu, and he'll speak in English.
All right Asher, whenever you want to start.
[174:13] Speaker 4: Hey Ali, what do you oh hey.
What do you think about Austin so far?
[174:18] Speaker 3: I'm having a great time here.
Oh sorry, I was supposed to.
All right, let's try that one more time actually.
I forgot to switch languages.

All right.
go ahead Asher.
[174:36] Speaker 4: Hey, what do you think about Austin so far?
[174:41] Speaker 3: Uh, mujhe abhi tak bahut accha lag raha hai.
[174:45] Speaker 4: Nice, I like it pretty too, pretty well too.
What do you think about the weather here?
[174:53] Speaker 3: uh mosam thik hai, pichle hafte behtar tha.
[174:58] Speaker 4: All right, and as you can see, we can also click on the numbers here and hear the translation.
So, I'll try it here.
Uh, the audio is not working.
Uh.
Let's go back.
All right.
Like uh, it's it's supposed to be on headphones, all right.
Okay, we'll do MacBook Pro speakers instead.
okay, let's see if it works.
[175:39] Speaker 4: acha mujhe ye bhi bahut pasand hai bahut acha bhi yaha ke mausam ke bare mein aapka kya khayal hai
[175:49] Speaker 3: and then he said, and I said.
[175:51] Speaker 4: the weather is fine.
It was better last week.
[175:55] Speaker 3: So, you can also adjust the speed and uh yeah, that's about it.
Thank you very much.

[176:18] Speaker 2: So, how many people here have heard of Gathertown?
If you've heard of Gathertown and then you're going to like this next project, it's called Hacker House where AJ is bringing AI agents to your virtual office.

[177:11 - 177:15] (Silence)
[177:15] Speaker 5: Hello Gantlet AI demoday.
How's it going?
I'm AJ and I'm here to tell you about Hacker House AI.
So, I think the group here is probably a little bit more familiar than most with the sort of tension between remote work and in-person office work.
Some people have really, really strong opinions about why one is purely better than the other, but I think a lot of you guys know that the truth is more nuanced and there's actually tradeoffs between remote and office.
You actually want kind of the benefits of both remote plus office work.
You want a remote office.
And there is a product that already kind of solves for this.
It's called Gathertown.
I used it at a previous company.
Um it's pretty good.
It's like a short little uh like video game world where you can walk around and talk to people, and it does actually do a good job of um bridging a bit the gap between the in-person experience and remote.
But I actually think we can do better than Gathertown because after all, it's not 2020 anymore when they took off.
It's actually 2025 and we have AI now.
And that means two things.
It means we should be able to as an individual developer get a product like this up and running in about two weeks.
And we should also be able to have AI agents in this world.
It's kind of a no-brainer.
And so you want AI agents that can be able to talk in person, that can interact with MCP servers or APIs to like do interesting things in the the virtual office environment.
And then that really will like connect you and your AI agents to the broader world in in a virtual space.
So, that's where Hacker House AI comes in.
Hacker House AI helps you get the best of both worlds for the in-person and remote work.
It helps you keep the culture alive even if you have like a globally distributed team of top talent, which is kind of what's required to get the top AI builders right now.
And then it helps you uh allows for interactions between agents and people seamlessly in a way where there's not like a huge divide between those two things.
So you can imagine this for offices like Gathertown.
You could also have it in a school context or even just meetups, any place where people and AI agents connected to another service might meet.
So just to show you a bit of what I've built the last two weeks, here's a demo from earlier.
So when you enter, you're in the public chat context where anyone in the workspace can talk to each other.
You can see who's online, what's going on.
But if you walk into a conference room, it'll immediately switch to the private meeting room context for that meeting room.
And if you walk up to somebody, your DMs open, seamlessly, a very smooth experience.
You might notice that not everyone in this workspace is a human person.
There's also bots, and you can walk up to them, and you can ask uh requests such as this one where I'm asking it to tell another coworker in the office space about uh commit on one of my other repos.
And it tells me it got the message before using a tool to walk over to him.
And then from uh his perspective, the second half of the video, you can see it got that info and told him about it.
So, walking you through what just happened, we have a person who's talking to a bot, the AI agent, that has access to three different tools.
First it's using Webrag to actually get data from the internet live on GitHub outside its training window.
Then it's using our tool to insert itself and walk around this office space.
And then another one of our tools in order to initiate rather than merely receive messages from uh other people in the office, um who then get it.
And you could imagine this interaction happening not just with people, but it could also talk to other bots.
And I think that's really interesting that like um not only can it talk to people or bots, it also can access more than just GitHub, right?
You can add notetaking, educational content, um coding tools, other chat services, data analysis, or even some of the video agents that people have built could have a bot presence in in a site like this.
But the most interesting thing is when you have multiple AI agents like this, right?
And they can all.

--- Segment 19 (Starting at 180:00) ---

[180:00] Speaker 2: all talk to each other on Hacker House AI and collaborate um in a place where they're interacting not only with each other but humans that then can see what's going on and interact with them as well, super seamlessly.
And that's why I'm really excited to have um brought this to you today.
Thank you guys.

[180:47] Speaker 3: All right.
Next up, we have Paul who's going to be going over Embedding Bridge.
Embedding Bridge is going to be the easiest way to manage your vectors embeddings over time, and we're also going to be setting up for a surprise as Paul after Paul's presenting.

[181:42] Speaker 4: Hello everyone.
I'm Paul Jermal and I'll be presenting on Embedding Bridge, a gift for embeddings.
This came about when I was trying to define a recipe data set that was pre-embedded.
Obviously, I looked at hugging face and I was curious why I see all these data sets but they're not pre-embedded.
There isn't a public repository or place where you can just find embeddings that are already pre-embedded and any sort of provider, be it open AI or Voyage Voyage 2, which is in the embedding model that claude or anthropic uses.
And so I had to embed them all these thousands of documents in half an hour.
And I realized, well, there should be a site or a place to have all these embeddings hosted, so we can save costs and time.
And I realized that the next step would be in the basic rag pipeline to create sort of this embedding bridge.
And that to connect like a glue between the documents and the vector DB.
So you can manage version your vector embeddings across any provider, right?
You have so many documents and your documents could have multiple embeddings.
Being able to version them is probably pretty important, right?
So this is sort of like the glue between the vector vector DB and documents.
So we can pick any sort of vector DB.
And so I will be highlighting few of the commands that I created in this command line interface in C.
One is EB roll back.
And EB roll back, the main point of this is that you have models, multiple models and multiple histories.
And you can roll back from your current to a previous vector embedding for that document.
So I'm going to give you a quick sort of run down and may be flying fast, but keep your eyes peeled.

And so we're initializing.
We're registering some models.
We're restoring.
And don't worry, there are Python bindings for all your data science out there.
We're not fami not uh not loving the CLI as much, but and so we're seeing a log.
You can see for Voyage 2, I'm rolling back to a previous history and you can view that.
Next is EB diff.
We're going to analogous to, so Git diff.
You guys are aware of that.
And I was thinking, how can we do this for vector embeddings?
And I was like, similarity, right?
Similarity between vector embeddings.
So here I'm I'm trying to showcase sort of preliminary of how that would work.
So, within the Open AI 3 small embedding models, I'm I'm showing the the diff between um it's showing the cosine similarity and you clean similarity as well.
So EB remote.
Remember, these are vector embeddings, and vector embeddings can be large, right?
They take up a lot of your space and you want to just manage them, obviously version them, but also put them out in the cloud where you could then maybe plug them into other vector databases.
So, what I'm doing here and the capability that it's available at the present moment is to be able to push them into S3 at the moment.
So I'm here I'm push pushing to S3 and it's sh showing up as a list.
And we're pushing, Bam.
And then we can load them onto your vector databases.
Okay?
So, as you can see, I'm I'm loading them from the the S3 path onto the pine core vector database and it's all loading up.
So there's a specific schema going on here that you can put them in to the to the the remote, the cloud end, and these could be then pulled into any sort of vector database.
So just a one stop shop to keep all your vector databases, not worry about the the vector databases at all, just manage them all and that that is the goal with this.
And I I'd like to give out some of the command lists that I have available here.
Um, one that I would like to keep uh show show off is EB hooks.
Think of this, man, like if you have get documents and you're ragging across them and you make a change to a document, you want to be able to generate an embedding and keep track of those embeddings for that document.
Like a get commit, and boom, generated embeddings.
So this is an aspirational goal of mine is being able to make it easier for developers to manage version their vector embeddings.
So, here is a GitHub repository.
Uh, go ahead, take a screenshot, take a look.
I have Alpha V2 out, and and I would love your feedback, your issues, okay?
This on the CLI, I have Python bindings, and I'd love the developer feedback.
Um, thank you, uh, thank you for listening to my presentation on Embedding Bridge.

[188:18] Speaker 3: All right.
For the next part of our program, we're going to start a live speed build.
Now, you must have seen Austin's tweets and you must have seen how he says that people are building things in in 10 hours, in 24 hours, in 36 hours, but it's a different story to see it happen live.
We're going to bring on three contestants and this is going to be our Gantlet AI speed build challenge.
Each contestant is going to have a hype man.
The hype man's going to tell you what AI tool are they're using, cursor or wind surf.
They're going to tell you what models they're using.
They're going to talk about their process and they're going to let you know how they can build so fast.
So let's welcome our contestants.
Abraham, Robert, Campbell, Anthony, Spencer, and Benji.
So let's set up station one.
Station two.
And then grab your mics.

Grab a mic for yourself.
Uh, oh, Anthony, this is your mic.
All right, so let's look at the setup here for just a second.
Each of the hype man is going to be watching the screens of the competitors.
They're going to let you know what's going on and they're going to give you insight into what's happening.
Now, before we get started, we need an idea for an app they can build live.
Now, I want the audience to know that we did not plan this.
In fact, we told them like 10 minutes ago.
So, let us know an idea you think that you want them to build right now.

[190:23] Speaker 5: So content game where you can play on your phone and you can see what other people are playing.
Is that kind of right?
And then you can see the card of what everybody's doing?
Oh, cards are getting.
Okay, so like a card game like cards against humanity that you can play on your phone.
Okay?
All right, so that's going to be the idea that these three competitors work on.
Let's put the timer up.
So yeah, I'll give you.
So we have 10 minutes.
10 minutes to see if they can make something like cards against humanity for the mobile phone.
And what I'll be doing is going through each of these contestants and we'll be walking through what they're doing, how they're doing it, and the approach they're taking.
Then we'll have them come up to the podium and then we'll have people judge.
Ready?
Five, four, three, two, one, go.
[191:38] Speaker 6: All right, Team Robert over here is starting with Wind Surfing.
Unique choice.

Team Campbell appears to be using cursor.

[191:59] Speaker 7: Anthony, let us know what uh Abraham's doing.
[192:02] Speaker 8: Yep, we're going on we're on cursor as well right here.
What models are we using?
[192:08] Speaker 9: We got Claude 3.7 thinking going on right here.
We'll see if he changes.
It's still early.
[192:17] Speaker 10: Over here we have Claude 3.7 not thinking, just the original Claude.
It seems like he also made the boy the bold choice of using Bun instead of Nodejs.
[192:27] Speaker 3: So it it I think it's going to be have to be a mobile app.
So what are the mobile app approaches going on?
Is it a PWA?
Is anyone coding natively?
Are we just making a web app that's responsive?

--- Segment 20 (Starting at 190:00) ---

[190:00] Speaker 2: For the audience he said he's going for a PWA.
[190:05] Speaker 3: Yep, we've got a responsive web app going on here.
[190:10] Speaker 4: Yeah, we're going to go web up over here, it looks like.

[190:15] Speaker 5: Building for mobile, the bottleneck is still uh deployment. There's a lot you have to do to even get it running on a device. So that will alone take 30 minutes, but actually creating the mockup will be much shorter.
[190:32] Speaker 6: Honestly a good choice. Apple kind of holds you in a bottleneck using X code, you know, it's it's even just set up alone, it's it's kind of tough. Curser makes it really easy though.
[190:43] Speaker 7: Move the mic closer here.

[190:46] Speaker 5: All right, we've got our first prompt fired off asking for a detailed PRD for this mobile app.
[190:56] Speaker 6: Campbell might be pulling ahead because he's already fired off multiple prompts.
[191:02] Speaker 8: What do his prompts say?
[191:04] Speaker 5: Good question. Right now there's a lot of text from the LLM. Let's see the prompts. We're pivoting to mobile optimized game. Mobile can be a challenge for curser, but I think he'll prevail.
[191:24] Speaker 6: Over here the strategy is we're building a PRD in a markdown file to kind of set it up for the AS to follow along, it looks like. So instead of going right into building, we're first we're planning it out with the AI first.
[191:40] Speaker 5: Yeah, we're being humble, you know, we're letting Claude think for us. These guys think that they can think better than Claude, but I don't know.
[191:49] Speaker 7: All right.
[191:50] Speaker 6: You got to tell us a little bit about the prompts here. I mean, that's what everyone's trying trying to look at. Like what are these people doing that they don't know?
[192:11] Speaker 8: Yeah, bring some of the prompts as they come in.

[192:15] Speaker 5: Right. So the first prompt we fired off was more or less the prompt we were given. Uh, let's see. Yeah, build a running responsive mobile app like Cards Against Humanity, mock up multiple users and have it runnable in the mobile browser as well as desktop, make it beautiful, make it delightful and pull in great content from the web. Start by writing a PRD. So we got our PRD and then we asked for let's see. Yeah, make sure other users can join for multiplayer and build it and run it. So we'll see where we get.
[192:54] Speaker 6: This prompt says, should we remove the sidebar, should we change it? So not only can Claude write the code for you, but it can also help you design the UI.
[193:05] Speaker 8: What's it running on?
[193:08] Speaker 6: So we are we're on the PRD right now and looks like he's making some edits on different aspects of the CSS and he is going to say, I see we already have a Nextjs product set up with Typescript. Let's check the package Json to see if we have all the dependencies we need.
[193:34] Speaker 5: All right, we got Veeding right now.
[193:41] Speaker 6: We're about to hit the six minute mark. Six minutes in, uh four minutes in, six minutes left to see who can make a Cards Against Humanity clone to see if there can be any multiplayer game that you can make in just 10 minutes.
[194:00] Speaker 9: We're about to fire off another prompt. Campbell is furiously prompting over here. Why is the game called cards against mobile while our simple core gameplay loop, we have five minutes left. So looks like cursor does not realize the gravity of this situation. Campbell is telling cursor to hurry up.
[194:24] Speaker 9: Campbell is in the zone. He's just looking at the screen right now. Oh, there's a next prompt. We have a console log error.
[194:37] Speaker 5: And over here Claude has decided to try a different approach, something that it does quite often.
[194:44] Speaker 6: Over here we're iterating through the PRD right now. Cursers is on an absolute tear. He's on probably on his seventh or eighth file right now. is setting up the entire project based off the PRD that we've given it.
[195:00] Speaker 9: Now, each of these contestants have honed in on their AI coding framework. It's something that we emphasize at Gauntlet. Everyone's going to have their own methodology. Everyone's going to have their own way of coding with AI, and if you can hone in and do it in 10 minutes, you can definitely do it in 24 hours.
[195:23] Speaker 5: Yeah, right here our project structure has a client folder and a server folder, so I can see it's setting up for multiplayer right now.
[195:35] Speaker 9: I cannot imagine the stress these coders are under. I'm so glad I have this job instead of coding.
[195:41] Speaker 5: Can you imagine the stress that Claude is under working on three at once?
[195:50] Speaker 6: So we have our first website up. It's looking a little basic. We're going to start adding more functionality. Make sure everything's clickable here too.
[196:00] Speaker 5: Index.ts is coming up. It's a good sign.
[196:05] Speaker 6: Now we're taking screenshots, feeding that into the AI so they can get a better idea of what we're seeing on the front end too.
[196:13] Speaker 9: We have another prompt coming through. Is this the best next step for the final four minutes of the challenge. So it looks like Campbell is once again trying to make cursor realize that we're on a time crunch.
[196:40] Speaker 9: Now, has anybody done uh the game logic yet? Come uh let let me know Hypeman, like is anybody working on this cards against humanity app and like how the game is actually going to function?
[196:59] Speaker 5: I mean, not much. The thing is when you're starting what did you say?
[197:03] Speaker 5: Oh yeah, Claude's working on it. That's the thing is when you're doing something like this and you're trying to make a full app in 15 minutes, you just want to see what Claude comes up with. So you give it the idea and yeah, I can see Claude making all the game logic, but it's got a lot to do. It's got to set up the server, it's got to set up the pages, all the logic and everything. Any sort of tweaks are are going to happen after the 15-minute mark. So we'll see what Claude gets in this 10 minute time.
[197:38] Speaker 6: Over here too, we're working on the front end to make sure that everything's looking good so that we don't have to kind of mess with stuff on the back end. It's really a pain when you set up all the logic and then you have to add a button, take away a button, change routing, stuff like that. So we're setting up the front end here first.
[197:55] Speaker 9: Is there something unique that each of the contestants is doing? Maybe like uh some of the document documentation they used or some of the ways that they're interacting with cursor. Can we give some insight to everybody watching live as to things they can pick up?
[198:14] Speaker 5: There's no time for documentation in a short challenge like this. That's why Campbell is sticking completely to the essentials.
[198:27] Speaker 6: Big thing we're using here is synthetic prompting. kind of similar to you might hear synthetic data. Again, we're having the AI generate the prompt itself and then going off of that.
[198:37] Speaker 5: All right, this is huge. Claude says that he's going to set up and run our cards against digital humanity game. So we're about to see what we get right now.

[199:07] Speaker 9: Campbell has run into a couple of errors throughout the challenge, but he's taking these in stride. Claude is seeming to handle these errors pretty decently and by the end of it, it should be a decent product.
[199:24] Speaker 6: Looks like Claude's going to rewrite the entire piece of code. We're running into a little bit of an error here.
[199:29] Speaker 5: Well, Claude told us that it was going to build it and now we're looking in its thoughts and it says it needs to continue building. So we'll we'll see if it churns it out.
[199:42] Speaker 6: Two minutes left. It's getting intense.

[199:48] Speaker 5: I see a pretty nice UI here. I wonder how the functionality looks.

[200:08] Speaker 9: All right, it's about two minutes, so we're going to start seeing some of the things that they've built. See how far they've gotten in about eight minutes and then hopefully we can examine some of their prompting.
[200:30] Speaker 9: Campbell's using a very advanced prompting trick. He just told Claude in all caps to think deeply.
[200:45] Speaker 8: I don't do any thinking.
[200:47] Speaker 9: He just said I don't do any thinking for the audience.
[200:49] Speaker 5: We have Robert screen up here.
[200:53] Speaker 8: Thought process.
[200:57] Speaker 9: And we need to change light mode so you can see properly.

[201:17] Speaker 5: All right.
[201:18] Speaker 9: All right, so Claude is saying, I need to continue building Cards Against Humanity game. Let me update the app file with our game components and then proceed to create the necessary components.
[201:37] Speaker 6: So we've got functionality going in over here as well. We're starting to get buttons to show up and drop downs as well for setting up the game.
[201:49] Speaker 9: Looks like the only thing holding us right now is going to be the actual uh LLM, not these contestants.
[202:00] Speaker 9: All right, let's let's disconnect and see where the timer's at.
[202:06] Speaker 9: 20 seconds left.
[202:10] Speaker 6: We're getting our last prompt. We're using all caps, use Shad CN for the components.
[202:18] Speaker 5: We're just trying to get rid of Linter errors right now.
[202:22] Speaker 9: 8 7 6 5 4 3 2 1. All right, let's give it up for our contestants.

[202:38] Speaker 9: Building an app in 10 minutes is not an easy feat. No matter how far they've gotten, they they were quicker than you. All right. So what we're going to do is we're going to get up to the podium and we're going to showcase some of these applications and the crowd here at Gauntlet HQ is going to let us know who won. Simple? All right, we're going to start with Abraham. You want to connect it?
[203:04] Speaker 5: Sure.
[203:06] Speaker 6: We're we're building right now. Let's see if we can even show it.
[203:10] Speaker 9: And you know, since Claude is doing something, we're going to we're going to let their last prompt finish. It's only right, right? If the LLM is the thing that's slowing us down, we got to let that prompt complete and he'll get to showcase what he has.
[203:24] Speaker 5: Skill issue.
[203:27] Speaker 9: Oh, Spencer.
[203:29] Speaker 10: Woah.
[203:30] Speaker 5: I'm confident in our team.

[203:44] Speaker 5: All right.

[203:55] Speaker 6: Sure. So here's here's what we currently got. We got some game set up here. The problem we're having is

--- Segment 21 (Starting at 200:00) ---

[200:00] having it, everything was going to the top of the page and that's why we had that last last prompt saying to just use Sha's end. Um, and so, looks like we have able to set up happens if we hit start a game? Still dead, actually. Let's Let's try and go back. And so this is what we have for the front. So, everything's kind of on the on the top, it seems kind of unusual, but already 9,000 players online, though. Can you look at that? All right. Let's give it up for Abraham. Next up, we have Robert. Oh, our prompt still rolling. We're on we're on minute eight of this prompt. All right, so maybe we'll give it Maybe we'll give it over to Campbell, see what she has. Here in a second, Campbell will be showing off his app. Go ahead and show it off in the best way you see fit and I'll just be here to announce what's happening. We have We have Cards Against Mobile. We have Cards Against Mobile. I have literally no idea if it works. Let's do a live test. I gave him instructions because it's been a while since I've actually played it, too. It says, read the black prompt card at the top. What's the next superhero movie about? Select a white answer card from the hand. Does anybody have what they want to pick? We have it Let's try it out. That's a nice animation there. We have nice hover animations to make it beautiful. Now, the judge picks the answer. I have to judge these four cards now. Does anybody want to be a judge? I did not verify this. Looks like Claude got creative with its cards. Then we have start a new round. I don't know how many rounds there are. I'm just going to skip through it. Hopefully, it's not too many. This is looking like some incredible functionality and user interface design. How many rounds is there? I don't think it's counting because this should go up, right? So I think it's I guess it doesn't have the rounds. It just has the play. And that's it. Good job. Let's give it up for Campbell. He has the gameplay working, the cards are being generated. You can see the players are playing with you, and he has the judge and the player selection down. Let's give it up for Campbell, everybody. All right, our final contestant, Robert. Even after that demo, Campbell is a little bit self-critical. He's saying there are still things he could have done better. But I think it was incredible. How would you improve, Campbell? I'll give at least another one to two minutes to actually verify it. Like, testing is not fun, but it's critical because it has the core functionality, but if it doesn't work in the end then if someone has the end-to-end functionality working, like uh Robert, then I would consider that a win, if it actually works. Well said. All right, well Claude says this works. Yeah, Claude, yeah. Claude says this is running and it tests on the command line, but when I run the actual page, um, get nothing, so I don't know. That's uh I have a like, it was doing all kinds of multiplayer stuff and all It's grand, it would be fancy. Yeah. That's the funny thing about doing these speed builds. Is there's a whole app in there, but if you can't get it running, it's not impressive to show you guys. And in 5 minutes, we could get it running and uh we'll see a an entire app that's hiding there. Yeah. Um Oh. So, uh do you want to plug it in? So we're going to plug it in here and show the blank screen and he's going to come back and actually show you the application in 5 minutes. But I think the audience would agree with me that the winner is clear. Let's give it up for the winner of our speed build challenge, Mr. Campbell Cow, ladies and gentlemen. Now, it takes a lot of guts to get up here and to just start building in front of a 7,000 people. So please give it up for all of our contestants and please give them a hand for giving up, getting up here and trying. They're going to be posting this application on their X account completed for you in the next 10 minutes. So if you want to see that, feel free to follow Joint Gauntlet AI. All right, thank you guys. All right, we're going to be doing one more round. So, I'd like to bring up our next set of contestants. We have Lamar and Gary. We have Jared and Jefferson, the Lambert brothers. And we have Brian and Joshua. Hello, hello, hello, hello, hello. Mic number one. Hello? All right. So, we have a request from the X audience that we're going to be taking. And that request is to rebuild the app that we all hate, Jira. So, we're going to be taking 15 minutes on the clock and we're going to try to see how far they can get to build Jira. Now, each of these individuals have their own hype man, similar to the round we had earlier. They'll be letting us know what prompts they're using, what tools they're using, what process they're following. At the same time, we'll connect some computers to the screen so you can see the process that they're going to take. Does everybody understand what they have to build? I I would just I would just like to clarify that this is very much not possible, so we can make you a nice page. A nice page coming right up. Skill issue. Granted. All right, let's go. Ready? When do we start? All right. Are we all ready? All right, Jefferson. Let's get the timer on the screen. It's going to be 15 minutes. All right, 5, 4, 3, 2, 1 speed built. All right. I know some of us are not as familiar with Jira. I think there are two one or two contestants who have never used Jira. Um, Jera is just going to step into cursor real quick and uh he's been taking a quick look at Jira, what the different offerings are. Um, and we're just getting the first steps of the project set up. So Lamar doesn't type anything ever when he writes his code and I'm really surprised that when he was whispering while he was talking, it actually still worked. It's really interesting. I love what Brian's doing here. He just straight up told Claude, hey, we're building Jira, do it quickly. All right. Yes, so Lamar is often uses uh Chat GPT. He's currently using O3 Mini High for this PRD. Looks like Brian's got Claude 3.5. Low R is now putting into Claude. And why? Tell us tell us about some of their prompting. What are they saying? Uh right now, it's hard to tell what I'm doing, but I'm actually using

--- Segment 22 (Starting at 210:00) ---

[210:01] Speaker 2: um Claude by Anthropic um to build the app.
And I'm actually using another app called Replit to build the app and then I'm also using another app called Manis to build the app.
And I'm also using another app called ChatGBT to build the app.
And so now we're using all four of those.
So at the end I'm going to have four different apps and I'm going to simultaneously while I'm waiting for one prompt, be working on the next program.

So, this is going to be like kind of hard to follow.
That's why I'm talking instead of Gary.
But um there's all kinds of stuff going on right now and at the end we should have at least one app that works.

[210:57] Speaker 3: Uh Jared just got Claude code running in the terminal of cursor.
This is pretty helpful for being able to look at the code a little bit more closely and make more precise edits using the cursor app while simultaneously making some large uh swaths of editing with Claude code.
Um and it's rocking and rolling, it's going to be the best one.
Don't worry.

[211:24] Speaker 4: It looks like Brian's a little old fashioned.
He's using plain cursor, but it looks like he already has a Jira board up and running.
I see him adding a task to the board and now it looks like he's thanking Claude saying great job.

[211:42] Speaker 5: So Lamar, why do you like Replit?
It seems like it's making some good progress compared to the other ones.

[211:47] Speaker 2: Oh, I like Replit because it's like a good um tool to build fast and the agent is really good.
Replit has done a really good job uh becoming like one of the top uh tools for AI building.
Though it's like overlooked because it kind of started um not as good as some of the others like cursor, but now it's um uh one of the top ones in my opinion.
Uh if you don't have like a required text stack because it uses the same text stack every time.
Uh well, almost.

[212:22] Speaker 5: Yeah, it has a different default than some of the other platforms.

[212:25] Speaker 3: One of the most important steps when using AI to build is making sure from the get go you have a pretty good set of uh requirements for your app and going over that as well.
So right now Jared is taking a look at a bunch of different things as far as Jera is concerned, making sure we've got the right pages and the right way to prompt uh so that those pages get created correctly.

[212:51] Speaker 4: I would also like to point out that earlier Brian said that this was impossible and just 30 seconds ago he told me, oh wait, actually this is definitely possible.

[213:05] Speaker 6: So It's only been four minutes so far.
So Brian, let me know why you changed your mind.

[213:09] Speaker 7: I uh did not know what Jira was and I skimmed the Wikipedia page and it read a lot more complicated than how it actually is.

[213:19] Speaker 7: So.
I'll just make it look nice for the next 10 minutes.

[213:24] Speaker 6: All right.
Lamar, where are you at?

[213:27] Speaker 2: Uh one of the four programs has made like a really nice looking Jira app um with ticketing and a dashboard.
Um so it looks pretty good.
So we can actually spend a lot of the time if we want to quit the other ones and just focus on this one.
Um and then we can continue building it and uh make sure that it actually works.
And then we um should be good to go.
But um the progress on, oh, um I I thought people could see it, but yeah, so that's where we're at.

[213:56] Speaker 3: It's looking good, it's looking good.

[214:03] Speaker 4: Brian just told Claude to make the theme dark because that's how he is preferable.

[214:09] Speaker 3: All right, we're about to hit the 10 minute mark and it looks like we already have the UI running.
It looks like that we have Lamar making four applications, Jared's on a console log error, but that's about to go away.
And Brian's already looking at colors and theming for his application.

[214:53] Speaker 3: Lamar may have four apps building, but Jared has seven sub processes running hacking Lamar and he will be shutting down three of those apps at least within the next few minutes.

[215:12] Speaker 8: Oh Lamar, you have to click mirror your screen.
Mirror your screen.

[215:16] Speaker 2: Got it.
All right, cool.

[215:17] Speaker 3: Oh, dope.

[215:19] Speaker 2: So, so this is the first first one that we're building.
Uh it's kind of behind um and we'll see what happens with this one.
Continue.
Boom.
And then uh we'll go to Replit.
So this is the one that's going going the best so far.
So in speed builds, I like to try all kinds of different things.
Um so if you ever check out my Twitter, you'll see like all kinds of different stuff going on with the videos like this.
Um but uh this one's looking good.
So we're going to come back to this one and work on it.
Um and then we have another one going in Manis, which is uh China's version of Open AI's operator.
Um and this one's uh going pretty good.
And um uh this one actually I have the most uh confidence in this one.
So right now we're just seeing the code, but um eventually we'll see uh some cool UI.
Um and then ChatGBT is still thinking or something.
And then um I got bored so I made another one.
So now this is the fifth one with a different program called V0.
And so um in between prompts, it's kind of seems like inefficient.
But in between prompts, I can just easily switch through because each prompt takes like uh one or two minutes to execute and go to the next part.
So instead of sitting here being bored, I just um uh move on to something else.

[216:48] Speaker 3: While Lamar has been bored, Jared has been working extremely hard and building an even better app that looks much, much better.
You won't believe it when you see it.

[216:58] Speaker 4: Stay focused.

[217:03] Speaker 3: Well, how's Brian doing over there?

[217:06] Speaker 4: It looks like he's doing pretty good.
I see Claude is thinking very hard.
Would you mind showing me the app so I can describe it?
Ooh, it looks sleek indeed.

[217:27] Speaker 4: Yep, he's got tickets.
He can move them between boards.
Looks like anybody who uses this app is going to be super productive.

[217:44] Speaker 3: Lamar is trying to figure out how to make ChatGBT show like an artifact.
You good?

[218:11] Speaker 3: All right, we have Jared screen now.
We'll have Jefferson walk through what he's working on and how far he is.

[218:18] Speaker 9: So, the best practice here is to use bun in opposition of NPM.
Uh so we we've got that going.
The app is about to be running in just a little bit here.
Um and we're just figuring out a few things here, but then you'll be seeing the app.

[218:50] Speaker 3: You're pretty impressed with Manis?

[218:52] Speaker 2: Yeah, it's like doing um a lot of end points right now and doing like different things that we probably don't need.
But it's actually going to be pretty cool if we can get it going, get it done in uh five minutes.

[219:06] Speaker 3: All right, let's do a time check.
Where are we at at the timer right now?

[219:09] Speaker 9: 5 minutes and 35.

[219:11] Speaker 3: We're about to hit the 5 minute mark.
And in 5 minutes, what what's going to happen is we're going to have each one of them connect and showcase their applications.
And then we're going to have the audience decide who's the best builder of Jira in 15 minutes?

[219:32] Speaker 5: So often times when you prompt Replit etc, they'll have mock data and not working API endpoints.
So one thing that Lamar is trying to figure out is how do we quickly get a real API endpoint so we can file a real ticket.

[219:55] Speaker 2: No, no.

[220:09] Speaker 9: So we've been getting some wacky errors coming from uh the terminal here.
So we're uh looking at re opening another directory and moving some files over, creating them there.
Uh but we'll have that up up and running in a little bit.

[220:34] Speaker 5: So the cool thing about Manus, which is like Lamar said, Chinese version of Open AI operator.
Um it's basically just been doing its own thing with almost no no intervention.
Or did you intervene at all since you prompted it?

[220:50] Speaker 2: Uh no, not yet.

[220:52] Speaker 5: So Manus has been just going for 10 minutes straight without any 11 minutes straight without any intervention.

[221:05] Speaker 4: So it looks like Brian's just trying to

--- Segment 23 (Starting at 220:00) ---

[220:00] Speaker 2: make it extra useful. He's working on ticket customization for figuring out how to basically add useful fields to your tickets.
Right now he's implementing giving it a title and he's already got priority and due dates and descriptions up and running.

Looks like he's alternating between doing that and working on more of the theme.
[220:00 - 221:45] (Silence)
[221:45] Speaker 2: All right, let's do a time check. How far are we, how much time do we have left?
All right, we're three minutes left. Let's give it up for all of our contestants. It takes guts to get up here and just start coding in front of an audience. Let's give it up for all of them.

And in just a second, we'll have Brian showcase his application. We'll have Jared showcase his application and then we'll have Lamar showcase whichever number of applications he'd like to share.
All right, whenever you're ready.

Still have two minutes, huh? You're already ready to go?
Claude's thinking about it. Okay.

How's Jared doing? Did you resolve that error, Jefferson?
There is a very strange error popping up persistently that is very mysterious. Very mysterious error.
I want everybody to know that when we did this at South by Southwest, we did have a champion and that was Jared Lambert. He beat out all the other contestants live here at South by, so he's defending his title for the speed build competition at Gauntlet.
How's Lamar doing, Gary?
No comment.
Looks like there must be an error there.
No, it's beautiful code.
All right, minute 30 left.
Again, it's very important to understand that each Gauntlet challenger has their own AI programming methodology. They pick and choose the tooling that works for them. They decide if they're going to use voice, they're going to use text, they're going to use prompting, which models they're using, and over the 12 week program, they use that methodology to build these applications and ensure that they can build it quickly, efficiently, and properly.
My computer just died so.
Okay, yeah, you can take the podium. Looks like Brian's computer is dying so we have to do a last minute uh uh move for the charging cable.
All right, 40 seconds left, and Brian will be the first one to present just in case his computer dies.
I would like to clarify that my computer has already died, and I have 33 seconds to see if it uh can come back on and hopefully it's saved.
It's interesting. With Lamar, Mannis has still not produced a single viewable output. It's it's just working, working, working, but may not be the right solution for a 15 minute window. And then Replit still working on on a database.
So we'll see if there's time. 10 seconds left.
Eight, seven, six, five, four, three, two, one! Let's give it up for the challengers of our speed build competition.
All right, so Brian's computer is rebooting, so we're going to start with Jared over here.
All right, we're going to start with Lamar.
Oh, we can't, somebody has to go, all right, here.
Yes, you could. Oh, wait, I got it, I got it. Oh, oh here you go. There you go.
All right, Brian's computer is up and running, we're going to have him walking through the features that he's completed and then we're going to go and see if he built the best version of Jira in this group of contestants. All right, Brian, over to you.

Lamar's like skip all the errors, please just show me something. Please!

All right, Brian, go for it.
All right. Uh so here's my Jira clone, Jira 2, the Jiraing. Um and as you can see we have one very important task, uh robot uprising. Let's put the due date for uh when should that one be due?
2027. All right.
11 25 2027.
And uh love Plyny.
And we can add tickets.
Any any tasks anybody needs done? That they'd like to recall.
Rally the troops.
All right. We'll rally the troops. Sounds good.

Yeah, so we've got uh descriptions, due dates, priorities. I can drag these between uh in progress and done. I think that one's still to do, not quite in progress.
And that's pretty much all I have for now, but 15 minutes in.
Good job, Brian. Let's give it up.
All right, next up we have Lamar.

All right, here we are. So, I did a lot of stuff and we'll see if any of it worked out. Um, this one didn't.
This one's still cooking. So this one's like the best option. Um, Replit, um but right now it's working on auths and a lot of stuff. So I actually would have skipped auth if I was like actually thinking about the time limit because auth takes more than like 15 minutes usually for these automated systems to set up because they try to make it more secure, especially right now since um, some of the CEOs of this uh this these um companies such as the Replit CEO and Vercel are arguing about security, so it's taking a little bit more to uh do security and make sure the apps are secure so they can back up their words. So um, that's where we're at on that one. And then Mannis is still going. So this one, this app is actually really good for like just putting in a prompt and just leaving it, and then coming back like an hour later and then it's a a fully built app. So, on my Twitter there's a a video like this where I actually ran Mannis for an hour and a half and um, I shortened that down and sped it up really fast, but it went for an hour and a half with one prompt and built an app. Um, so it can do, right now you can see there's a checklist here, and it's going through like this whole checklist. So these are all the things that it has left to finish the app. And so um, if we didn't want to build actual Jira and we just said make a simple ticketing system, then um, we could just shorten that down to like five steps and it'll do it. But, if we just leave this going, it's going to build the app. And then um, this one's is empty, so <laughing>. And then we tried uh with uh V Zero. And if I create account and sign up for it, um, this thing will go away and everything will work perfectly and it'll be the best app, but we're not going to demo that right now.
Let's give it up for Lamar, everybody.

So, I was running into some issues today. Uh so Tailwind, in case we've got any web devs in the house, Tailwind recently updated to a new version. And that's actually been really difficult with uh with the vibe coding because uh Claude loves Tailwind version 3. Um so I ran into some build errors with my first project where it set up a ton of styling with uh with Tailwind version 3.
And it just uh kind of dug itself into a whole but one of the benefits of when you're writing uh code this fast is you're not really tied to one project. You can rewrite things so quickly. And so like 7 minutes in I I threw in the towel and I started a new project. And this is what I got. Um, I used Cloud Code for this. Uh Cloud Code is pretty good at initializing like empty databases or em- empty uh empty repos and um, just getting a like a rough, a rough draft up and running. So you can see I I literally made every single single feature.
I'm not going to click on any of these because We don't have time.
But yeah, pretty impressive.
Um, any investors in the building, just uh just give me a call if if you're interested in Jira clone. Thank you. Good job, Jared. Let's give it up for Jared, everybody.

--- Segment 24 (Starting at 230:00) ---

[230:16] Speaker 2: All right, so here comes the moment of truth.
We're going to go one by one and we're going to see what the audience thinks and who the winner was.
To start out, let's make some noise for Brian and Joshua.
How about Lamar and Gary?
What about Jefferson and Jared.

Looks like the reigning champion keeps his title!
Jared Lambert, Gauntlet AI Speed Build.
Let's give it up for all the contestants, ladies and gentlemen.

All right, we have 8,000 people joining us live.
So it's time to call on Austin Allred to walk through some of the things that are going on here at Gauntlet AI and why this Speed Build challenge is so important.
All right, I think we we have an update to one of the apps that was built real quick.
Uh we're going
I think you can use any of the this one?
Sorry, we're making messes up here for all the AV people.
We apologize.

All right, we've got I don't know if there's anything Nope.
I think that worked.
Yep.
Okay, I'll let All right, so just an update from the first Speed Build.
Benji can join me to do most of the talking, if you want.
Um, so apparently all most of the errors we got were CSS errors, that was keeping us from running.
However, when I get to it, I'm still not actually able to play.
But just for kicks, while I was uh building all this in the background in my extra few minutes after the competition, I just thought, you know what, this app that we that my team built in the past few weeks, that we're going to show you later, um, I could actually maybe do this in there.
And so while I was building, or while I was fixing the errors on my speed build, I actually just put it in Dream Up, which is our like our AI game builder, and each one took I did five times, and each one took about a minute to build, and uh each created a working version of Cards Against Humanity.
So, go ahead, take it away.
I'll show it while you while you talk.
Speaker 3: Yeah, that's awesome.
Uh yeah, so it looks like uh Robert and his team built an app that can build this game better than Cursor, which I think is pretty awesome and they did it in what, two weeks?
That's right.
Speaker 3: Yeah, about two weeks.
Pretty incredible.
Speaker 2: Walk us through the functionality.
Speaker 3: Sure.
Speaker 2: Yeah, uh Of Dream Up or of the of the Cards games.
Speaker 3: Okay.
So, um as we'll demo more formally later, uh Dream Up is an AI game builder where anybody without any skill, or any code, whatever, you just come up in with an idea.
So I'll do this one again.
Build a Cards against Humanity clone.
Right?
And then, when it runs, you get a little a little menu of games you can play while you wait, and it takes about a minute, maybe like a minute and a half and depending on the game, and while we're doing that, I'll just show you the other versions of cards that we got.
Cards against Decency.
Okay, so this one put in Benji here and and another name.
And then, I don't even know what I Each one has its own unique instructions.
So I might have to read through the play instructions to actually get these working.
Let me find one that like is more intuitive.
Yeah, so see this uh this one has bot players that have been adding their cards.
I need to pick mine.
So it says, what is it good for?
Yikes.
I'll say growing apparent.
Okay.
All right.
So anyway, that's cards against friends.
There's quips against humanity, slightly different take on it and different UI.
Okay.
Right?
And then, let's see is this one Let's let that cook for a minute.
But anyway, and I haven't even iterated on any of these.
You can also put in improvements and like iteratively build a game while you're, you know, waiting and you get to play your existing version while you're waiting.
Um And so I just did this five times.
Every single time it worked and built me something that I can play with.
Got that's a little risque.
Okay, so there you go.
That's the one we just built live.
Speaker 3: For the record, the audience wanted to make Cards Against Humanity, so.
Speaker 2: Yes, exactly, yeah.
Speaker 3: We need to we need to sanitize this a little bit content-wise.
Speaker 2: Awesome.
Speaker 3: But anyway, yeah, so that's the update on the speed build and uh what you can do with Dream Up.
Speaker 2: All right.
Good job.
Awesome.
Uh thank you guys and thank you to all of the builders who've been uh thrust on stage to build stuff really quickly for us.
Um lunch is just arriving, so we'll be setting that up momentarily.
Um, before I get here, I wanted to point out a couple of things.
First, obviously, speed building is fun games.
It's all super, super fast.
Um, a lot of the important stuff of engineering comes when you have more complex applications, uh, bigger code bases.
Uh, you know, code is not necessarily going to all be one-shot anytime soon.
There's a whole lot of work that goes into building real, stable, deployable applications.
But, I think, uh, you know, that's that's an example to show you if you learn how to wield AI, you know, this is probably intermediate level, not quite advanced level.
Um, it gets very, very powerful.
Um, another thing that is pretty interesting to me, if you talk to the challengers about AI in its current state versus when they joined almost three months ago, each of them will tell you that it is just wildly better today than it was three months ago.
Three months ago, a lot of what they just showed you wasn't possible.
Some of those products didn't exist, many of those models didn't exist.
So a lot of what we try to do at Gauntlet AI is, yes, build, uh, yes, understand enough so that you can layer in AI where it makes sense.
But again, I I can't emphasize this enough, a lot is just continuing to stay on the edge of uh what is possible and that takes a significant amount of dedication and learning.
Um, somebody was saying a new Gemini model came out since we've been talking and I know people downstairs are already trying to build with it.
Um, so that's kind of, that gives you a sense of what we're we're trying to just keep riding that wave at Gantlet AI.
Um, we're going to take a break uh for a few reasons, both for lunch for the people who are here and because X has streaming limits that we're about to run into.
Um, so we'll actually stop this stream and then in about uh two hours, we'll start another stream um for the last little bit of the demos.
Um, for those of you that are here for that part, um uh 2:00.
2:00 is is the the time.
Um then we'll have dinner and drinks and all sorts of time to hang out, meet challengers, talk about whatever is of interest to you all.
Um, and yeah, if you're uh coming remotely, you can check out demodate.gauntletai.com and that will show you all of the projects, all the challengers, some of their history.
Um you can leave feedback for them, you can leave feedback for us.
Um, and then if you're interested in a future gauntlet cohort, uh either as a challenger challenger or a prospective hiring company, uh that is at gauntletai.com.
You can apply for either of those.
Um, so thank you so much and we'll uh lunch is still just getting here, but we'll uh we'll take a break and keep going.
Thank you.

[247:02 - 249:41] (Silence)

--- Segment 25 (Starting at 240:00) ---

[240:00 - 240:33] (Silence)
[240:33] Speaker 4: Euh,
