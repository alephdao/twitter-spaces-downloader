## Part 1 

--- Segment 1 (Starting at 00:00) ---

[0:00 - 0:05] (Silence)
[0:05] Speaker 1: Alright.
[0:06 - 0:09] (Silence)
[0:09] Speaker 1: Um, you know, there are now like five,
[0:21 - 0:24] (Silence)
[0:42 - 0:47] (Silence)
[0:47] Speaker 1: Okay.
[0:53 - 0:57] (Silence)
[0:57] Speaker 1: I'm putting out an experience on that.
[1:40 - 1:45] (Silence)
[1:52 - 2:05] (Silence)
[2:17 - 2:23] (Silence)
[2:28] Speaker 1: water and thing?
[4:03 - 4:24] (Silence)
[4:24] Speaker 1: Oh, uh, now it's a good time to grab a drink, right?
[4:33 - 4:53] (Silence)
[4:59 - 5:04] (Silence)
[5:40 - 5:46] (Silence)

--- Segment 2 (Starting at 10:00) ---

[10:00] Speaker 1: Benji.
[10:01] Speaker 2: I mean, I'll, I'll move out to the center, but I'll, I'll be kind of glued here.
[10:05] Speaker 1: Here.
[10:05] Speaker 2: Let's see, this, I don't think, have you tested it with a phone?
[10:09] Speaker 1: Okay.
[10:09] Speaker 2: USB C?
[10:11] Speaker 1: Yep.
[10:14] Speaker 2: Okay, it'll show up there once it connects, if it connects.
[10:19] Speaker 1: I can run down and get another dongle.
[10:21] Speaker 2: Okay.
[10:21] Speaker 1: Oh, I saw it.
[10:22] Speaker 2: Oh, you saw it?
[10:24] Speaker 1: Do you have to configure it somewhere in here?
[10:31] Speaker 2: Is it showing up?
[10:32] Speaker 1: Says dock connector.
[10:33] Speaker 2: Dock connector. So maybe let's try to mirror.
[10:35] Speaker 1: Oh, there it is.
[10:36] Speaker 2: Okay.
[10:36] Speaker 1: Oh, sweet.
[10:38] Speaker 2: Cool.
[10:39] Speaker 1: No audio, right?
[10:41] Speaker 2: No audio.
[10:42] Speaker 1: Okay.
[10:42] Speaker 2: You're gonna be glued here stuck with the phone, right?
[10:44] Speaker 1: So, you're gonna.
[10:44] Speaker 2: Uh, can I use this?
[10:45] Speaker 1: Yeah, yep.
[10:45] Speaker 2: Okay.
[10:46] Speaker 1: Cool.
[10:47] Speaker 2: I'll use this one.
[10:49] Speaker 1: Okay.
[10:49] Speaker 2: So you're gonna use this but then roam the stage.
[10:52] Speaker 1: Maybe a little bit.
[10:53] Speaker 2: Yeah, as much as I can.
[10:53] Speaker 1: You can't move the.
[10:54] Speaker 2: I don't want this to get disconnected.
[10:55] Speaker 1: Right.
[10:56] Speaker 2: Yeah, I'll be careful.
[10:57] Speaker 1: Can you leave it on?
[10:57] Speaker 2: I just feel safer that's not gonna.
[10:58] Speaker 1: Yeah, all right.
[10:59] Speaker 2: Just leave it on the.
[11:00] Speaker 1: Yeah.
[11:00] Speaker 2: Okay.
[11:01] Speaker 1: Cool.
[11:01] Speaker 2: Yeah, you want to use this one?
[11:02] Speaker 1: Cool.
[11:02] Speaker 2: Just grab it and do it here.
[11:03] Speaker 1: Okay.
[11:03] Speaker 2: Okay, thanks Benji.
[11:04] Speaker 1: Right now?
[11:05] Speaker 2: Yeah, yeah.

[11:20 - 11:26] (Silence)

[11:26] Speaker 1: Benji, also just, just gonna have a tell everybody.
[11:27] Speaker 2: Yeah.
[11:27] Speaker 1: Right.
[11:27] Speaker 2: You weren't going to test it on the screen or anything?
[11:28] Speaker 1: Yeah.
[11:29] Speaker 2: Okay, great.
[11:29] Speaker 1: I'm just having this old, holding slide.
[11:31] Speaker 2: Yeah, yep.
[11:35] Speaker 1: Uh, sure.
[11:36] Speaker 2: Cool.
[11:36] Speaker 1: So we're good.
[11:38] Speaker 2: Thanks.
[11:38] Speaker 1: Thanks.
[11:38] Speaker 2: Yep.

[11:45 - 11:50] (Silence)

[11:50] Speaker 2: All right.
[11:51] Speaker 1: Uh can you hear me?
[11:53] Speaker 2: You're good.
[11:53] Speaker 1: Awesome.
[11:54] Speaker 2: Uh welcome everybody to Gauntlet AI demo day.
[11:57] Speaker 1: I know there's still a bunch of people, uh, excuse me, so a bunch of people coming in, but, uh, we are so excited to have you all and those of you that are still joining and those of you that are, uh, on the live stream.
[12:09] Speaker 2: Hello to you too.
[12:09] Speaker 1: Um, we are very, very, very excited to show you some of what we've got today.
[12:15] Speaker 2: Um, for those of you that don't know what Gauntlet AI is, uh, we're a 12 week program for engineers where essentially on the outside, what we do is take very smart people.
[12:28] Speaker 1: Um, and week one, we shock the system by having them build apps, but not allowing them to write any code.
[12:37] Speaker 2: So our entire goal at Gauntlet AI is to get everybody on the cutting edge of AI, uh, because that slope is pretty vertical.
[12:47] Speaker 1: We think, uh, and I'll just say for, for myself, when, when we started Gauntlet AI, on day one, I was pretty excited about the potential of what could happen if you have people building with AI, uh, trying to adopt the newest models, the newest generative AI, everything as fast as you can.
[13:12] Speaker 2: But I've been pretty blown away and I think that what you will all see in a couple of different ways today is pretty wild as well.
[13:20] Speaker 1: So, uh, to give you an idea, nobody had ever heard of Gauntlet AI about four months ago.
[13:29] Speaker 2: Every single student that you'll see today, uh, could have possibly heard about Gauntlet AI three weeks or three months in about two weeks ago.
[13:40] Speaker 1: Um, so pretty much everybody who is here saw that this was going to happen, dropped everything in their lives, jumped right on board and two weeks later they were enrolled and building and uh, doing all of that as fast as they can.
[13:55] Speaker 2: Um, we're very selective.
[13:57] Speaker 1: Uh, we picked about two or 3% of the applicants.
[14:04] Speaker 2: Um, and then Gauntlet AI, the name comes from our somewhat sinker swim environment.
[14:11] Speaker 1: So every week there's a new challenge.
[14:13] Speaker 2: If anybody doesn't make it acceptably through that week, um, and I set those standards, uh, then you don't move on to the next week.
[14:23] Speaker 1: So the people that you see today, they made it all the way through the Gauntlet and that's pretty exciting for us.
[14:30] Speaker 2: Um, but I want to zoom out just a little bit and talk about why any of this exists in the first place.
[14:37] Speaker 1: Um, there are a lot of different training programs out there, there's a lot of different ways that you can learn different things.
[14:44] Speaker 2: Um, but we have a couple singular focuses in mind, some of which are not obvious, uh, from the outside world.
[14:55] Speaker 1: So the first is, for me, transparently, uh, I want to build a credential that is a stronger hiring signal than anything like a Stanford CS degree or an MIT CS degree.
[15:09] Speaker 2: That's our, that's my main overarching objective.
[15:14] Speaker 1: Now, how do we get there?
[15:15] Speaker 2: That's going to be a long journey.
[15:17] Speaker 1: Um, there's a lot that goes into it.
[15:19] Speaker 2: Um, but for these students, what you're going to see is that they can build things really, that they can utilize the cutting edge techniques in AI.
[15:31] Speaker 1: They all have very unique workflows.
[15:33] Speaker 2: They all have, you know, they've been building, uh, a lot.
[15:37] Speaker 1: You will see that they are very smart.
[15:39] Speaker 2: Uh, we select for that very intentionally and you'll see that they are very hardworking.
[15:46] Speaker 1: Um, the generally speaking, uh, people show up in this office here at 8:00 a.m., uh, 8:00 a.m. Central every day and they're here until midnight, seven days a week, the entire time.
[16:02] Speaker 2: Uh, Sunday starts a little bit later for church, but that's the only, uh, only exception that we've got.
[16:08] Speaker 1: Um, so people have been building 80, 100 plus hours a week and, you know, people being really smart and working really hard, creates a little bit of magic.
[16:20] Speaker 2: But I think perhaps the unspoken thing that all of the the Gauntletters or the gauntlet students or the challengers or whatever everybody has decided to call them, um, the unspoken part of that that I think is the most important is how quickly they learn.
[16:41] Speaker 1: Everybody who is in Gauntlet AI is spending hours and hours each week, learning about and keeping a record of the new things that they're encountering.
[16:53] Speaker 2: They're figuring out how to jump into new spaces.
[16:57] Speaker 1: Um, they're building workflows for learning.
[17:01] Speaker 2: They are, uh, rapidly acquiring expertise in whatever they're building, um, and they're staying on the cutting edge of AI.
[17:10] Speaker 1: Our our thesis is that the the rate of change of AI is basically going vertical.

--- Segment 3 (Starting at 20:00) ---

[20:00] Speaker 1: Um, we're not necessarily, you know, pioneering new methods of AI. We're not doing, you know, fundamental research. Our goal is just to get to the edge of what AI is capable of and ride that wave.
Um, we think doing that is possibly the biggest force multiplier in human history.

And even versus what I expected, it's it is very, very, very powerful.
Um, I've been running schools, code schools for a very long time. Uh, now what I what I see, when I see a project, I have a, you know, kind of background mental process running of approximately how long I think it would take to to do any project, right?
Devil's always in the details, but um, there have been a number of companies that have come in and worked with us where their mental model says that something should take six months, maybe a year, and Gauntlet Challengers are knocking it out in literally 48 hours, 72 hours.

I know that seems crazy and people are going to keep calling me on it and I'm going to keep showing evidence of that.
Um, but roughly, you know, now I have to tweak my thinking so that what I'm used to anticipating taking about a month, uh, is somewhere between 24 and 48 hours.

And we see that regularly.
Um, as one piece of evidence to that effect, um, we don't have enough room on this floor for all of the students to be here.
Um, so I posted a tweet or a post or a Z on X, um, about half an hour ago and the students who are not up here presenting are downstairs on our other floor, and they're going to be building.

Um, and they're going to be building whatever they find the most impressive from the replies to that tweet.
So from right now, they have about five hours to build whatever it is that you're requesting and they're going to come back in five hours and show us all what they've built.
Um, this is a demo day in a couple different ways.
Um, the other thing that they're going to do today and the bulk of where our time will be spent is walk us through the demos of what they've built.
Now, they've been at Gauntlet for about three months, a little less than three months.
Um, the first month is online, the last couple months have been here in person in Austin.
Um, but they started their projects that they're going to show to you about three weeks ago, and most of them had two or three projects running simultaneously.
So when you see something up here, adjust your mental model for that's taken them basically a couple of weeks to build.
Um, it's pretty, pretty wild.
Um, so we're very excited to have you all here.
Uh, for those that are joining the stream, we're excited to uh have you all wherever you are virtually.
Um, and I just want to say thank you to all of the the companies who've made this possible.
Um, I mean, millions and millions of dollars went into making this happen.
Uh, the the students pay nothing.
Uh, the students were all flown here from all over the world.
Uh, they're put up in housing, they have offices, we have buses that run to and from the apartments, food, all sorts of events.
Um, that's only made possible by the hiring companies that hire these folks and fund this all.
So thank you to the hiring companies.
Thank you to the sponsoring companies.
Um, and thank you to the the Challengers.
Um, I think it's a little bit understated how crazy it is for some some people, and especially, you know, nobody had heard of this program three weeks before it started.
We hadn't announced it.
It wasn't possible to have heard about it.
Um, from when we announced it, there are a couple hundred people in those, you know, two or three weeks, who dropped everything, decided to take a risk, uh, decided that it was worth going a hundred hours a week after something, um, at a pace that's mind-boggling and sometimes unsustainable.
Um, but that's, that's not an easy thing to do.
They put a lot of uh hope and a lot of trust in me, um, in the companies that are hiring, and uh they they took a big risk and worked extremely hard.
So, um, I want to say thank you to all those of you who who did that and who did this crazy thing.
And I think all of us to a person are very, very pleased with what the results have been.
Um, so with that, I am going to uh a couple items of business.
First is, uh you can find my tweet.
I've pinned it.
Um, I'm Austin on X.
AUSTEN is my username.
Um, so please reply to that with recommendations or suggestions or requests for what the Challengers downstairs are going to build, and they're going to show you what they can build in five hours.
Um, and now I'm really, really excited to get to the demos.
Uh, so I'm going to have Ash, who's one of our program managers here and's been working with the the Challengers day in and day out.
Uh, he's going to go over some logistics and introduce you to some stuff that in very Gauntlet nature, uh, was built in the last 24 hours or so, um, to to make it a little bit easier to follow along.
So, uh, Ash.

[29:22] Speaker 2: All right.
Hey guys, and welcome again to the 1,000 people watching us on X right now.
So I just want to go through some of the logistical details and some of the things that we have available to you to sort of meet our graduates and understand what they've created.
So at demod day.gauntletai.com, you'll be able to create an account, and you'll be able to see each of the graduates and all the projects that they have.
At the same time, as the graduates get up here, you'll be able to see who's presenting.
Each of the graduates are going to be presenting for three to four minutes.
They'll be going through their applications and then walking through a little bit of their AI first coding framework.
They've been preparing for this for about two to three days now, and the goal of these presentations is to make sure that they can showcase what they can build in just a few weeks, but at the same time, how quickly they can build with AI and build a viable product that they can launch.
So you'll see some people trying to uh launch a product and get users.
You'll see some people showcasing a product that they've built super quickly, and uh how quickly they're able to build with AI and add to the sort of ecosystem and domain.
So, without further ado, we're going to get started with our first Gauntlet Challenger, and we'll be going one by one throughout the entire day.
And as we go through these different presentations, if you want to rate the presentation, you can do so on the website, and if you have any comments or suggestions or feedback for the presenter, you can leave them there as well.
So I want to welcome our first Challenger, Benjamin Visik.

[31:25] Speaker 3: All right.
Good morning, everybody.
Just make sure we're getting hooked up to the monitor here.
All right, we're good to go.
All right, good morning, everybody.
Uh, this is a special day.
You're going to see a lot of cool presentations, you'll meet a lot of great people, and you'll probably forget most of it.
And I would argue that most days something special happens.
Something you want to remember, and these days are even more difficult to remember.
We have some solutions for this.
Maybe people write things down in a notebook, which is very difficult to query.
Maybe you use Apple notes or voice memos, which all have their own drawbacks.
They're built for a lot of things, but not one specific thing.
So my name is Benji, and today I'll be presenting Momo, where we reimagine voice memos from the ground up with AI.
I've got my app set to the action button, so you can open it anytime, and with one tap, you can start recording.
We have some tags up here.
In your settings, you can decide which tags show up on the home screen from somebody you met, a great idea you had, whether it happened now or maybe a dream that you just woke up from.
And of course, our app is localized in Japanese.
But we'll keep it in English for this audience.
And uh let's record something.
Presenting right now in front of a beautiful group of people.
One thing about voice memos is Apple's audio transcription is not so great.
So we use Whisper AI, Whisper API, which allows you to transcribe in any language, and using AI, it's way more effective.
And so let's check it out.
We'll make this a little bigger.
And as you can see, I've already been recording what I've been doing all day, tagging it.
And if I want to go to the filter, we can see who I met.
Awesome.
I realized Nathan was from Tampa, where I used to live.
And uh can't remember where I was talking to Nathan.
Perfect.
We can see it was right here in the office, of course.
So some of us will have a lot of our moments in the same place.
We go to the office every day or we're at home.
But imagine you're

--- Segment 4 (Starting at 30:00) ---

[30:00] Speaker 1: you're traveling. Imagine you're on a business trip.
You want to remember where you were at and when you were at.
The beauty of Momo is that it takes where you were and when you were, so you can always remember everything about that moment.
And the best part is, it's all done in iCloud, which means I can't see your data, uh if you believe Apple, they can't see your data. So it's completely private and with one tap, you can remember everything you've been doing all day as soon as you want to.
So, thank you. I'm Benji. This was Momo and uh thanks for coming today.

[31:17 - 31:35] (Music)

[31:35] Speaker 2: All right. Next up, we have the team Tinyvec. We have Tyler, Matt, and Spencer. Now, Tinyvec has become a popular library across the entire cohort to power local vector databases and RAG based solutions. That's welcome team Tinyvec.

[32:15 - 32:26] (Music)

[32:45] Speaker 3: Hi, my name is Tyler. This is Matt, and this is Spencer.
I set out to create Tinyvec, an open source vector database that doesn't suck.
Vector databases power the core of almost every single AI application you see out there today to provide relevant context to bridge the gap of LLMs and their training data sets.
Tinyvec is unique in the fact that it does not require an internet connection or large server setup to run.
It's loved by developers, especially here at Gauntlet. It's extremely fast and simple to use.

So, why should you use Tinyvec?
As you can see from this bar graph, it's up to five times faster than other vector databases available today.
We think that any serious AI application is going to be required to use Tinyvec.
It offers the fastest search time, the lowest latency, therefore, your customers are going to have the best end user experience possible.

Tinyvec runs on any device, no matter how constrained the resources are.
It can run on iPhones, tablets, desktop computers. Anything that can use a file, Tinyvec can run.
While Tinyvec has only been out for a couple of weeks now, we've already seen some great market adoption with about 3,000 downloads in the Nodejs and Python ecosystem.
This tells us that developers and AI application owners love incorporating Tinyvec into their AI applications.
In fact, here at Gauntlet Demo Day, you're going to be seeing a handful of applications that are using Tinyvec to supercharge their AI applications.
Now, I want to pass it to Matt, who's going to talk about some of the awesome innovations we've been able to see for mobile applications.

[34:51] Speaker 4: Open source models are increasingly competitive with closed source ones.
There is no moat.
The key will be user experience and personalization.
And an important part of that is privacy.

Especially given recent regulations that restrict the movement of user data across borders.
Tinyvec solves these problems.
It runs anywhere, it's so fast, the resource requirements are tiny, and the user's data never needs to leave their device.

Existing local vector databases are slow, complicated to use, and resource intensive.
Tinyvec on iPhone is 20 times faster than the leading alternatives.
Sorry.
Tinyvec runs anywhere.

But sometimes, Tinyvec is the leading option for applications that want to use AI on device, but sometimes you need to expand beyond the user's device, which is why today we're excited to announce that Tinyvec is expanding to the cloud.

[35:57] Speaker 5: Tinyvec cloud is the Vercel for AI deployment for the next million vibe coders. Uh it's production scale with zero hassle, very simple pricing. And essentially, we want to take your and development environment and very quickly make it available on the cloud. With a single line of code change, following the same Tinyvec API, we want to keep things as easy as possible. You set your public URI, badabing badaboom, you have your vector search working for you. To move from Tinyvec, we've been developing locally. It's as easy as drag and dropping a couple files. This means you have production vector search in a matter of seconds.
This completely wipes out a lot of the competitors in the space and with a full platform like this, we're able to do some really neat things. For both technical and non-technical people. We have this clustering and visualization, so you can see what embeddings are closer to each other, where your documents lie on the grand scheme of the black box that is um AI.
And so this pricing advantage is absolutely insane. A lot of these vector databases will charge you for nickel and dime you left, right and center. Tinyvec uh-uh. It's very lean under the hood, so the pricing is very lean on top of it. So up to 10 times cheaper and this is where Tinyvec can go. I mean there's millions of ideas where to take an incredible idea and scale it even further. Um over time, the goal is to become a cornerstone in AI development, especially for this new age of how quickly we need to continue to build things with how great of an API Tinyvec really is.
So that's Tinyvec. I uh appreciate you all coming out. Uh this is Tyler, Matt, and I'm Spencer.

[37:47 - 37:48] (Applause)
[37:49 - 37:50] (Silence)
[37:50] Speaker 2: I'll get this shit up.

[37:54] Speaker 6: Next up, we have a challenger who is using AI assistant to see if we can change radio signals coming across from different stations across Austin. We have Stephen Newman and Gig Valm.

[38:13 - 38:16] (Silence)

[39:24] Speaker 7: Okay, we're ready to go.
So I'm actually working on uh a gifted and talented education platform that does incorporate software defined radio as one of its modules.
Gifted and talented education is probably one of the most important parts of our society but it often gets overlooked. Countries like China, Singapore and Finland, they don't overlook it. They're pouring a lot of funding into these programs and we should probably follow suit if we don't want to get left behind.
So, these kids are capable of learning new technologies. Not just uh a curriculum that's advanced, but exposing them to advanced technology as well.
So, what I've done here is made available some modules, learning modules in radio technology, quantum computing, material science, computer security, robotics, sensors and circuits. And these are available for rental for individual students or entire classrooms.
And the cool thing about them is that they don't require an instructor. They're all AI guided, first principles learning from the very ground up. And here's a an example of the first module here, which is the software defined radio module. And you'll see we've got our signals here on the left and on the right is integrated uh an AI tutor that will walk the students through everything from learning what a a wave is, what a radio wave is and building all the way up to intercepting satellite signals, doing general signals analysis and even launching their own radio station, choosing their own programming and music. So not only is it very educational but that's a lot of fun. This is the kind of thing kids are going to want to participate in. It'll be available for middle school students, high school students and the AI tutor automatically works with whatever level they're already at. So doesn't doesn't get boring, they jump right in.
And I've got a QR code right here, so if anyone knows any students who want to launch their own radio station, then go ahead and uh we'll put them on the list and have a great time. Okay, thank you.

[41:56] (Applause)

--- Segment 5 (Starting at 40:00) ---

[40:00 - 40:04] (Silence)
[40:04] Speaker 1: All right, next up, we have AI powered video generation with splify.me, Scott, Chris and Kevin.

[40:21 - 40:25] (Silence)
[40:25] Speaker 2: All right, how's everyone doing?

[40:28] Speaker 2: Perfect. All right, I'll start while they're getting set up.
[40:31] Speaker 2: Hi everyone.
[40:32] Speaker 2: My name is Scott Gubau.
[40:33] Speaker 2: I'm here with Chris Vitich and Kevin Ma.
[40:37] Speaker 2: We are focusing on short form video content.
[40:40] Speaker 2: So short form video content is a $100 billion dollar industry.
[40:46] Speaker 2: And as this industry has grown, several companies have made video editors specialized for short form content creators.
[40:53] Speaker 2: Uh, but these editors are often difficult to learn, expensive, and quite frankly boring to use.
[40:59] Speaker 2: Um the truth is most short form video content creators aren't trying to make, you know, this super high polished content.
[41:06] Speaker 2: They're trying to make funny, ridiculous, viral memes to share with their friends.
[41:12] Speaker 2: So that's why we created, hopefully they'll get this plugged in a second so we can do the big reveal.

[41:24] Speaker 2: All right, well that's why we created splify.me, a fun, ridiculous, and ridiculously fun and ridiculously easy to use viral content generator.
[41:38] Speaker 2: So we're going to do a live demo today.
[41:40] Speaker 2: So this is the current uh landing screen.
[41:44] Speaker 2: And when you the user logs in, they're taken to a dashboard where they can navigate to the different features of the website.
[41:53] Speaker 2: Now, uh the main feature is the video maker, which Chris will walk through in a second, but they can also access their projects, the media library, which contains a library of commonly used memes that anyone can access and like quickly load into their videos, as well as they can upload their own media.
[42:09] Speaker 2: So now Chris is going to do a walk through of the video maker.
[42:13] Speaker 3: Awesome.
[42:14] Speaker 3: Thanks Scott.
[42:15] Speaker 3: I'll walk through some of the video maker uh capabilities.
[42:19] Speaker 3: We try to emphasize simplicity over complexity uh for this for this uh feature right here.
[42:24] Speaker 3: So I can actually put in an existing uh media resource.
[42:28] Speaker 3: We'll have this video uh explaining mitochondria.
[42:33] Speaker 3: Uh we have a few different options.
[42:35] Speaker 3: We can use modify the video format as well as the split screen uh that's sort of common in a lot of meme formats you might see nowadays.
[42:42] Speaker 3: So we'll actually try that.
[42:43] Speaker 3: We'll put in a popular uh Subway Surfers uh clip.
[42:47] Speaker 3: We have the ability to control the simultaneous audio.
[42:52] Speaker 3: Uh there's YouTube to MP3 transcript, uh but we'll actually do our own text to speech transcript using some of the compelling voices that we have actually available on the application.
[43:01] Speaker 3: So we'll try Donald Trump uh just to try to create a compelling video.
[43:06] Speaker 3: This is a pretty quick process.
[43:08] Speaker 3: We're using Spechify to create the MP3 file and then opening AI whisper to transcribe the VTT.
[43:15] Speaker 3: So we'll get the audio clip.
[43:17] Speaker 3: Uh we'll add that to the video and then we'll add some subtitles as well.
[43:23] Speaker 3: So we'll try the full uh feature right here.
[43:26] Speaker 3: the volume.

[43:30] Speaker 4: So the basic unit of all.
[43:32] Speaker 3: by.
[43:33] Speaker 3: I'll restart just to emphasize.
[43:35] Speaker 4: have a double membrane structure and use aerobic respiration to generate adenosine triphosphate.
[43:40] Speaker 4: Here is a sign.
[43:41] Speaker 3: So you can hear the simultaneous audio as well as the newly generated text to speech.
[43:45] Speaker 3: We have a large number of other features that we want to implement or we couldn't present uh today including AI clipping um and some other image overlays and dynamic AI image overlays as well.
[43:55] Speaker 3: Uh but I'll uh lead this off to Kevin who'll explain our marketing strategy and approach.
[44:01] Speaker 5: Thanks Chris.
[44:02] Speaker 5: So we have a weight list live at splify.me on the internet and we're planning on launching within the next few weeks.
[44:08] Speaker 5: And so during the next few weeks, we're going to be using our platform as our only uh marketing tool.
[44:13] Speaker 5: So we're going to be generating all our own or advertisement using our tool and then uh posting it to the internet and then we'll be launching soon.
[44:21] Speaker 5: And then we're going to be launching with two tiers, uh one core plan with the uh core functionality that you've seen here today.
[44:27] Speaker 5: It's going to be $9.99 a month.
[44:29] Speaker 5: and we're also going to have an additional premium tier with access to video generation models um an iOS app, uh automatic script writing and a lot of other features.
[44:39] Speaker 5: So uh sign up for the weight list now at splify.me and get slopping.

[44:56 - 45:00] (Silence)
[45:00] Speaker 1: All right.
[45:01] Speaker 1: Next up, we have CCO Vibe with Roger, Pat, Roger, Pat, Adam, and Rosston.

[45:30 - 45:34] (Silence)
[45:34] Speaker 6: Okay.
[45:34] Speaker 6: Okay, we are CCO Vibe.
[45:36] Speaker 6: Um our website is CCOvibe.com and you can see our presentation by clicking right here.
[45:43] Speaker 6: Um we are a platform to uh train and uh validate CCO Viber because the future of coding and development is going to be with Viber.
[45:57] Speaker 6: Viber.
[45:58] Speaker 6: Viber.
[45:59] Speaker 6: Viber.
[46:00] Speaker 6: Relax.
[46:01] Speaker 6: Relax.
[46:02] Speaker 6: I'll take over from here.
[46:03] Speaker 7: Earlier this past Monday, we were on a call with a potential client.
[46:09] Speaker 7: For us, selling software is second nature.
[46:13] Speaker 7: We've done it for years.
[46:17] Speaker 7: Confidently, we told him, look, what used to take us three months, we can now achieve as AI engineers in just one week.
[46:28] Speaker 7: Initially, it seemed like he believed in us, but soon we sensed hesitation and skepticism.
[46:36] Speaker 7: The problem was clear.
[46:37] Speaker 7: He didn't trust that high-quality production grade software could really be delivered in just one week.
[46:45] Speaker 7: And honestly, we get it.
[46:48] Speaker 7: From the outside, this speed sounds almost unbelievable.
[46:54] Speaker 7: Well, it's clear to us inside our bubble, the wider world remains skeptical.
[46:59] Speaker 7: This is our big challenge, selling AI first engineering to a market accustomed to traditional timelines.
[47:08] Speaker 7: How do we bridge that gap?
[47:10] Speaker 7: That's exactly why we built CCO Vibe.
[47:14] Speaker 6: Um users can come to our platform and they go through a series of learning paths which are then reviewed by uh their peers.
[47:25] Speaker 6: So that they can then go to folks out in the world who don't necessarily trust vibe coders and say, look, I've built all these things and we're validated by my local peers.
[47:34] Speaker 6: Um they can also make pull requests to the platform because it's open source.
[47:38] Speaker 6: So with that package, they can then go on and say to people like, look, I've built all this stuff.
[47:43] Speaker 6: You can see it, it's great.
[47:45] Speaker 6: Um you can see it all on GitHub and then they become part of this community, which then helps them continue to sell.
[47:52] Speaker 8: We also built a mobile app, which acts as a virtual water cooler environment for all vibe coders.
[47:58] Speaker 8: It's a place where you can show off your work, get feedback, chat with other developers and manage all of your projects.
[48:08] Speaker 8: It also acts as a central marketplace for all developers.
[48:13] Speaker 8: There are a lot of executives out there uh with lazy engineering teams reporting to them who are excited by this new wave of vibe coders who can build things quickly and they're jealous.
[48:26] Speaker 8: They have big backlogs of work they want to get done and they're looking for new avenues to distribute it.
[48:33] Speaker 8: We want our marketplace to be that distribution channel.
[48:37] Speaker 8: Users can log in, view the leads in the platform and pick up work based on their personal expertise.
[48:46] Speaker 8: They can also update their master checklist in the mobile app.
[48:50] Speaker 8: So whether you're vibing at your desk or you're out touching grass, you can keep all of your projects up to date all the time.
[48:58] Speaker 9: And so now that I've received validation from the community, I'm starting to get clients.
[49:02] Speaker 9: I now need to start projects for my clients.
[49:04] Speaker 9: We've integrated GitHub Oauth.
[49:07] Speaker 9: So with one click, you can import your current project or start a new one.
[49:12] Speaker 9: We have an agent which scans your GitHub repository and creates all your documentation from your project requirements stock to your universal checklist based off your current architecture and setup.
[49:22] Speaker 9: To make sure you spend your time vibing rather than having meetings with clients and stakeholders, we've implemented what we call a CCO, Chief cognitive officer, which loads your platform experiences, learning paths and spiky POVs into a LLM.
[49:39] Speaker 9: Clients can

--- Segment 6 (Starting at 50:00) ---

[50:00] Speaker 1: and prospects can now or can give your CCO feedback and feature requests, which triggers an update to your project's universal checklist.
[50:14] Speaker 1: This way you keep the vibes high and the stress low, so you can make more contributions to the community.

[50:23] Speaker 1: We've had a lot of contributions already from the Gauntlet IO group including Norman's Monster Hunter, so you can play that on our platform.
[50:30] Speaker 1: We use form locker and AGA Slot Starters.
[50:34] Speaker 1: Uh we're going to be presenting at the Morpheus Agents Builder event uh in Austin on Tuesday April 22nd and you can scan this QR code to register for that to see what we've come up with by then.

[50:55] (Clapping)

[51:08] (Silence)
[51:12] Speaker 2: All right.
[51:13] Speaker 2: Next up, we have Jacob and Robert.
[51:17] Speaker 2: We're going to be talking about Vision BI and their solution for natural language to SQL.
[51:34] (Silence)
[51:58] Speaker 2: All righty guys.
[52:00] Speaker 2: Um welcome today.
[52:02] Speaker 2: We're here to present to you Vision BI.
[52:06] Speaker 2: Um this is meant to be a little bit beyond uh just a text to SQL.
[52:13] Speaker 2: We are hoping to build a fully integrated AI data analyst.
[52:21] Speaker 2: Um some of the problems that we have uh that I've experienced uh businesses are growing more and more uh data sources come from different platforms, the large volume of data that's hard to manage.
[52:38] Speaker 2: Current BI solutions, they take a lot of time to set up.
[52:41] Speaker 2: You've got require massive teams, it requires a lot of infrastructure.
[52:49] Speaker 2: And one of the big shortcomings that a lot of those tools have is they're very descriptive and not very prescriptive.
[52:55] Speaker 2: So honestly that's where I would come in uh my career as a data analyst, you know, you send out a report, you get it back and or I get back all the questions that come in at the end.
[53:08] Speaker 2: Um so what we have kind of architected uh planned some solutions.
[53:14] Speaker 3: I'm sure.
[53:15] Speaker 3: So, one of the great things that we do here at Vision BI is that you can take data from multiple sources, plug it in and we standardize it in a way that we can store the metadata regarding your data sources and then pull what we need whenever we query the AI agent and cache it in memory and then be able to spit out results based on it.
[53:52] Speaker 3: It makes it very intuitive for users who aren't very tech savvy and it basically replaces everything that you would want a data analyst to do.
[53:59] Speaker 3: So if you want to make charts, graphs, reports, etcetera, um just ask what your monthly sales are.
[54:07] Speaker 3: Um you can ask our AI data analyst and it'll pop it out for you.
[54:13] Speaker 2: We'll take you through a quick demo video real quick the little voiceover.
[54:18] Speaker 2: Um kind of walking through the process and how the platform is set up.
[54:26] Speaker 2: Um so as you log in, you're greeted and it's easy enough to add data sources.
[54:31] Speaker 2: Right now we've built out Google Sheets integration.
[54:34] Speaker 2: We've got uh other database and other type of connections as well.
[54:42] Speaker 2: So whenever you connect data into uh into our platform, it's we're trying to make it as seamless and easy as possible so that anybody can do it.
[54:50] Speaker 2: Um and then as you connect your data, we have AI running that will go in, analyze the structure of the data, the types of uh the data types within each columns uh as well as uh a summary of what's in the table, what's in each column.
[55:11] Speaker 2: So we can see here as we inserted our marketing data, we've got it's analyzed each of the columns that are in there to give uh give context to any later uh access to that data.
[55:28] Speaker 2: And then, as we move into the Imagine screen where the users can ask anything of their data,
[55:39] Speaker 3: so we're asking about our sales.
[55:43] Speaker 2: Um we can see as the agents thinking using a variety of tools and it's able to um find out relatively quickly what the data is.
[55:56] Speaker 3: So here we've asked what our top sales month is and quickly it runs through the tools.
[56:01] Speaker 3: It gets this data sources, it returns the tables regarding those data sources and it runs a query against it that spits out the in natural language the data regarding it.
[56:16] Speaker 3: And then here, we've asked the state in which um the most sales occurred, which requires a multi-table join, brings data in from multiple sources and we spit out natural language regarding that.
[56:31] Speaker 3: And next, we'll ask to generate a table as you would a data analyst in any company.
[56:38] Speaker 3: Um, and we can see how that pans out.
[56:41] Speaker 2: There's voice support as well, which I guess you can't really hear over the demo but so in the case where, you know, we have it on mobile or if you're on the go, it's easy enough to uh get anything spit out.
[56:54] Speaker 2: And so we've got charts thrown out immediately in the chat.
[56:59] Speaker 2: Um and we can iterate on that and you can keep asking more and more questions.
[57:04] Speaker 3: So thank you for checking out Vision BI.
[57:07] Speaker 3: Um we've got a QR code that you can scan to go to our website.
[57:13] Speaker 2: Well, you get the whole, the whole Google sheet, yeah.
[57:15] Speaker 2: So we've got uh more information available online at Vision BI.
[57:19] Speaker 2: You can scan the QR code, um and there's form there to contact us if you have any more questions.
[57:23] Speaker 2: Thank you for your time.
[57:25] Speaker 3: Appreciate that.
[57:25] Speaker 2: Take care.

[57:34] (Silence)
[57:39] Speaker 2: All right, so at Gauntlet AI, we have a tradition of cloning applications with AI and then making them our own and adding our own features.
[57:50] Speaker 2: So next up is Refall, who's taken an app he uses every day, but matched it to his own workflow.
[58:02] (Silence)

[58:17] Speaker 4: All right.
[58:18] Speaker 4: Hello, my name is Rafael.
[58:22] Speaker 4: Um, yeah, we can start.
[58:24] Speaker 4: Um, so at Goly AI, we spend the first part of our uh first half of the program, cloning several great apps in a week and adding increasingly complex AI features.
[58:38] Speaker 4: Um the next week, we cloned Slack, Zendesk and TikTok.
[58:50] Speaker 4: So after over 1,000 hours spent on AI first development, I thought it would be very fitting to close this experience by using cursor to make a clone of cursor.
[59:04] Speaker 4: Let me show you Derser.
[59:08] Speaker 4: It's a uh Visual Studio Code extension with a built-in AI agent to help you write code.
[59:17] Speaker 4: The general workflow is very familiar.
[59:22] Speaker 4: It's uh we have a chat where we can select files for context and type instructions for the agent to follow.
[59:32] Speaker 4: But while it is a cursor clone, I wanted to make two changes to better reflect the workflow I would develop over the past three months.
[59:42] Speaker 4: Um the first is thinking process.
[59:47] Speaker 4: So the prevailing uh sentiment over our cohort is that LLMs work best with really detailed prompts.
[59:56] Speaker 4: Um highlighting where and which uh in which files uh what and which files should be changed, essentially trading cognitive loads spent on research for better code output.
[01:00:10] Speaker 4: We can write this by hand or have a um solution planning step that's completely separate from code generation and that's what happening in Derser.
[01:00:22] Speaker 4: Second is uh checkpointing system.
[01:00:29] Speaker 4: More experienced developers um naturally separate concerns that we're working on to a different LM chats and maintain a relatively clean and reasonable Git history.
[01:00:45] Speaker 4: Derser forces developers to put this problem first by having each chat to be a separate Git branch and ending every agent response with a commit.
[01:00:59] Speaker 4: When all changes related to a single problem are done, user can accept a chat, which is then squashed and merged to a parent branch, making all changes related to a single problem end up in a single commit.
[01:01:14] Speaker 4: Let me show you how it works.
[01:01:23] Speaker 4: First, the uh analysis is done.
[01:01:32] Speaker 4: Then the code is generated, which is which will then be applied to our files.
[01:01:52] Speaker 4: Once the changes are done, Derser checks for errors, then writes a summary and makes a commit.

--- Segment 7 (Starting at 60:00) ---

[61:00] Speaker 1: Let's see it again.
[61:03] Speaker 1: Let me, uh, swipe there, and, um, yeah, so we have a research.
[61:12] Speaker 1: Then we have code changes.
[61:21] Speaker 1: And then they are applied to the files.
[61:25] Speaker 1: Thank you.

[61:32] Speaker 2: All right.
[61:33] Speaker 2: The next challenger thinks that the job search process is completely broken.
[61:38] Speaker 2: So he made an AI to fix it entirely.
[61:41] Speaker 2: Next up is Neils, with Nexus AI.

[62:09 - 62:14] (Silence)
[62:14] Speaker 3: All right.
[62:15] Speaker 3: So I'm not sure if all of you have been looking for a job recently.
[62:19] Speaker 3: In case you haven't, I'm here to remind you that job search sucks.
[62:27] Speaker 3: Okay?
[62:27] Speaker 3: If you need me to be a little bit more specific about that, I can.
[62:35] Speaker 3: So I asked my friends who are currently looking for jobs, and they are complaining that they see exactly the same jobs every day, even though they already decided they don't like them.
[62:49] Speaker 3: And there are hundreds of people applying all for the same jobs.
[62:53] Speaker 3: Everyone is asking for AI skills.
[62:59] Speaker 3: And I needed to see this disaster for myself.
[63:01] Speaker 3: So I went on my LinkedIn, looked at my top recommended job, and it was an unpaid internship, even though I have three years experience in the field.
[63:12] Speaker 3: So clearly there's something wrong there, right?
[63:15] Speaker 3: And I think what's fundamentally wrong is that job boards try to predict your future from your past without knowing what's going on in your head right now.
[63:29] Speaker 3: And the solution for that is an integrated career companion.
[63:37] Speaker 3: So we have AI career coaching, interview coaching, which develops a relationship with the user and grows the context about the user, knows what's in your mind, what plans you have for your career at any given time, and it gives you job recommendations based on that.
[63:59] Speaker 3: So you will only see jobs that you actually want, that are attractive to you.
[64:05] Speaker 3: At the same time, it's also a filter, so there's not 500 people applying to every single job.
[64:13] Speaker 3: Um, so the platform is up and running right now.
[64:17] Speaker 3: And the main elements that we have is the career coach, which is a voice AI agent, which was developed in collaboration with a real career coach, so the scripts are all up to the current standards of career coaching.
[64:36] Speaker 3: Then we have a growing context window that summarizes all the sessions that you've done, all the documents that you've shared.
[64:45] Speaker 3: And then you get job recommendations based on all that context, which I'm basically at this point scraping off the web from hundreds of career pages of major companies.
[65:01] Speaker 3: The cherry on top of that is an application agent that turns any job site on the internet into one click apply.
[65:12] Speaker 3: So once I've taken the job off the career's page, put it into my app, I have this agent running in the back end, because it knows everything about you and it has all your documents, it can just go to the job and apply for you.
[65:24] Speaker 3: And that's it.
[65:26] Speaker 3: It's all in one for basically anything you could want in your career journey.
[65:36] Speaker 3: Um, so there's a couple people working on this right now, but I think I have the best solution on the market, because this is going to scale with intelligence, right?
[65:50] Speaker 3: I'm using AI models at all the crucial parts of my application, and as they get smarter, my application will get better.
[65:59] Speaker 3: It's going to be hyper personalized as well, and I'm owning the context window, right?
[66:06] Speaker 3: The more time you spend with my AI agent, the less likely you're going to be to switch, because I have the data that leads me to give you the best job recommendations.
[66:19] Speaker 3: So why would you?
[66:22] Speaker 3: Um, so the app is live right now, it's at NexusAI.careers.
[66:27] Speaker 3: I have about 80 users signed up right now, and actually my phone just vibrated, which means I got another one.
[66:34] Speaker 3: Um, yeah.
[66:35] Speaker 3: So I would be super excited to to get a lot of feedback.
[66:40] Speaker 3: I'm already getting feedback from users every day.
[66:43] Speaker 3: Um so if you want to sign up, let me know how you what your experience with my agents is, that would be amazing.
[66:51] Speaker 3: Thank you.

[66:57 - 67:06] (Silence)
[67:06] Speaker 2: Now, have we ever traveled to a city, wanted to see the sites, but we didn't know where to go, or the history behind what we were looking at?
[67:13] Speaker 2: Well, the next solution is meant just for that.
[67:16] Speaker 2: We have Michael with audio guide.

[67:23 - 67:30] (Silence)
[67:30 - 67:41] (Silence)
[67:41 - 67:50] (Silence)
[67:50 - 68:02] (Silence)
[68:02] Speaker 4: Hi.
[68:02] Speaker 4: Imagine you are traveling to Rome for your friend's wedding.
[68:07] Speaker 4: You are staying at the Sofitel Hotel and have a few hours to kill before the festivities start at a venue called Masa Rooftop.
[68:16] Speaker 4: You want to explore the city, but didn't do any research beforehand, nor you have time to book a tour.
[68:22] Speaker 4: This app was built for you.
[68:26] Speaker 4: With a simple prompt Sofitel Rome, Masa Rooftop.
[68:38] Speaker 4: We tell you, you say whether you're interested in history and architecture, let's say, and you have two to three hours to tour, and you're willing to walk within two kilometers.
[68:48] Speaker 4: Google Places API will fetch 15 of the best recommendation location for you.
[68:54] Speaker 4: You want to explore Trefontaine, Coliseum, and Pantheon.
[69:01] Speaker 4: We generate the tour and create an itinerary based on what you're wanting to see.
[69:08] Speaker 4: We save the tour and what happens in the background is we fetch Wikipedia data and Wiki Voyage, send it to Open AI for transcript and then generate text to speech.
[69:28] Speaker 4: So here your audio files are generated, uh in three versions, brief overview.
[69:35] Speaker 5: Sound of gentle water flowing.
[69:38] Speaker 5: Welcome to by Giuseppe.
[69:41] Speaker 4: All the way to in depth.
[69:44] Speaker 5: of gentle water horses.
[69:48] Speaker 5: One For every single location you chose.
[70:00] Speaker 4: Um, also you can look at your itinerary in Google Maps, if you so desire.
[70:09] Speaker 4: And a cool feature is you can change your language preference if you'd rather listen to it in French.
[70:15] Speaker 4: Now, all your other files are going to be in French.
[70:19] Speaker 4: Go back to your tour.
[70:23 - 70:27] (Silence)
[70:27] Speaker 4: Start again.

[70:37] Speaker 4: And now it is in French..
[70:40] Speaker 5: coule doucement.
[70:44] Speaker 5: Bienvenue de nouveau Ã .
[70:46] Speaker 4: Another feature we have is a live agent that you can talk to, right now only in English.
[71:03] Speaker 4: Hi, how high is the Trevi fountain?
[71:06] Speaker 5: Ah, you mean the Trevi fountain.
[71:09] Speaker 5: It stands at an impressive 26.3 meters, which is about 86 feet.
[71:15] Speaker 4: Let's do it for the Pantheon as well.
[71:22] Speaker 4: Hi, can you tell me in what year was built the Pantheon?
[71:27] Speaker 5: Ah, the Pantheon.
[71:30] Speaker 5: It's a fascinating structure.
[71:33] Speaker 5: The Pantheon as we see it today was built around 126 AD by Emperor Hadrian.
[71:40] Speaker 5: However, there was an earlier temple on the same site.
[71:43] Speaker 4: So right now, this live is a website and a PWA, so you can have it in your phone.
[71:51] Speaker 4: Um, some features I want to build in the future is offline mode, so if you don't have connectivity in the new local your arts, you can uh download it beforehand.
[72:03] Speaker 4: And I also want to have the live agent in different languages as well.
[72:08] Speaker 4: Thank you.

[72:18] Speaker 2: All right.
[72:19] Speaker 2: Now we have access to so much healthcare data in our pockets.
[72:24] Speaker 2: What if we could analyze that data and understand the health of ourselves and our families?
[72:31] Speaker 2: With this problem in mind, we have Max, Nicholas, and Ryan with Tiger Care.

--- Segment 8 (Starting at 70:00) ---

[70:00 - 70:03] (Silence)
[70:03] Speaker 1: Imagine, you and your family just moved to a new city.
[70:10] Speaker 1: Your spouse and you both work full-time, have kids, you're taking care of elderly parents that uh have multiple conditions.
[70:25] Speaker 1: You have to get the new patient paperwork, you have dozens of medications, you have to manage who is taking what medications.
[70:36] Speaker 1: How do you and your family bring all the data together and how do you know the whole picture?
[70:45] Speaker 1: So the issue today is that the landscape of electronic health records is evolving.
[70:53] Speaker 1: Um the 21st century Act data standards are moving towards greater data portability.
[71:02] Speaker 1: But for individuals and families, key barriers still exist.
[71:06] Speaker 1: You know, privacy concerns make data access complicated.
[71:12] Speaker 1: Uh there's fragmentation that leaves records scattered across different systems and the lack of interoperability means these systems don't always communicate with each other.
[71:24] Speaker 1: So chronic so what does the data show?
[71:30] Speaker 1: The data shows that chronic diseases rates are up.
[71:35] Speaker 1: Um the CDC data shows that 60% of US adults have at least one condition and trust in institutions are down.
[71:45] Speaker 1: People feel that healthcare is reactive rather than proactive.
[71:52] Speaker 1: So they take matters into their own hands, and this has led to the rise of the health conscious consumer, because health insights offer promise.
[72:02] Speaker 1: What you know might just save you or at least make you feel like it could.
[72:07] Speaker 1: But more data leads to more confusion.
[72:11] Speaker 1: How do you interpret it all?
[72:14] Speaker 2: So what do families really want with their healthcare data?
[72:20] Speaker 2: They don't just want to they just don't just want access to all their data, they want to understand it, they want meaning to it.
[72:28] Speaker 2: So they also want the whole picture, secure access to all their healthcare data all in one place.
[72:38] Speaker 2: But now having all this knowledge, what do they do with that?
[72:41] Speaker 2: They get if if they don't have some way to interpret all this data, they get very overwhelmed.
[72:48] Speaker 2: This is where we harness AI to understand your family's healthcare data.
[72:54] Speaker 2: Advanced warnings and screenings for for your healthcare data and warnings before things actually become serious can invigorate people to take action before anything is gets to an emergency level.
[73:17] Speaker 2: So that's where we come in, introducing Tigercare.
[73:23] Speaker 2: The family doctor in your pocket, but not really because saying that would be illegal.
[73:33] Speaker 2: We give you a a full hub of everyone all your families and your own personal data as well as incorporating an AI for natural language query which you could ask any questions related to yours or your family's health data.
[73:54] Speaker 3: So let's talk about how we built this thing.
[73:58] Speaker 3: We started with a family.
[74:02] Speaker 3: You and your family uh have all your records in one place now.
[74:06] Speaker 3: We sync up with all of your iOS and Android compatible devices, my fitness pal, uh nutrition, um wearables.
[74:18] Speaker 3: Uh.
[74:20] Speaker 3: And then uh we host it on the Azure Cloud.
[74:24] Speaker 3: World class security keeps your data secure and we leverage the power of open AI with the latest data models to do advanced screenings on every new data point coming in and to let you chat in real time with an LLM that has access to all your healthcare records.
[74:40] Speaker 3: Uh we integrate with Fast and Connect, which has connections with over 40,000 different healthcare institutions in the United States um to bring all those records in securely.
[74:56] Speaker 3: So, in conclusion, we're fixing security, portability and interoperability of your medical records, getting everything in one place and then giving you the power with AI generated insights that will ultimately give you and your family improved healthcare outcomes.
[75:20] Speaker 3: Uh sign up on our uh email weight list at tiger.care.
[75:26] Speaker 3: Uh I'm Max, this is Nick and that's Ryan and we're Tiger Care.
[75:30] Speaker 3: Thank you.
[75:44 - 75:50] (Silence)
[75:50] Speaker 1: All right, so in this Gantlet cohort, we have a lot of gamers and the next solution is your table top AI assistant and its name is Fry.
[76:04] Speaker 1: Next up is Marcus.
[76:43 - 76:53] (Silence)
[76:53] Speaker 4: All right.
[76:57] Speaker 4: Hello everybody.
[77:00] Speaker 4: How many of you play or know about D and D?
[77:06] Speaker 4: All right, that's a number of you.
[77:08] Speaker 4: So, how many people worldwide?
[77:12] Speaker 4: Around 50 million.
[77:15] Speaker 4: That's a lot of people.
[77:17] Speaker 4: And what are people overwhelmingly looking for when playing this game?
[77:27] Speaker 4: Roleplay and exploration.
[77:32] Speaker 4: But there's a problem with this.
[77:36] Speaker 4: You looked at the core rule books for like D and D Pathfinder, most of it's taken up by combat rules.
[77:45] Speaker 4: When you play a session, a lot of it's just, you know, combat.
[77:49] Speaker 4: New players may not know what the rules are.
[77:54] Speaker 4: They may show up to session, don't know how to build, level up their character, how to take a turn and that wastes time.
[78:04] Speaker 4: It slows down the game, that's not fun.
[78:09] Speaker 4: So, what's our solution?
[78:16] Speaker 4: Fry, your or tabletop AI assistant.
[78:23] Speaker 4: So, who is Fry?
[78:32] Speaker 4: Fry is your assistant that will help you level up your character, take your combat turns and explain rules to you.
[78:43] Speaker 4: And if you're waiting, you can just chat with him.
[78:46] Speaker 4: So, let's start with a demo.
[78:49] Speaker 4: He sits straight into the foundry.
[78:52] Speaker 4: I am going to level up my character right now.
[78:57] Speaker 4: So what's going to happen now?
[78:58] Speaker 4: Fry is going to start thinking and he's going to tell us, hey, I saw you just leveled up.
[79:06] Speaker 4: Here's some suggestions.
[79:10] Speaker 4: So that'll pop up shortly.
[79:12] Speaker 4: All right, let's see what he has to say.
[79:16] Speaker 4: Skill increase.
[79:19] Speaker 4: Um hmm.
[79:23] Speaker 4: All right, what's his recommendations?
[79:28] Speaker 4: Acrobatics or nature.
[79:30] Speaker 4: Okay, I like that.
[79:34] Speaker 4: Let's have him do it.
[79:58] Speaker 4: All right, so you can tell him things to do and he'll just modify your character sheet uh so that you know, you don't have to do it yourself.
[80:10] Speaker 4: You can just say, hey, level up my character for you uh for me.
[80:16] Speaker 4: All right, seems like he's done that.
[80:19] Speaker 4: Now, oh, shoot, our combat turn's coming up and we don't know what we're supposed to do.
[80:28] Speaker 4: Well, Fry is here to help you with that too.
[80:32] Speaker 4: So, he's thinking and he will tell us uh give us suggestions so we don't have to dither around for a little bit.
[80:41] Speaker 4: So, all right, Fry, what should we do on our turn?
[81:02] Speaker 4: So, it takes a little bit to think, but the main aspect is uh it's combat leveling up, it takes time and it slows down the game.
[81:19] Speaker 4: So instead of worrying about that, just kind of wasting time and slowing it down.
[81:25] Speaker 4: Remember that D and D is fun and it's for everyone.
[81:31] Speaker 4: Let's get back to having fun.
[81:50] Speaker 1: All right.
[81:52] Speaker 1: Now we have over 4,000 people watching, so I have to say this.
[81:57] Speaker 1: Demoday.gauntletai.com.
[82:01] Speaker 1: Get on our website, see all the presenters, see the challengers, what are they presenting and see if you want to give them feedback or any criticism as to what you're seeing.
[82:12] Speaker 1: Next up, we have the App Halos, the team of Leway, Bjorn, Sude and Terry and they're going to take AI filters to the next level by mapping them to your face.

--- Segment 9 (Starting at 80:00) ---

[80:00 - 80:05] (Silence)
[80:05] Speaker 1: Hello everyone.
[80:07] Speaker 2: Hello.
[80:08] Speaker 1: Introduce uh we're team Hailos, and today we're going to be introducing to you a new way of doing digital filters.
[80:16] Speaker 1: I'm sure most of us have used digital filters.
[80:19] Speaker 1: It's a growing industry where everyone just wants to look like their best self online um in the digital world.
[80:28] Speaker 1: But the problem is when you do try to use these filters, often times it makes you look just like everyone else.
[80:34] Speaker 1: They're not personal.
[80:35] Speaker 1: They are sort of fake.
[80:38] Speaker 1: You can tell at a glance that someone has applied a filter, which honestly defeats the point of a filter, because the idea is that you should look like yourself at your best every day.
[80:47] Speaker 1: Not like you at your best with the filter on.
[80:50] Speaker 1: So, I'm going to hand it off to Terry to talk about the industry.
[80:55] Speaker 2: Uh yeah, so um the existing photo um editing market is huge.
[81:02] Speaker 2: Um this is uh the numbers for uh facetune, which isn't even the biggest company, it's only the second biggest.
[81:09] Speaker 2: And it's bringing in 100 million a year.
[81:11] Speaker 2: Um everyone uses this um these kinds of apps.
[81:17] Speaker 2: But um at the same time they're just not good.
[81:20] Speaker 2: They shove your face into like identical little like pointy chin, bug-eyed, um faces, they're really generic, um and uncanny, um and uh they have all these fiddly settings that consumers just don't want to deal with.
[81:39] Speaker 1: Yeah, so what do we do differently?
[81:43] Speaker 1: Um our process basically is that we take the user's face, we analyze it, and we carefully transform it according to um a proprietary process based in aesthetic science.
[82:01] Speaker 1: Once we have the transformation, we create a personalized 3D face mesh that we store and we can apply to any image, any video from any angle.
[82:21] Speaker 1: And yeah, I don't know if you guys can tell a difference because the screen is not bright.
[82:25] Speaker 1: Uh it's my face.
[82:27] Speaker 1: On the left is the unfiter face, and on the right is the filter face.
[82:32] Speaker 1: It's a little hard to tell, but like we deliberately made it exaggerated so that you can see that the nose is smaller on the right side.
[82:39] Speaker 1: And it blends pretty seamlessly into the face, so if you don't know, like you you just can't tell, which is the whole point of our app.
[82:50] Speaker 1: And yeah, I mean, imagine a world where like you just upload one photo of your face, and instantly our app has a personalized 3D imaging of your skeletal structure, your facial features, and from that point on, we can do any minor tweaks to your face and any photos that you upload in the future, we can apply the exact same changes to those photos at any angle because it's modeled in 3D.
[83:08] Speaker 1: So with that tech we there are so many use cases for we could do.
[83:11] Speaker 1: It's not just like a beauty digital filter, it's also a foundational tech.
[83:15] Speaker 2: Yeah, so now that we have this infrastructure, uh we can just build any sort of app on it, like you know a bunch of ideas like real-time analysis, uh custom color pallets.
[83:25] Speaker 2: Um because now that, you know, we have this uh mesh system, we can just uh map it onto anything.
[83:34] Speaker 2: Um yeah, and a bunch of different consumer app ideas, uh listed all here, yeah.
[83:41] Speaker 2: We got uh yeah, a bunch of things.
[83:45] Speaker 1: Another really interesting potential application is in plastic surgery.
[83:50] Speaker 1: So the plastic surgery market in the US is about $27 billion and that's in the US alone.
[83:58] Speaker 1: So this application or this technology can easily be extended to um 3D modeling and visualization of plastic surgery transformations both for the users or the patients as well as for the providers in treatment planning.
[84:21] Speaker 1: And that's it.
[84:22] Speaker 1: Join the Hailos revolution.
[84:26] Speaker 1: So yeah, hopefully you guys can see that the right side is filter, the left side is not, but the right side looks so natural that's like the no filter filter, which is our tagline.
[84:36] Speaker 1: Thank you guys for your time.
[84:58] Speaker 3: So at Gauntlet AI, we have a few students working at the foundational level too.
[85:03] Speaker 3: This is Yoris, who built a competition for a benchmark that he thinks he can make better with agents.
[85:23] Speaker 4: Hey, I'm uh Yoris and in a lot of our projects, a big problem many of us had was making LLMs work was making LLMs work well with long context.
[85:39] Speaker 4: So when you have like a small prompt, uh a small exercise and you want to make the LLM work, it actually works fairly well, but for an entire book it just gets lost and confused.
[85:55] Speaker 4: And there are some benchmarks which try to do that, but they are only evaluated on foundation models.
[86:05] Speaker 4: So you see a leaderboard on an agent like Longbench, which tries to where you see different foundation models like Gemini and um GPT4 perf performing, but in fact, the right way to approach this problem in a real world setup is to build some sort of agentic system.
[86:30] Speaker 4: So you build some rag or um read in chunks and it is not it is not obvious which model is the best for that.
[86:41] Speaker 4: So for that, I'm sort of set up a competition, a framework where people can submit their own agentic frameworks with the same model and compare the performance on this Long Bench 2 benchmark.
[86:58] Speaker 4: Um showing how easy it is to um participate in that and set up and change your agent and uh upload it in this video.
[87:11] Speaker 4: So you just have to clone a repository, you have a very simple you have a scaffolding and you have to basically just change one function to change a prompt.
[87:23] Speaker 4: And then you up then this automatically gets the token usage gets tracked, the results get uploaded to Hugging Face and if you have a good result, you can get a you you can just submit a pull request and get into the leaderboard.
[87:42] Speaker 4: The original authors of the competition have seen have indicated that they're interested in these results.
[87:49] Speaker 4: So there's a good chance that they will be parts of this leader world and I believe that if you build this sort of ontology of different agents, you will um be able it will be helpful for not just one particular app, but many different uh people trying to build this thing and you get your five um minutes of fame.
[88:22] Speaker 4: So, let's see what's happening here.
[88:24] Speaker 4: We we have basically we have we just have very quickly changed some prompt and we see um the result.
[88:34] Speaker 4: The um agent is being evaluated with some concurrency and um logging and tracking and the results are being processed.
[88:48] Speaker 4: And then on Hugging Face, you see associated with the commit ID, the performance of the uh the the exact results and can analyze them further.
[89:03] Speaker 4: Um yes, that's the that's my pitch.
[89:09] Speaker 4: I have um we can we can see we I I have here a QR code that you can scan to get to this repository and I'd be happy if some people reading this like watching this from online are interested and um submit something.
[89:38] Speaker 3: All right, it's March and we all know what March is about, March Magnus.
[89:43] Speaker 3: And if you don't know what bracketology is, you're about to find out.
[89:47] Speaker 3: This is your platform for your NCAA predictions with John Boyle.
[90:20] Speaker 5: Hey everyone, I'm uh excited to present to you guys today my demo day project, Bracket Lab.
[90:32] Speaker 5: So first, I want to start by answering the question, what is bracketology?
[90:38] Speaker 5: So I'm sure a lot of you guys are familiar with uh the March Maness tournament that happens every year.
[90:44] Speaker 5: I'm sure a lot of you guys filled out brackets last week that were busted about two hours into the tournament.
[90:53] Speaker 5: Um but bracketology is

--- Segment 10 (Starting at 90:00) ---

[90:00 - 90:05] (Silence)
[90:05] Speaker 1: a different process.
It's the process that comes before Selection Sunday, when all the teams are selected.
And to really put it succinctly, it's the science of predicting the 68 teams that will end up making the tournament field.
Um if we take a uh look at the current state of bracketology, um it's not very accessible to the casual fan.
There's a lot of technical barriers.
A lot of uh sharing predictions consists of just posting uh text predictions on people's websites and blogs.
There's no platform at all for users to create brackets.
There's no uh good all-in-one platform for tools in order to make these kinds of predictions.
So my aim with Bracket Lab was to solve all of that.
I wanted to build a platform where it had all the tools you would need to make the predictions.
Uh it would provide a nice user interface for you to build the bracket and view your predictions.
And then also a platform for sharing predictions with uh with other people.

So, without further ado, let me just do a quick demo of uh what creating a bracket looks like in Bracket Lab.
So, the user will first see this kind of create bracket screen.
They'll be met with a team list consisting of all the teams that can make the tournament, as well as an empty seed list on the right side, which is basically a ranking of all the teams uh that for the user to fill out in order to make the tournament.
Um I got a bunch of different tools in here like filtering on different stats, sorting, and then I also have some other buttons which are a little more advanced features for uh for more experienced bracketologists.
Uh for simplicity's sake, I just selected AutoFill bracket here and uh that just used the net as default.
It's customizable, but it just used the net to rank all the teams.
Um so just go ahead and submitting the bracket, it takes us to the bracket preview page.
And so what I did was development algorithm that's very similar to the actual process the NCAA selection committee uses uh in order to bracket the teams.
And so that's basically what's we see here.
And then I give uh the user also some chance for some last-minute customizability if they want to, you know, switch around pod locations or teams.
Then once they're happy, they can go ahead and submit their bracket and boom, they've created their first bracket and they can call themselves a bracketologist.

Um basically to wrap it up, I just wanted to go over some of the AI tools and methodologies I use to build the program.
Uh one of the best things for me is speech to text coding.
Just allows me to develop so much faster when I don't have to type out uh each prompt.
I can be very verbose in what I say and uh I can just allows me to develop a lot more rapidly.
I'm also not a very uh UI oriented guy, so using AI uh UI tools really helps me make the design look clean without being too proficient myself in that manner.
Uh and uh yeah, code suggestions, data processing and that's uh that's basically it.
Thank you.

[95:52] Speaker 2: All right, we have 4.9k individuals watching us live on X.
Just 100 away from that 5K mark.
If you haven't gone to demo day.gauntletai.com, go there right now, see who's presenting and check out our projects.
Next up, we have a group who believes that software is being democratized.
Building apps is going to be really easy, but getting customers is going to be the hard part.
It's Leadify with Malachi, Jesse and Reese.

[97:34] Speaker 3: Is this working?

[97:36] Speaker 4: Am I not?
We got one mic.
That works, right?

[97:42] Speaker 3: Is this coming through?

[97:44] Speaker 4: It is.
I'll bend down.
Test, test, test.

[97:50] Speaker 5: Yeah, what's up guys?
Do my best Tony Robbins.
Let's get up, guys!
Get some energy in here.
Let's go!
Do we go full screen?
Sweet.
All yours.
Go for it.

[98:20] Speaker 4: All right, guys.
Welcome to Leadify.
We're an AI agent that crawls the web to find you customers and reach out to them completely autonomously on your behalf.
So, if you look at the AI landscape, one thing is pretty clear.
Production is becoming democratized.
What that means is everybody can build apps now.
You don't have to be a developer.
You can just use cursor or lovable to create and ship products.
And so, if you fast forward three, five years when everyone is shipping apps, what is the next giant opportunity?
It's going to be finding customers.
Distribution will still remain a bottleneck and so that's what we are laser focused on.
So, in the past, everyone would have maybe a dozen ideas and only build one of them.
Now with AI, everyone has a dozen ideas and builds a hundred products instead.
And so building is fun as we all discovered at Gauntlet AI, but what's not fun is going out and marketing your idea and finding customers for all of them.
Luckily, we think the future is one where you can simply push a button and get a customer.
And so we're stoked to build in that direction.
So, it's an incredibly simple three-step process.
It'll literally take you two minutes of work and we'll autonomously reach out to every platform on the internet to find you customers.
You give us your website, we scrape it, learn what you do and generate a proposal for what type of customers we think you want.
You can go through, correct it, make any fine tunings you want, and then boom, that's literally all you do.
We crawl the web, go through Twitter, go through Reddit, go through websites, we find you leads and draft tailored messages to each and every one of them from you.
And so the vision is to do this for the entire internet, to have agents that crawl the entire web.
But for MVP, we focused on just two platforms, Reddit and Twitter.
So here, you might see that um you can put in your website for your product and a description and we'll figure out what your product does, what your target audience is going to be.
Um any problems, any major problems that your product solves, and then we're going to match that to an appropriate subreddit.
So for each subreddit, we've uh scraped and figured out what the most popular problems that people talk about in that subreddit, what they um what problems that are unsolved and then we'll use this uh customer profile um location and automate the process to find your customer.

[101:55] Speaker 5: Yeah, we do.
So, for this example, we went to Gauntlet AI.
Who needs Gauntlet?
People who need jobs.
They're complaining.
They're like, dude, there's no CS jobs out there.
You've seen those people on Reddit.
Like pessimists who are always down on the world.
We literally go out there, we find these posts and we comment on them to be like, dude, there are CS jobs out there.
In fact, we can guarantee you a 200k CS job if you join Gauntlet, make it through the 12 weeks and then do a demo day pitch like this.
Fantastic.
So that's how you can use Reddit to find leads.
And there's another giant treasure trove of potential customers and that is Twitter, which we all know.
And so after you search through Reddit, we can also search through Twitter.
And how it works is after you enter in who your ideal customer profile is, our agent is going to go through Twitter.
So this is not me doing it.
This is the agent putting in the work and it's going to be looking for people that might be buying a Docusign alternative.
So let's say I'm building a Docusign clone, the agent will have just found any tweets that might relate to that and here it actually found, this is a real tweet.
And so if I were building a Docusign clone, I could totally promote my product right there and potentially close a customer.
And so a lot of exciting things are happening right now in this direction.
Yeah, so like literally everyone I talked to, their face lights up.
They're like, dude, I want more customers.
We've a massive weight list of people interested.
So we've onboarded five beta clients.
For those clients, we found them over a thousand qualified leads and to date we've sent 500 tailored comments.
We are super excited by this direction.
Thank you guys so much for the time.
If you want to stay tuned, feel free to check out our weight list through that QR code and much more is coming.
Thank you guys so much.
Quick bonus I want to throw in there.
Sign up to the QR code and I'll get you 10 leads for free.
Thank you, guys.

I know what you're thinking.
How do I hire these people?
If you go to gauntletai.com,

--- Segment 11 (Starting at 100:00) ---

[101:54] Speaker 1:  AI.com and click apply to be a hiring partner or if you want to join the next gauntlet.
[102:02] Speaker 1:  You can apply to be a challenger.
[102:06] Speaker 1:  gauntlet ai.com.
[102:11] Speaker 1:  Next up, we have Gary, and he's obsessed with the silver tsunami.
[102:16] Speaker 1:  He'll tell you what that is.
[102:22 - 102:41] (Silence)
[102:42] Speaker 2:  Please work.
[102:43] Speaker 2:  There we go.
[102:47 - 102:52] (Silence)
[102:52] Speaker 2:  All right, so I want you all to meet Mark, one of my friends, grandpas.
[103:00] Speaker 2:  Mark is a NASA veteran, 80 years old, and all that he wants to do is make sure the lessons that he's learned in his illustrious career are passed down to his grandkids and the next generation.
[103:14] Speaker 2:  By the way, one of his grandchildren just had a baby last week.
[103:19] Speaker 2:  So this is incredibly relevant.
[103:23] Speaker 2:  And so one thing I started doing was, hmm, I'm not sure what I want to automate.
[103:31] Speaker 2:  So I want us manually start emailing him from this autobiography agent Junie, right?
[103:39] Speaker 2:  And I it was a pretty manual process, but I figured out how to make it really engaging.
[103:45] Speaker 2:  And then once I figured that out, I basically automated that process by creating a platform called talktojunie.com.
[103:55] Speaker 2:  In this particular case, I'm onboarding Dallas, who wants to tell his story.
[104:02] Speaker 2:  Um, I basically provide context on Dallas, and then I say create subject.
[104:17] Speaker 2:  And then what happens is that Dallas now gets an email from Junie, his personal autobiographer.
[104:28] Speaker 2:  And here, Dallas is typing out a a a response to the starting questions.
[104:45] Speaker 2:  Good job, Dallas.
[105:01] Speaker 2:  And then once he's done, all he does is he presses send, and then what I get because I selected that I want to manually review the automated generated follow-up questions because I just want to make sure there's no hallucinations, but it just makes it so easy for me, right?
[105:21] Speaker 2:  And so when I'm ready, I say send the subject, and then boom, Dallas gets another email from Junie for a follow-up.
[105:36] Speaker 2:  And and and and what what is beautiful is that I can see the whole process.
[105:46] Speaker 2:  I can see that the AI is developing a a greater and greater understanding of Dallas.
[105:56] Speaker 2:  And it is honestly trivial at this stage to be able to then put that into a printed book, turn that into an animated, you know, short video.
[106:08] Speaker 2:  So we really automated the process of collecting, um, biography.
[106:16] Speaker 2:  And it was through many, many iterations of building lots of different kinds of interfaces that we found that especially elderly people really love telling their story via email, right?
[106:29] Speaker 2:  It's the interface that they learned, they don't want to relearn something else, but we can still add AI automations behind the scenes.
[106:38] Speaker 2:  Um, if you want to be an alpha user of talktojunie.com, um, please DM me on Twitter at @garyshang, g a r y s h e n g.
[106:53] Speaker 2:  Um, if you have a a a a loved one that just retired, you have a grandparent you know that has an amazing story.
[107:02] Speaker 2:  Um, I'd love to have Junie talk to them and then we can uh preserve their legacy forever.
[107:11] Speaker 2:  Thank you.
[107:15 - 107:23] (Silence)
[107:23 - 107:38] (Music Playing)
[107:39] Speaker 1:  All right, next up, we have Donald with D3.
[107:44] Speaker 1:  If we're going to make coding accessible to everyone, then we have to move away from code syntax and move completely over to natural language in English.
[107:54] Speaker 1:  So, he's coming up next with his coding solution just using documents.
[108:01 - 108:18] (Silence)
[108:18] Speaker 3:  I don't see it.
[108:19 - 108:25] (Silence)
[108:25] Speaker 3:  Testing, testing.
[108:29] Speaker 3:  Uh okay, my name is Donald and I've been working on D3 for the last couple weeks.
[108:36] Speaker 3:  So, about a year ago Open Open AI released GPT 40 and with that release, uh serious code generation became a reality.
[108:50] Speaker 3:  Google and Anthropic were not far behind.
[108:54] Speaker 3:  So, Andre Carpathy saw this coming about two years ago already and more recently, earlier this month, YC reported that a quarter of their startups were using AI to generate 95% of their code bases.
[109:11] Speaker 3:  So, I started to think about the limitations of the chat-based interface.
[109:17] Speaker 3:  Um writing software like having a conversation isn't quite how it works usually.
[109:25] Speaker 3:  Arguably writing software is a lot more like writing down laws.
[109:32] Speaker 3:  Uh, so we skirt around this problem using at rules and cursor, we go outside of our IDEs to generate to generate PRDs and specifications otherwise.
[109:48] Speaker 3:  Uh, so for me this felt inherently like a work around and I thought that documentation is code made a lot more sense.
[109:56] Speaker 3:  So, with this, we can see the whole system at once.
[110:02] Speaker 3:  We can clarify and improve our intent over time and we can keep track of things properly with chat.
[110:11] Speaker 3:  We chat back and forth and then the chat is gone with uh documentation, it's a living document that evolves over time.
[110:19] Speaker 3:  And in a way we're kind of doing this uh with these long prompts that we're writing, we're bringing in external documents, trying to get it all in one place.
[110:33] Speaker 3:  So, I'll show you what I've been building.
[110:42] Speaker 3:  Uh, so the initial process is very much like VZero or lovable if you're familiar, you input your project description.
[110:54] Speaker 3:  The different starts here where the users asked a number of clarifying questions.
[111:01] Speaker 3:  So this uh allows us to gather the user user's intent and with that intent we can generate a list of specifications.
[111:28] Speaker 3:  So, that pops us into this new interface here and on the left we have all of our specifications.
[111:40] Speaker 3:  And with those specs, we can build the product.
[111:46] Speaker 3:  So, this actually generates the entire app all in one shot.
[111:51] Speaker 3:  This is the front end and the back end.
[111:59] Speaker 3:  You'll see that the uh AI agent had some fun with the uh address, Amsterdam, New York.
[112:14 - 112:16] (Silence)
[112:16] Speaker 3:  And uh, so this is the front end and you'll see that there's a an API in the back end here too.
[112:32] Speaker 3:  So, the idea is that the specifications on the left map to the preview on the right and then you can just update the specifications and rebuild the app.
[112:51] Speaker 3:  All right, my name is Donald.
[112:52] Speaker 3:  Thank you for listening.
[112:56 - 112:59] (Silence)
[112:59] Speaker 1:  Next up, we have Devin with Align AI.
[113:04] Speaker 1:  If a curriculum builder wants to align with national and state standards, they can do that in an instant with this application.
[113:13 - 113:47] (Silence)
[113:47] Speaker 4:  Can you guys hear me okay?
[113:49] Speaker 4:  Cool.
[113:51] Speaker 4:  Hi, um, I'm Devin, and this is Align AI.
[113:58] Speaker 4:  And we help curriculum builders align with state and national standards instantly.
[114:04] Speaker 4:  Um, here's what's crazy about my presentation today.
[114:09] Speaker 4:  Everything I'm about to show you today came together in about 24 hours, and this included going from no code to an MVP to a paying customer of $6,000.
[114:22] Speaker 4:  Uh, so here's what happened.
[114:27] Speaker 4:  Four days ago, my original client came to me and said, hey, we can't show our demo on stage.
[114:34] Speaker 4:  Um and I was a little bit screwed uh because Gauntlet AI gave us about three weeks to do this demonstration.
[114:43] Speaker 4:  So, I go home and I'm talking to my housemate and she lays this gem on me where she says, hey, I think I have an interesting problem at work you might like.
[114:54] Speaker 4:  And the idea was so good that the next morning she and I were on a call with her CEO.
[114:59] Speaker 4:  Let me summarize that meeting.
[115:05] Speaker 4:  So when you were a kid in Ja in gym class and you were jumping rope,

--- Segment 12 (Starting at 110:00) ---

[110:00] Speaker 1: that activity aligned with a very specific government standard.
And curriculum builders in the United States need to align with hundreds and thousands of local state and national standards.
And this process is extremely tedious, slow, and takes full-time specialists to do.
So I had out-of-body experience at that moment and I asked the CEO, what if I built you an AI agent that does this in hours for a tenth of the cost?
And he was intrigued.

So I went back home, and about 5 hours I built my first working prototype.
And I begged my roommate to get another meeting with the CEO the same day.
She was not thrilled about the request, but she gave in and this was what I presented to that CEO.
A full working end-to-end prototype of this solution.
It site scrapes thousands of standards, puts in a database and then uses computer vector alignments to compute the exact course alignments.

The CEO was stunned, and lo and behold I had my first customer.
I went ahead and did a little bit more market research after this, and I found out that there's 300 companies, 13,000 school districts all doing curriculum building.
This is a market of about $6 billion every year in the US.
Everybody needs to do this process.
With my product they can save millions of dollars in labor costs.

Here's my current team.
[112:23] (Laughter)
Speaker 1: I also went ahead and posted about this on X.com and thanks to Austin and Gauntlet AI, this post has received about 71,000 views and the initial positive feedback has been great.
Ladies and gentlemen, I need your help to turn this into a rocket ship.
Uh, and I hope you will reach out.

Thank you.
[112:58 - 113:05] (Silence)
[113:05] Speaker 2: All right, to the 5.5 thousand people joining us online and to everyone here live at the HQ.
We're going to be taking a five minute break.
Just stretch your legs, get some coffee, get some water and we'll be back with more presentations in just 5 minutes.
If you haven't checked out demo day.gauntletai.com and made an account, met all of our challengers and all the projects that are working on, you should do that right now.
And if you want to be a hiring partner, check out Gauntlet ai.com where you can apply to be a hiring partner or apply to be a challenger and meet me and Austin later this year.
Thank you guys.
See you in 5 minutes.
[113:59 - 117:46] (Silence)
[117:46] Speaker 3: All right.
All right, everybody both here in virtual.
Thanks for uh hanging out with us for a little bit.
Uh for those of you that are just joining, uh welcome to Gauntlet AI Demo Day.
Um we're through the first section.
I've got another little section and then we're going to take a break and we'll actually restart the live stream uh after lunch because X only lets you go for so long before it forces you to stop.
Um but we're super excited to to have you all here.
Um for those of you that are unfamiliar with Gauntlet, uh it's a 12-week program.
We take people from all over the world.
Uh we are very, very selective for uh intelligence and coding ability.
Um and then the reason it is named Gauntlet is we put you through a series of challenges.
Each week is a new challenge in order to move from one week to the next.

--- Segment 13 (Starting at 120:00) ---

[120:05] Speaker 2: you have to successfully complete the challenge. So we started out with around 230 students who were accepted from more than 3,000 applicants uh almost three months ago. Uh now we're down to right around 110. Um so that's that's going to be the number that graduates with us. Uh if you're interested in being a Challenger in the future, it is completely free, all expenses paid. We cover your flights, we cover your housing, we cover your food, we cover the events, um all of that stuff.
[120:51] Speaker 2: Um if you're interested in applying to be a hiring partner, uh you can do that at gauntletai.com for subsequent cohorts.
[121:04] Speaker 2: Um there is no guarantee that if you want to hire, you are able to hire. We have to be selective on both sides. Um with that said, uh we're going to kick off the uh next set of demos and we'll go from there. Thank you.

[121:45] Speaker 3: All right. Next up, we have David, who's going to bring you Dexter. It's an investigation management platform for law enforcement where he uses AI to supercharge their workflow.

[122:32] Speaker 4: Hey, so I'm David and this is Dexter. Uh an investigation management platform. So consider that uh most crimes go unsolved uh reported crimes. Uh and this is not because they're particularly difficult to solve, but uh just sheer volume. Uh and even worse, uh often rushing can lead to uh mistaken convictions from mishandling evidence.
[123:02] Speaker 4: Uh so Dexter accelerates the investigation process while also making it more robust. So, let's take a look. Uh so you can manage your cases here uh and inside a case you can add uh all of the case files that you care about. Uh and once you add them, it uh automatically analyzes them, uh extracts transcripts and uh relevant information. So you can see uh for scanned documents, uh we get all of the text side by side, uh so you can look through that. Um and for longer documents, it uh keeps pace of the content. So, you know, you you want to match the page that you're actually on with the text that you're looking at. Uh and this makes it easy to copy out or uh do search and things like that.
[123:54] Speaker 4: Um we describe pictures, uh so we get a detailed description of like crime scenes and things like that. Uh and through all of this, we can search. Uh so we do semantic search uh which gives both uh good sensitivity uh for everything we might care about as well as good specificity uh so that we can tell precisely what we're looking for all together in one context. And uh we can also uh transcribe audio and out here. Uh transcribe audio and uh get location awareness. So this is my favorite feature. We can extract all of the uh locations of interest in a document uh and then from that context uh find precise locations on the map uh to look into.

[124:50] Speaker 4: We can also do manual annotations uh that mirror handwritten notes uh in the traditional workflow. Uh so we can uh annotate specific locations on the documents uh so that when we uh go and uh try to search for those annotations later, uh we can actually find them directly in context by jumping back to the page and the location that it's on.
[125:16] Speaker 4: Um so this is a lot better than managing a big stack of papers uh and pinboards. They really do use pinboards. And uh throughout all of this, uh you know, we're working with uh many different teams in law enforcement. So we have uh sharing uh cases uh at the file granularity. Uh and this allows us to uh keep our files accessible while also keeping them uh secure uh because this is really important for you don't want to leak evidence. So, uh that's the uh the state of the demo right now. This started as a last week as a two-day sprint uh for Othram. Uh and uh they they requested this and I'm going to continue working through it uh to bring it to market uh in the next week. And so many thanks to the original team that worked with me on this uh to get a lot of the original features. Thank you.

[126:33] Speaker 3: Next, we're going to go to the area of augmented reality and AI with Dan. He has this app called Tono, which helps you learn Chinese using augmented reality and AI.

[127:33] Speaker 5: All right. How many people here want to learn a foreign language? Anybody? And how many people here have ever tried to learn a foreign language and have given up? So, I think it's pretty common. uh as someone who's been learning Chinese for the past 15 years, it's quite common um to to fail because learning a foreign language is hard and Chinese is especially hard. Um as everyone knows, China's economy is booming. A lot of people want to learn Chinese for that reason, but it's a hard language to learn and all languages um are hard if you learn them in the wrong way.
[128:15] Speaker 5: And a lot of students do learn languages in the wrong way. So in schools, we use textbooks, but they're often outdated and kind of boring and you can't really learn to speak a language by reading a book. Um and some apps like Duolingo, for example, you get some amount of speaking, but it's not really personalized and a lot of people just kind of end up giving up. So, how can we solve this? I think AI can help. So, my solution is to use AI to make language learning more about interacting with the real environment around you as opposed to just looking in a book or something like that. And so what I AI can do, first, it can interpret the environment. Second, it can personalize the learning, teaching you the things that you care about that you want to learn. And third, it can test your output. You can speak to the AI and it can actually um understand what you're saying and tell you if it's right or wrong.
[129:10] Speaker 5: So, let me show you guys a demo of how this works. So, the app is going to look at our environment. It's going to recognize all the objects in the environment. Then we can click on the objects.

[129:22] Speaker 5: æ°´èï¼èæã æ¢¨å­ï¼è è¿ï¼åã ç¤ç®±ï¼æ¤å­ï¼é¤æ¡ã å°ç®±ãå°ç®±ã Then all these objects are going to be saved to our collection and we can go ahead and repeat the pronunciationsã
[129:45] Speaker 5: é¤æ¡ï¼æ¤å­ï¼ç¤ç®±ã Learn the pronunciations. We can check out our collection of all the objects we saved.

[129:55] Speaker 5: å°ç®±ã Practice speaking them trying to engrain them in our memory.

[130:04] Speaker 5: æ°´èã If you guys can remember those two, bing means refrigerator and means sync. Now we're going to go ahead and practice. So what is the Chinese word for refrigerator? Can anybody tell me?

[130:18] Speaker 5: æ¤å­ã å°ç®± Excellentã å°ç®±ã Okay? So, we already learned a word. What's the word for a dining table?

[130:26] Speaker 5: é¤æ¡ã é¤æ¡ã
[130:28] Speaker 5: Okay? So it's going to go ahead and quiz us and it's going to use spaced repetition to, you know, show us these cards just as we're about to forget them. In addition, we can go backwards, we can go from the Chinese to the English. We get the Chinese. What does bing mean? I knew I remember refrigerator, right?

[130:48] Speaker 5: What is Sanjo é¤æ¡ã Dining table. Great. Okay? So, we've already learned two words in Chinese and these are, you know, words that we chose, words that we're interested in. Um and finally, possibly most importantly, we can practice our pronunciation to make sure we're getting it right.

[131:04] Speaker 5: å°ç®±ã So, we speak into the app.
[131:07] Speaker 5: å°ç®±ã and it's going to quiz us. Give us a score, tell us how good our pronunciation is and we can get the feedback based on this. Okay? So, um, I have not released this app in the App Store yet, but I did release it on WeChat, which is a new you guys may know is like China's social network. A lot of people gave me some great feedback and they also said, when is an English version of this app coming out? So, I think this could work for Chinese, it could really work for any language, and it's going to be a way to make language learning more interesting, more fun, and, um, more useful in the real world. Thank you.

[131:59] Speaker 2: If you're just joining us and haven't gone to demoday.gauntletai.com, use this QR code to check out our challengers, the projects they're working on, and to give them real feedback on their applications. Next up, we have Dallas. Dallas can take any TikTok creator and make them into an agent. What am I saying? Well, he's about to just show you.

--- Segment 14 (Starting at 130:00) ---

[130:00] (Silence)
[130:06] Speaker 2: Okay, so where do you get your information from?
For me, 99% of it comes from online.
So I thought, what if we could take our experts and turn to turn them into agents we can interact with?

And so, I'm going to start with Tik Tok.
I connected with a Tik Tok influencer with over 50 million views on teen dating advice.
And she was saying that the struggle for teens is that they might be afraid or embarrassed to talk with their parents or friends, so they need somebody that they can get real advice from privately.
And so, I think the best way to illustrate this would just be to let her talk for herself here.
So, you can come in, give it a call, and get on the phone with her.

And I'm going to get the sound on.

All righty.
So you come into the app, click call.
[132:04] Speaker 3: Hello, it's L.
How's it going?
[132:07] Speaker 4: Hey, can you help me reach out to somebody?
I'm afraid when I ask them something it's going to sound rehearsed.
[132:15] Speaker 3: Of course, it's totally normal to feel that way.
Let's make it more natural.
What do you want to ask them?
We could brainstorm some casual ways to phrase it together.
[132:24] Speaker 4: Um, she's on the soccer team.
[132:27] Speaker 3: That's cool.
Do you want to ask her about soccer or are you thinking about something more personal?
Like hanging out or getting to know her better?
[132:34] Speaker 4: Uh probably the soccer team.
[132:36] Speaker 3: Got it.
You could start with something like, hey, how's the season going for you?
Or I saw you guys had a game last week.
How did it go?
These sound casual and show you're interested in what she does.
What do you think?
[132:50] Speaker 4: Yeah, that's probably good.
Then what do I say after?
[132:53] Speaker 3: After she responds, you can keep the conversation going by asking follow-up questions.
For example, if she mentions how the game went, you could ask what her favorite part of the game was or if she has any memorable moments from the season so far.
You could also share something related, like if you play sports or enjoy watching soccer.
Just keep it light and fun.
[133:26] Speaker 2: And so, we can also meet you where you're at.
So if they're texting, we can go into IMessage.
You can take a screenshot of the question in your messages, pull up that screenshot all within the message app, send it to AI and get a response where you can easily insert the response into the chat and send it, helping you overcome anxiety and make new relationships.
Thank you.
Yeah.
[134:27] (Silence)
[134:30] Speaker 5: All right.
Next up, we have Cyrus with two applications, Logcow and Bud.
Logcow lets you catalog your memories and your events and Bud is your plant-based companion within your mobile.
Let's welcome Cyrus.
[135:19] (Silence)
[135:53] Speaker 6: Okay.
Uh hello.
Uh I'm Cyrus.
I have two apps to present, Log Cal and Bud.

Uh Log Cow is a social diary.
Uh scan this QR to join uh an app web app to join an entry for this event.
Uh this is how most people join Logcow.
If you never join an entry with other people, it's a purely private journal, but when you join an entry with someone else, they are considered your friend and you can now add each other to hangouts by username uh instead of always scanning or using a uh invite link.
Uh from there you'll be able to see uh common entries with them, but you'll only ever see hangouts that you're in.
Um, so it's uh as opposed to a more open social media, it's uh a social diary.

Uh scientifically, journaling is good for you.
Uh second point.
A lot of people don't like current social media.
Who even uses Twitter anymore?
No one.
It's called X now.
Uh three, in person social interaction has been on the decline for years, not even counting Covid.
Scientifically, uh loneliness has skyrocketed.
So, what about a social journal that gets you to have more social experiences, not less?
Um, I'll just explain some of the views here.
So this is the like landing.
Uh you have a sort of calendar mosaic of your hangouts.
Uh and this is at least for me, like I built this app for myself, uh trying to fill this up actually gets me to hang out with people uh with people more than I would.
Um, then we have uh log entry uh and log view uh and there are more features than just that, but I will move on to Bud.
I have another QR for this.
Uh scan that one and you can claim this little workout bud.
Um, I'm not giving you as much time for this one.
Uh I think that's that's probably fine.
Uh, Bud consists of two parts, a desktop web app for building your bud and a mobile app for carrying a bud with you uh to chat with about your goals and good habits.
Um, the editor allows you a high degree of freedom when building your bud.
Plant parts cost stars.
Uh you can place parts wherever, however, uh whichever color uh so on.
Uh the plant chat is equipped with conversation history, memory and goal tracking to push you further in your life while earning stars to build more buds.
Uh as you grow, your plant collection grows.
You can even gift buds to your friends.
Uh Bud won't solve your life problems.
You will.

And that's it.
I mean, uh the video does show uh the actual carrying of one of the buds that you edit into the mobile companion app.
Um, which is a very simple app.
only allows you to uh chat with your bud and um uh collect stars to then go back and create more plants if you want.
Yeah.
That's it.
Thank you.
[139:31] (Silence)
[139:35] Speaker 5: All right, next up, we have the Mortenson Brothers, Christian and Benjamin.
Why is it that the most intelligent minds in our country have to go through so much rigor mortis to get funding?
What if you could just apply for grants at the push of a button?
That's what Christian and Benjamin are solving.
[140:06] (Silence)
[140:09] Speaker 7: All right, I'm Christian.
[140:10] Speaker 8: And I'm Ben.
[140:11] Speaker 7: And this is our product Turbo Grant.
So every year in the US, we're spending about $1.2 billion dollars on grant applications and not even research, just purely the applications.
On an individual level, it takes about 170 hours and $8,000 per application.
And from hearing from researchers in the field, this is a massive pain point and not a good use of their time.
So we took a deep dive into all the complexities of the grant application process and identified several steps where uh AI can assist at a minimum or entirely automate a step.

So first, we start with three main inputs.
The first is a research description, which is a one to two uh page outline of at a high level the research that they intend to conduct.
Then we have scientific figures, which is images of tables, charts, data that supports their research, which we send to an image model, so we can get a text representation of it and we can plug that in at different points in the uh application process.
The last is a 30 to 60 minute chalk talk where researchers can um in free form, uh describe all the research that they intend to conduct rather than dealing with all the formatting complexities and just simply get their thoughts out.
And from there,
[141:50] Speaker 8: And then we have our funding matching agent.
So it's going to take those inputs and it's going to extract it.

--- Segment 15 (Starting at 140:00) ---

[140:00] Speaker 2: answers and then ask the researcher follow up questions to best match them up with funding opportunities for their research.

[140:33] Speaker 3: And then once we know have a good idea of the type of funding opportunities that they're actually eligible for, what we've done is we've taken all of the NIH and NSF grants and we've uh scraped all the web pages and use LLMs to turn them into structured data, so that you can filter quickly for various things like are you doing animal trials?
[141:00] Speaker 3: Is there a certain deadline you need to hit, um, or award amounts.
[141:07] Speaker 3: Um, you can also search through and score, um, different uh, funding opportunities based on uh what you're looking for.
[141:19] Speaker 3: And then even once you get onto the page, we have a chat interface.
[141:24] Speaker 3: So if you need to ask more fine grain questions, you can quickly get answers and as soon as you select it, we'll start generating your outputs.

[141:36] Speaker 2: And then we have our research agents.
[141:40] Speaker 2: So it's going to take the um Emerald Cloud Labs uh equipment catalog and it's going to find the most relevant pieces of equipment for the researchers experiments.

[142:01] Speaker 3: And then the second agent that we've built is a uh sources agent.
[142:07] Speaker 3: So we look through your inputs and see the claims that you're making and the research that you're going to conduct and it finds uh questions that could use support.
[142:18] Speaker 3: And then it will go out using the perplexity API and pull in uh cited sources that you can use later on in your research.

[142:34] Speaker 3: And then the next is the AI writing co-pilot.
[142:39] Speaker 3: So there's a bunch of documents that you need to create for your uh research application and what we've done is combined the inputs that you've given us earlier along with the context of what the documents purpose is and we generate a first draft for you.
[143:00] Speaker 3: And then from there, we have a built-in AI writing co-pilot where you can come up with edits and say things like fix all the spelling errors, condense these paragraphs and you collaborate with it until you're uh happy with the result and it uh simplifies that time considerably.

[143:32] Speaker 2: And then we have our requirements agent.
[143:35] Speaker 2: So this is going to allow you to skip the 300 plus page agency requirement documentation.
[143:44] Speaker 2: It's going to generate a bunch of questions for the researcher and extract the required documents for their specific funding opportunity and their research.

[144:06] Speaker 3: And now that you've got your full list of requirements and you've completed all your documents and you have your output documents ready to go, uh you're now ready to submit in just a fraction of the time.
[144:17] Speaker 3: Thank you.

[144:22 - 144:40] (Silence)
[144:40] Speaker 4: What if we could take product management and bring it to everybody?
[144:47] Speaker 4: This is an app that supercharges product management, Sonata PM, with Chris and Eli.
[144:55] Speaker 4: Let's give it up.

[145:00 - 145:33] (Silence)
[145:33] Speaker 5: All right.
[145:35] Speaker 5: So, building products is easier than ever.

[145:44] Speaker 5: But building good product is still really hard.
[145:51] Speaker 5: You need to know your market, your competitors, your customers, inside and out, front to back.

[146:06] Speaker 5: Pricing, positioning, messaging, and obviously your product, design, technology, the team, what they're working on and when and why.

[146:32] Speaker 5: And to do all that and to know all that, you're working in a dozen different systems at a minimum, and you're having a dozen conversations and writing a dozen documents, doing a dozen edits on it, and it's a lot, it's hard.
[146:51] Speaker 5: And so we're building Sonata PM to make it easy for founders, product managers, for anyone, an integrated hub that reads from and writes from to all the systems you're already working with and equips you with a team of product specialists, AI agents that do research, competitive analysis, documentation, customer insights, prioritization and planning, the whole works.

[147:28] Speaker 5: Um maybe not the whole works just yet, we started with research and documentation.
[147:36] Speaker 5: And to show you what we've built so far, here's Eli.

[147:41] Speaker 6: Thanks.
[147:44] Speaker 6: So, to show you what um our flow looks like right now.
[147:51] Speaker 6: We starting?

[147:55] Speaker 6: There we go.
[147:57] Speaker 6: So, um to get you started with your project planning process, uh we provide a number of templates for documents that are commonly used in project management like user persona.
[148:10] Speaker 6: When you go to fill this out, you're just having a conversation with Arya, our document edit editing specialist, describing your ideal user.
[148:26] Speaker 6: She'll take from the conversation the key points and create the user persona and you can go in and modify it yourself if you want.
[148:37] Speaker 6: It's a collaborative process.
[148:40] Speaker 6: For documents that uh are a little more technical, you can make use of of Reed, our research agent to go and do competitive analysis in the background while you continue to edit the document with Arya.

[149:00] Speaker 6: Uh, this particular project requirements stock is for um a new team member.
[149:06] Speaker 6: We'll be rolling out in a few weeks called Echo that will be taking uh user feedback across multiple sources and aggregating it into one place.
[149:23] Speaker 6: And it looks like our uh research report is done.
[149:29] Speaker 6: So, um we can go in there, grab the relevant points and bring that into our project requirement doc.

[149:50] Speaker 6: We also can integrate into whatever ticketing management software you're using like GitHub or Jira.
[150:02] Speaker 6: So, when you think that your project requirement doc is good enough, you're ready to move on and start building, uh we will automatically generate for you uh a set of user stories.
[150:25] Speaker 6: Once these are generated, you can go over, make any final tweaks, and then from within Sonata PM, send out those tickets to whatever platform you have integrated with.
[150:44] Speaker 6: Here we have GitHub, so click a button and we have a GitHub issue.

[150:54] Speaker 5: Thanks Eli.
[150:56] Speaker 5: So that's Eli Guting, I'm Chris Sutton with Sonata PM.
[151:00] Speaker 5: Thanks.

[151:02 - 151:16] (Silence)
[151:16] Speaker 4: Next up, we have Campbell, who'll be presenting a solution that is linear for event planning.
[151:23] Speaker 4: He's taking Gauntlet AI and seeing if you can manage the ops of all the events end to end.

[151:29 - 151:43] (Silence)
[151:43] Speaker 4: Woo!

[151:51 - 152:46] (Silence)
[152:46] Speaker 7: Gauntlet Ops is the solution to a problem posed by our by our very own Gauntlet Ops team.
[152:58] Speaker 7: I asked them, what one app would help you manage our 120 students and staff the most?
[153:08] Speaker 7: What resulted was a streamlined all-in-one event planning app that focuses on a clinically sharp UI, a focus on performance, and most importantly, a magical feel when navigating through it and overall using it.
[153:36] Speaker 7: The main problem with event planning apps or event planning in general is that all of the apps are fragmented.
[153:52] Speaker 7: So they use Google Docs, Google, I mean, they use Excel, Google Docs, party full and other forms for invites, and they also use Airtable to manage all their data.

[154:15] Speaker 7: So my solution streamlines all of that and I'll show you through a demo.
[154:22] Speaker 7: So we click on Demo Day.
[154:28] Speaker 7: We have the event Command Center.
[154:34] Speaker 7: You see a high level overview of everything you need.

[154:43] Speaker 7: You have favorited time blocks, which we'll get to soon.
[154:48] Speaker 7: We have a fully fledged task manager.
[154:55] Speaker 7: We have vendors, and of course, we have a way to manage our participants.
[155:08] Speaker 7: Last but not least, we also have a way to manage our budget.
[155:13] Speaker 7: Let me go through with the timeline first.
[155:17] Speaker 7: We have a beautiful timeline view of the entire event.

[155:27] Speaker 7: It's color coded by the actual part, not the priority, but the status of the event.
[155:34] Speaker 7: So we have pending, in progress, and complete.
[155:41] Speaker 7: You can click in on it, get all the details you need, add it to your favorites, so you can see it on your event Command Center.
[155:52] Speaker 7: And you can also assign it to a task.

[156:04] Speaker 7: Next, we have the actual fully fledged task manager, color coded by priority.

[156:20] Speaker 7: You can assign it to a logistics team member to deal with.

--- Segment 16 (Starting at 150:00) ---

[150:00] Speaker 2: you can assign a budget item to it. So you can make a task, assign a budget item, and deal with that on a different page.
You can also chat with it with a real time chat with your team members and of course you can make the budget item straight from this page.

Next is a critical page, basically the financial and the budget page. The important thing here is that we have a total budget but we have planned expenses and actual expenses. This actual expense and this budget health are the critical things that stakeholders need to know.
Basically you need to know an easy way to track if you're on over or right on budget.

Also from this page you can add budget items straight from this page.
You can also see what vendors those budget items are connected to. Last but not least, we have a Rolodex of vendors. Over like these last eight weeks, of course there's going to be way more vendors from these from our event planners.
So we have an we have an easy way to filter sort them.
We have all the info we need here and they're also all intricately connected to the budget items.
And last but not least, you can upload any docs for that vendor.

So let's say you have receipts, you have menus, you have any emails from that vendor, you can connect it here.
Essentially you have an end to end event planning app that's ready for iteration based on any feedback.
Thank you.

[153:42] (Silence)
[153:49] Speaker 3: Next up, we have Cameron who tried to use AI to do graphics programming.
He's going to show you what he was able to render and how he went about it.

[154:27] (Silence)
[154:30] Speaker 4: Thank you Ash.

[155:06] (Silence)
[155:33] Speaker 4: Morning, I am Cameron McWilliams and for the past three weeks I have been working on a Ray tracing render.
Um, so in traditional rendering, uh the only thing you get is the projection of the scene onto the camera plane and this gives relatively little information uh to work with for actually shading the scene.
Here instead we are casting rays out into the scene and by simulating the paths of light reflecting off materials, um, we can use very accurate reflections to get um, specular surfaces like this metal ball in the front.
Um, we can use uh fuzz and randomized reflections to get um rougher surfaces are all of these diffuse balls and we don't even have to have reflections.
We can have uh rays um go through objects and um that's how we get um physically accurate glass refraction.
Um, this is a solved problem, so why did I make another renderer?
Um, the problem that I had with currently existing renders is that um they tend to um have some shortcomings for prototyping rendering techniques and procedurally generated assets.
Renders generally fall into two categories with respect to this.
You have renders like um Renderman and Cycles, which in principle can be used for this kind of work, but they are very opinionated um renderers because they are designed for production use by artists, so they're not super ideal.

The other um kind of render you have is renders like Adobe Substance render which is ostensibly for this purpose, but they can be a non-starter for a lot of projects because their model is proprietary.
Um, so you don't fully know what you're doing.
Um, during these three weeks I focused on um texture generation.
Um, I'd like to say that I reached uh future parity with substance designer in three weeks.
Um that's not true, but um it is uh got a suitable array of uh features for making and rendering textures.
Um, so we do uh AI first building here.
So a question you might have is, how does AI do on graphics programming?
And uh the answer is, um, at least for writing code, not great.
Um I've got mixed results with it.
But AI still has a lot of uses in uh graphics programming.
You can cruise through boilerplate.
There's like 2000 lines of Vulkan boilerplate that I did not have to write most of because I can just tap through it all.
That's very nice.
Um, you uh don't need to use Google anymore because uh LLMs can give you uh synthesized info and contextualize it in terms of the code you've already written, which keeps you in the flow and keeps you iterating faster.
And finally, uh you have the world's smartest rubber duck to consult.

So while uh LLMs are not super great at writing code um for graphics, they are still surprisingly good at reading and understanding it.
Um and Claude often was able to find bugs that I made faster than I could.
Um, so does AI have a place in graphics programming?
Um the answer is a resounding yes.
Um, if you're interested in uh someone who thinks this is a fun use of three weeks fiddling with uh rays and vectors, um, please get in touch.
Um, and thank you for your time.

[159:37] (Silence)
[159:44] Speaker 3: All right, next up we're going to have Azaldin, who's going to be presenting a minimalist extensible reservation system, and he's calling it Romeo.

[160:15] (Silence)
[160:30] Speaker 5: Hello everyone. Uh Romeo is an extensible abstract minimalist reservation system. Uh the system is comprised of two main components, uh reservables and reservations.
The abstract uh nature of the systems allows for extensibility.
Uh the following reservable has been created for demonstration purposes. Uh reservables may be associated with child reservables as follows.
Uh an end user may create reservations as follows. Um, add notes and confirm reservation.
The power of the system lies in the um constraints and validators that may be associated with reservables.
Uh constraints and validators uh uh uh may be used to uh constrain the reservations that are uh created on a reservable.

For demonstration purposes, um the a single validator will be tested.
Uh the uh an end user may specify a simple description in uh natural language.
For example, give me a 15 minute break after every reservation. Um, the system will take the description and um generate a corresponding uh uh uh validator uh that will be applied against every reservation creators for the reservable.
Uh we'll go and create the uh validator.

Uh after the validator is created, we can test the validator by attempting to create another reservation.
Uh uh attempting to confirm the uh reservation uh returns the following error.
This demonstrates the uh extensibility of the system.
Uh uh infinitely many uh validators or constraints can be applied against the reservation system.
Uh the system is very minimalist. Uh um if interested, uh um uh um one may reach out to uh to me personally to gain access to the system.
Uh this concludes the presentation. Uh thank you very much.

--- Segment 17 (Starting at 160:00) ---

[160:00] Next up, we have Ayush, who's going to bring you Vibe Check. It's going to be day emotions and how you're feeling over time.

[161:17] Alright.
[161:18] Speaker 2: Alright.
[161:19] Speaker 2: Hello everyone. Uh, my name is Ayush Shah. and, uh, today I'm going to be presenting you guys, uh, an app called Vibe Check. And this app addresses a problem that I personally had throughout my first few years of a college, where I felt like I was living life on a sort of like a autopilot mode, where I would wake up, I would go to the same classes, I would interact with the same people and do the same things day in and day out, oftentimes for several weeks to months. And a lot of these times I wouldn't be feeling too well. I, I wouldn't really know if this is what I want to be doing, and I wouldn't really be able to make any changes to it because I was just so like used to the same environment. And what I wanted to do is come up with a solution to help people break out of that habit and allow them to have a way for someone to actually tell them what they can do to better improve their wellbeing.

[162:49] Speaker 2: So, Vibe Check is the most simple way to track, understand and improve your overall wellbeing. And the problem that we're solving, kind of that we're addressing is the high stress rate amongst adults. Uh, obviously, they can uh, adults really struggle to identify different happiness factors that they're having. And also, they have reportedly, um, stated that they would be willing to take guidance and uh, take action if they were given proper feedback on what to do to improve this.

[163:35] Speaker 2: So, this Vibe Check allows you to input your mood and different voice notes throughout the day, and it's a single action app. And so all this will be in your specific individualized data store. And over time, you'll be able to accumulate enough data to talk to your AI voice assistant and be able to get proper feedback to you and tell you if you need to do something that maybe can potentially improve your overall wellbeing. And with this, I'll take you guys to the live demo and I'll show you, um, how I'm using this app.

[164:55] Speaker 2: So as you can see, um, here we'll just input the mood, how how the user is feeling. Um, they'll have a very easy to do a way to do that. They can add voice notes here, as you can see. Um, they can save how they feel. Uh, they'll have a chance to go through different uh places such as their timeline, view their past history. Uh, they can make changes, uh, very easily and we want to really emphasize the user experience. Uh, this is the voice assistant, we'll come back to that. Uh, here you can see your overall insights has the how they have changed over time. Uh, you have a easy way to reference all of this and go back and just see overall how you're doing over certain periods of time.

[166:38 - 167:42] (Silence)
[167:42] Speaker 2: I've been looking over your mood data, and I can see that your overall mood has been declining over the past few weeks. Let's take a closer look together and see if we can figure out why. One pattern I've noticed is that your mood tends to dip on Mondays and Thursdays, especially after your CS theory classes. For example, last Thursday, your mood dropped to a 310 after your class and your note mentioned feeling frustrated and lost. It seems like these classes might be a significant source of stress for you.
[168:16] Speaker 3: Yeah, I think so. Um, I don't really like these classes. Um, I think I might be a change of major. Um, could you maybe help me, um, point me in the right direction maybe with, uh, how I can change my major potentially to something maybe I'll enjoy?
[168:29] Speaker 2: Got it, Alex. It sounds like you're seriously considering a change, and that's a big step. I'm here to help you think it through. Looking at your mood data, there are some interesting patterns that might help guide you. For example, one, on March 16th, you logged a 710 mood during a discussion about psychology with a friend. You noted feeling curious and engaged. That suggests psychology might be a subject you find stimulating and enjoyable. Two, your mood consistently improves during creative or social activities. For instance, on March 19th, during an art history class, you rated your mood a 710 and mentioned enjoying the storytelling aspect of art. This shows you might thrive in fields that involve creativity or connecting with people.
[169:28] Speaker 3: All right. Yeah. So, um, that is the voice demo. Uh, sorry we couldn't get on the first try, but, uh, connecting this and as you can see, it's able to get personalized feedback from the assistant and it's able to identify different patterns, patterns in their day-to-day uh changes and voice notes and give them very applicable and actionable advice. So, thank you so much.

[170:20 - 170:50] (Silence)
[170:50] Speaker 2: Next up, we have Asher, who's going to be showcasing Trace, the next way to journal with AI.

[172:14] Speaker 4: Hey guys, I'm Asher, and today I'm going to be presenting Trace, which is the new way to journal. So, what I'm doing is because many people want to journal, right? But the problem is that some people don't have time or don't want to spend the time just taking 30 minutes out of their day and sitting in the computer, staring at the screen for 20 minutes to write down your day, what happened, format everything. So what I've built is Trace, which lets you just chat with AI and you can it'll format your entry, you just talk to about your day, go back and forth, tell it what you did, uh how you're feeling, and then it'll format that, store it for you and then you can view it, you can go in edit it, change, whatever you want. Um, talk with it more, change. It's just to make it really easy to journal and just do that.

[173:45] Speaker 4: So, I can show you a demo right here. Um, let's see. As you can see, it has UI, you can enter things you want, but then the main thing is the voice assistant. So, I'm talking to it, I'm like, oh, hey, I went climbing today. I'm feeling pretty tired after that. Uh, I finished my new favorite book, which is Project Hail Mary. You'll see it'll pop up there after it thinks for a while. Um, and it formats it for you. And let's see, after it's thinking it will show you a little preview, so you can make sure what it's going to enter is what you want. If it's not, you can just accept it and then edit it yourself. There you go, that's a preview. That's what it got from my little talk with it. Um, you can accept it. There you go. You can see all the data entered, formats everything nicely. And, um, another thing we did is some places don't let you have your own data very well, like it's all stored through the cloud, you can't really access it. So what we did is we made it all stored locally, you can access your data, it's formatted in markdown. You can just view it and open it in whatever you want and look at it there.

[175:24] Speaker 4: So, that's Trace, and thank you guys for your time.

[176:32] Alright.
[176:33] Speaker 2: Next,

--- Segment 18 (Starting at 170:00) ---

[170:00] Speaker 2: Next up, we have Ali with live voice translation and his app Vocal.

[170:28 - 171:40] (Silence)
[171:40] Speaker 3: All right guys, hope you're doing well.
Um a common problem that I think we all run into is especially if you're traveling, but even if you're not traveling, just in the last two months that I've been here in Austin, Texas, sometimes I want to have uh voice translation.
Despite having over 600 days in a row on Duolingo, my Spanish has not improved to a level that I can communicate with people very easily.
Just in the last week, uh I had a delivery driver who couldn't speak English, and I was trying to get Rebecca's food actually up here, but he wouldn't give it to me.
And so she had to go downstairs herself.
Another time, I was in an Uber, I wanted to get dropped off on the other side of the street, and I had the same problem.
So, we have a lot of solutions for this, um and I've built a better one.
Uh I'm not alone in this.
One of the top tech influencers on Twitter, Levels.io, he speaks Dutch, English, and some Portuguese.
He was on a rant a few weeks ago talking about how he needs a better voice translation app.
And so a lot of people had attempts in their replies, and he ended up detailing exactly what he wanted.
And so these are all the specifications that he was talking about.
The industry leaders right now are Google Translate and Microsoft Translate.
Apple also has a translation app.
Uh but they just don't get the job done, and you can see why.
So, on this side, we have Google Translate.
I was speaking with my friend Drew Sutterquist.
He was speaking in English and I was speaking in Spanish.

And so, you can see that you have to wait for the person to be done talking, you have to press this button every time in a lot of languages that aren't supported in conversational mode, and then you hear the translation.

Uh we're supposed to at least okay.
Okay.

On this side, we have Microsoft Translate.
So, the UI there is a little bit better.
Um you can see that, you know, like there's a split screen view, but the same problems kind of exist there.
Now moving on to vocal.io.
So let's have a volunteer from the audience, perhaps this handsome gentleman right here.
And uh, we'll go to the live translate page, and we'll have a conversation.
I have forgotten how to speak English, so I'll speak in Urdu, and he'll speak in English.
All right Asher, whenever you want to start.
[174:13] Speaker 4: Hey Ali, what do you oh hey.
What do you think about Austin so far?
[174:18] Speaker 3: I'm having a great time here.
Oh sorry, I was supposed to.
All right, let's try that one more time actually.
I forgot to switch languages.

All right.
go ahead Asher.
[174:36] Speaker 4: Hey, what do you think about Austin so far?
[174:41] Speaker 3: Uh, mujhe abhi tak bahut accha lag raha hai.
[174:45] Speaker 4: Nice, I like it pretty too, pretty well too.
What do you think about the weather here?
[174:53] Speaker 3: uh mosam thik hai, pichle hafte behtar tha.
[174:58] Speaker 4: All right, and as you can see, we can also click on the numbers here and hear the translation.
So, I'll try it here.
Uh, the audio is not working.
Uh.
Let's go back.
All right.
Like uh, it's it's supposed to be on headphones, all right.
Okay, we'll do MacBook Pro speakers instead.
okay, let's see if it works.
[175:39] Speaker 4: acha mujhe ye bhi bahut pasand hai bahut acha bhi yaha ke mausam ke bare mein aapka kya khayal hai
[175:49] Speaker 3: and then he said, and I said.
[175:51] Speaker 4: the weather is fine.
It was better last week.
[175:55] Speaker 3: So, you can also adjust the speed and uh yeah, that's about it.
Thank you very much.

[176:18] Speaker 2: So, how many people here have heard of Gathertown?
If you've heard of Gathertown and then you're going to like this next project, it's called Hacker House where AJ is bringing AI agents to your virtual office.

[177:11 - 177:15] (Silence)
[177:15] Speaker 5: Hello Gantlet AI demoday.
How's it going?
I'm AJ and I'm here to tell you about Hacker House AI.
So, I think the group here is probably a little bit more familiar than most with the sort of tension between remote work and in-person office work.
Some people have really, really strong opinions about why one is purely better than the other, but I think a lot of you guys know that the truth is more nuanced and there's actually tradeoffs between remote and office.
You actually want kind of the benefits of both remote plus office work.
You want a remote office.
And there is a product that already kind of solves for this.
It's called Gathertown.
I used it at a previous company.
Um it's pretty good.
It's like a short little uh like video game world where you can walk around and talk to people, and it does actually do a good job of um bridging a bit the gap between the in-person experience and remote.
But I actually think we can do better than Gathertown because after all, it's not 2020 anymore when they took off.
It's actually 2025 and we have AI now.
And that means two things.
It means we should be able to as an individual developer get a product like this up and running in about two weeks.
And we should also be able to have AI agents in this world.
It's kind of a no-brainer.
And so you want AI agents that can be able to talk in person, that can interact with MCP servers or APIs to like do interesting things in the the virtual office environment.
And then that really will like connect you and your AI agents to the broader world in in a virtual space.
So, that's where Hacker House AI comes in.
Hacker House AI helps you get the best of both worlds for the in-person and remote work.
It helps you keep the culture alive even if you have like a globally distributed team of top talent, which is kind of what's required to get the top AI builders right now.
And then it helps you uh allows for interactions between agents and people seamlessly in a way where there's not like a huge divide between those two things.
So you can imagine this for offices like Gathertown.
You could also have it in a school context or even just meetups, any place where people and AI agents connected to another service might meet.
So just to show you a bit of what I've built the last two weeks, here's a demo from earlier.
So when you enter, you're in the public chat context where anyone in the workspace can talk to each other.
You can see who's online, what's going on.
But if you walk into a conference room, it'll immediately switch to the private meeting room context for that meeting room.
And if you walk up to somebody, your DMs open, seamlessly, a very smooth experience.
You might notice that not everyone in this workspace is a human person.
There's also bots, and you can walk up to them, and you can ask uh requests such as this one where I'm asking it to tell another coworker in the office space about uh commit on one of my other repos.
And it tells me it got the message before using a tool to walk over to him.
And then from uh his perspective, the second half of the video, you can see it got that info and told him about it.
So, walking you through what just happened, we have a person who's talking to a bot, the AI agent, that has access to three different tools.
First it's using Webrag to actually get data from the internet live on GitHub outside its training window.
Then it's using our tool to insert itself and walk around this office space.
And then another one of our tools in order to initiate rather than merely receive messages from uh other people in the office, um who then get it.
And you could imagine this interaction happening not just with people, but it could also talk to other bots.
And I think that's really interesting that like um not only can it talk to people or bots, it also can access more than just GitHub, right?
You can add notetaking, educational content, um coding tools, other chat services, data analysis, or even some of the video agents that people have built could have a bot presence in in a site like this.
But the most interesting thing is when you have multiple AI agents like this, right?
And they can all.

--- Segment 19 (Starting at 180:00) ---

[180:00] Speaker 2: all talk to each other on Hacker House AI and collaborate um in a place where they're interacting not only with each other but humans that then can see what's going on and interact with them as well, super seamlessly.
And that's why I'm really excited to have um brought this to you today.
Thank you guys.

[180:47] Speaker 3: All right.
Next up, we have Paul who's going to be going over Embedding Bridge.
Embedding Bridge is going to be the easiest way to manage your vectors embeddings over time, and we're also going to be setting up for a surprise as Paul after Paul's presenting.

[181:42] Speaker 4: Hello everyone.
I'm Paul Jermal and I'll be presenting on Embedding Bridge, a gift for embeddings.
This came about when I was trying to define a recipe data set that was pre-embedded.
Obviously, I looked at hugging face and I was curious why I see all these data sets but they're not pre-embedded.
There isn't a public repository or place where you can just find embeddings that are already pre-embedded and any sort of provider, be it open AI or Voyage Voyage 2, which is in the embedding model that claude or anthropic uses.
And so I had to embed them all these thousands of documents in half an hour.
And I realized, well, there should be a site or a place to have all these embeddings hosted, so we can save costs and time.
And I realized that the next step would be in the basic rag pipeline to create sort of this embedding bridge.
And that to connect like a glue between the documents and the vector DB.
So you can manage version your vector embeddings across any provider, right?
You have so many documents and your documents could have multiple embeddings.
Being able to version them is probably pretty important, right?
So this is sort of like the glue between the vector vector DB and documents.
So we can pick any sort of vector DB.
And so I will be highlighting few of the commands that I created in this command line interface in C.
One is EB roll back.
And EB roll back, the main point of this is that you have models, multiple models and multiple histories.
And you can roll back from your current to a previous vector embedding for that document.
So I'm going to give you a quick sort of run down and may be flying fast, but keep your eyes peeled.

And so we're initializing.
We're registering some models.
We're restoring.
And don't worry, there are Python bindings for all your data science out there.
We're not fami not uh not loving the CLI as much, but and so we're seeing a log.
You can see for Voyage 2, I'm rolling back to a previous history and you can view that.
Next is EB diff.
We're going to analogous to, so Git diff.
You guys are aware of that.
And I was thinking, how can we do this for vector embeddings?
And I was like, similarity, right?
Similarity between vector embeddings.
So here I'm I'm trying to showcase sort of preliminary of how that would work.
So, within the Open AI 3 small embedding models, I'm I'm showing the the diff between um it's showing the cosine similarity and you clean similarity as well.
So EB remote.
Remember, these are vector embeddings, and vector embeddings can be large, right?
They take up a lot of your space and you want to just manage them, obviously version them, but also put them out in the cloud where you could then maybe plug them into other vector databases.
So, what I'm doing here and the capability that it's available at the present moment is to be able to push them into S3 at the moment.
So I'm here I'm push pushing to S3 and it's sh showing up as a list.
And we're pushing, Bam.
And then we can load them onto your vector databases.
Okay?
So, as you can see, I'm I'm loading them from the the S3 path onto the pine core vector database and it's all loading up.
So there's a specific schema going on here that you can put them in to the to the the remote, the cloud end, and these could be then pulled into any sort of vector database.
So just a one stop shop to keep all your vector databases, not worry about the the vector databases at all, just manage them all and that that is the goal with this.
And I I'd like to give out some of the command lists that I have available here.
Um, one that I would like to keep uh show show off is EB hooks.
Think of this, man, like if you have get documents and you're ragging across them and you make a change to a document, you want to be able to generate an embedding and keep track of those embeddings for that document.
Like a get commit, and boom, generated embeddings.
So this is an aspirational goal of mine is being able to make it easier for developers to manage version their vector embeddings.
So, here is a GitHub repository.
Uh, go ahead, take a screenshot, take a look.
I have Alpha V2 out, and and I would love your feedback, your issues, okay?
This on the CLI, I have Python bindings, and I'd love the developer feedback.
Um, thank you, uh, thank you for listening to my presentation on Embedding Bridge.

[188:18] Speaker 3: All right.
For the next part of our program, we're going to start a live speed build.
Now, you must have seen Austin's tweets and you must have seen how he says that people are building things in in 10 hours, in 24 hours, in 36 hours, but it's a different story to see it happen live.
We're going to bring on three contestants and this is going to be our Gantlet AI speed build challenge.
Each contestant is going to have a hype man.
The hype man's going to tell you what AI tool are they're using, cursor or wind surf.
They're going to tell you what models they're using.
They're going to talk about their process and they're going to let you know how they can build so fast.
So let's welcome our contestants.
Abraham, Robert, Campbell, Anthony, Spencer, and Benji.
So let's set up station one.
Station two.
And then grab your mics.

Grab a mic for yourself.
Uh, oh, Anthony, this is your mic.
All right, so let's look at the setup here for just a second.
Each of the hype man is going to be watching the screens of the competitors.
They're going to let you know what's going on and they're going to give you insight into what's happening.
Now, before we get started, we need an idea for an app they can build live.
Now, I want the audience to know that we did not plan this.
In fact, we told them like 10 minutes ago.
So, let us know an idea you think that you want them to build right now.

[190:23] Speaker 5: So content game where you can play on your phone and you can see what other people are playing.
Is that kind of right?
And then you can see the card of what everybody's doing?
Oh, cards are getting.
Okay, so like a card game like cards against humanity that you can play on your phone.
Okay?
All right, so that's going to be the idea that these three competitors work on.
Let's put the timer up.
So yeah, I'll give you.
So we have 10 minutes.
10 minutes to see if they can make something like cards against humanity for the mobile phone.
And what I'll be doing is going through each of these contestants and we'll be walking through what they're doing, how they're doing it, and the approach they're taking.
Then we'll have them come up to the podium and then we'll have people judge.
Ready?
Five, four, three, two, one, go.
[191:38] Speaker 6: All right, Team Robert over here is starting with Wind Surfing.
Unique choice.

Team Campbell appears to be using cursor.

[191:59] Speaker 7: Anthony, let us know what uh Abraham's doing.
[192:02] Speaker 8: Yep, we're going on we're on cursor as well right here.
What models are we using?
[192:08] Speaker 9: We got Claude 3.7 thinking going on right here.
We'll see if he changes.
It's still early.
[192:17] Speaker 10: Over here we have Claude 3.7 not thinking, just the original Claude.
It seems like he also made the boy the bold choice of using Bun instead of Nodejs.
[192:27] Speaker 3: So it it I think it's going to be have to be a mobile app.
So what are the mobile app approaches going on?
Is it a PWA?
Is anyone coding natively?
Are we just making a web app that's responsive?

--- Segment 20 (Starting at 190:00) ---

[190:00] Speaker 2: For the audience he said he's going for a PWA.
[190:05] Speaker 3: Yep, we've got a responsive web app going on here.
[190:10] Speaker 4: Yeah, we're going to go web up over here, it looks like.

[190:15] Speaker 5: Building for mobile, the bottleneck is still uh deployment. There's a lot you have to do to even get it running on a device. So that will alone take 30 minutes, but actually creating the mockup will be much shorter.
[190:32] Speaker 6: Honestly a good choice. Apple kind of holds you in a bottleneck using X code, you know, it's it's even just set up alone, it's it's kind of tough. Curser makes it really easy though.
[190:43] Speaker 7: Move the mic closer here.

[190:46] Speaker 5: All right, we've got our first prompt fired off asking for a detailed PRD for this mobile app.
[190:56] Speaker 6: Campbell might be pulling ahead because he's already fired off multiple prompts.
[191:02] Speaker 8: What do his prompts say?
[191:04] Speaker 5: Good question. Right now there's a lot of text from the LLM. Let's see the prompts. We're pivoting to mobile optimized game. Mobile can be a challenge for curser, but I think he'll prevail.
[191:24] Speaker 6: Over here the strategy is we're building a PRD in a markdown file to kind of set it up for the AS to follow along, it looks like. So instead of going right into building, we're first we're planning it out with the AI first.
[191:40] Speaker 5: Yeah, we're being humble, you know, we're letting Claude think for us. These guys think that they can think better than Claude, but I don't know.
[191:49] Speaker 7: All right.
[191:50] Speaker 6: You got to tell us a little bit about the prompts here. I mean, that's what everyone's trying trying to look at. Like what are these people doing that they don't know?
[192:11] Speaker 8: Yeah, bring some of the prompts as they come in.

[192:15] Speaker 5: Right. So the first prompt we fired off was more or less the prompt we were given. Uh, let's see. Yeah, build a running responsive mobile app like Cards Against Humanity, mock up multiple users and have it runnable in the mobile browser as well as desktop, make it beautiful, make it delightful and pull in great content from the web. Start by writing a PRD. So we got our PRD and then we asked for let's see. Yeah, make sure other users can join for multiplayer and build it and run it. So we'll see where we get.
[192:54] Speaker 6: This prompt says, should we remove the sidebar, should we change it? So not only can Claude write the code for you, but it can also help you design the UI.
[193:05] Speaker 8: What's it running on?
[193:08] Speaker 6: So we are we're on the PRD right now and looks like he's making some edits on different aspects of the CSS and he is going to say, I see we already have a Nextjs product set up with Typescript. Let's check the package Json to see if we have all the dependencies we need.
[193:34] Speaker 5: All right, we got Veeding right now.
[193:41] Speaker 6: We're about to hit the six minute mark. Six minutes in, uh four minutes in, six minutes left to see who can make a Cards Against Humanity clone to see if there can be any multiplayer game that you can make in just 10 minutes.
[194:00] Speaker 9: We're about to fire off another prompt. Campbell is furiously prompting over here. Why is the game called cards against mobile while our simple core gameplay loop, we have five minutes left. So looks like cursor does not realize the gravity of this situation. Campbell is telling cursor to hurry up.
[194:24] Speaker 9: Campbell is in the zone. He's just looking at the screen right now. Oh, there's a next prompt. We have a console log error.
[194:37] Speaker 5: And over here Claude has decided to try a different approach, something that it does quite often.
[194:44] Speaker 6: Over here we're iterating through the PRD right now. Cursers is on an absolute tear. He's on probably on his seventh or eighth file right now. is setting up the entire project based off the PRD that we've given it.
[195:00] Speaker 9: Now, each of these contestants have honed in on their AI coding framework. It's something that we emphasize at Gauntlet. Everyone's going to have their own methodology. Everyone's going to have their own way of coding with AI, and if you can hone in and do it in 10 minutes, you can definitely do it in 24 hours.
[195:23] Speaker 5: Yeah, right here our project structure has a client folder and a server folder, so I can see it's setting up for multiplayer right now.
[195:35] Speaker 9: I cannot imagine the stress these coders are under. I'm so glad I have this job instead of coding.
[195:41] Speaker 5: Can you imagine the stress that Claude is under working on three at once?
[195:50] Speaker 6: So we have our first website up. It's looking a little basic. We're going to start adding more functionality. Make sure everything's clickable here too.
[196:00] Speaker 5: Index.ts is coming up. It's a good sign.
[196:05] Speaker 6: Now we're taking screenshots, feeding that into the AI so they can get a better idea of what we're seeing on the front end too.
[196:13] Speaker 9: We have another prompt coming through. Is this the best next step for the final four minutes of the challenge. So it looks like Campbell is once again trying to make cursor realize that we're on a time crunch.
[196:40] Speaker 9: Now, has anybody done uh the game logic yet? Come uh let let me know Hypeman, like is anybody working on this cards against humanity app and like how the game is actually going to function?
[196:59] Speaker 5: I mean, not much. The thing is when you're starting what did you say?
[197:03] Speaker 5: Oh yeah, Claude's working on it. That's the thing is when you're doing something like this and you're trying to make a full app in 15 minutes, you just want to see what Claude comes up with. So you give it the idea and yeah, I can see Claude making all the game logic, but it's got a lot to do. It's got to set up the server, it's got to set up the pages, all the logic and everything. Any sort of tweaks are are going to happen after the 15-minute mark. So we'll see what Claude gets in this 10 minute time.
[197:38] Speaker 6: Over here too, we're working on the front end to make sure that everything's looking good so that we don't have to kind of mess with stuff on the back end. It's really a pain when you set up all the logic and then you have to add a button, take away a button, change routing, stuff like that. So we're setting up the front end here first.
[197:55] Speaker 9: Is there something unique that each of the contestants is doing? Maybe like uh some of the document documentation they used or some of the ways that they're interacting with cursor. Can we give some insight to everybody watching live as to things they can pick up?
[198:14] Speaker 5: There's no time for documentation in a short challenge like this. That's why Campbell is sticking completely to the essentials.
[198:27] Speaker 6: Big thing we're using here is synthetic prompting. kind of similar to you might hear synthetic data. Again, we're having the AI generate the prompt itself and then going off of that.
[198:37] Speaker 5: All right, this is huge. Claude says that he's going to set up and run our cards against digital humanity game. So we're about to see what we get right now.

[199:07] Speaker 9: Campbell has run into a couple of errors throughout the challenge, but he's taking these in stride. Claude is seeming to handle these errors pretty decently and by the end of it, it should be a decent product.
[199:24] Speaker 6: Looks like Claude's going to rewrite the entire piece of code. We're running into a little bit of an error here.
[199:29] Speaker 5: Well, Claude told us that it was going to build it and now we're looking in its thoughts and it says it needs to continue building. So we'll we'll see if it churns it out.
[199:42] Speaker 6: Two minutes left. It's getting intense.

[199:48] Speaker 5: I see a pretty nice UI here. I wonder how the functionality looks.

[200:08] Speaker 9: All right, it's about two minutes, so we're going to start seeing some of the things that they've built. See how far they've gotten in about eight minutes and then hopefully we can examine some of their prompting.
[200:30] Speaker 9: Campbell's using a very advanced prompting trick. He just told Claude in all caps to think deeply.
[200:45] Speaker 8: I don't do any thinking.
[200:47] Speaker 9: He just said I don't do any thinking for the audience.
[200:49] Speaker 5: We have Robert screen up here.
[200:53] Speaker 8: Thought process.
[200:57] Speaker 9: And we need to change light mode so you can see properly.

[201:17] Speaker 5: All right.
[201:18] Speaker 9: All right, so Claude is saying, I need to continue building Cards Against Humanity game. Let me update the app file with our game components and then proceed to create the necessary components.
[201:37] Speaker 6: So we've got functionality going in over here as well. We're starting to get buttons to show up and drop downs as well for setting up the game.
[201:49] Speaker 9: Looks like the only thing holding us right now is going to be the actual uh LLM, not these contestants.
[202:00] Speaker 9: All right, let's let's disconnect and see where the timer's at.
[202:06] Speaker 9: 20 seconds left.
[202:10] Speaker 6: We're getting our last prompt. We're using all caps, use Shad CN for the components.
[202:18] Speaker 5: We're just trying to get rid of Linter errors right now.
[202:22] Speaker 9: 8 7 6 5 4 3 2 1. All right, let's give it up for our contestants.

[202:38] Speaker 9: Building an app in 10 minutes is not an easy feat. No matter how far they've gotten, they they were quicker than you. All right. So what we're going to do is we're going to get up to the podium and we're going to showcase some of these applications and the crowd here at Gauntlet HQ is going to let us know who won. Simple? All right, we're going to start with Abraham. You want to connect it?
[203:04] Speaker 5: Sure.
[203:06] Speaker 6: We're we're building right now. Let's see if we can even show it.
[203:10] Speaker 9: And you know, since Claude is doing something, we're going to we're going to let their last prompt finish. It's only right, right? If the LLM is the thing that's slowing us down, we got to let that prompt complete and he'll get to showcase what he has.
[203:24] Speaker 5: Skill issue.
[203:27] Speaker 9: Oh, Spencer.
[203:29] Speaker 10: Woah.
[203:30] Speaker 5: I'm confident in our team.

[203:44] Speaker 5: All right.

[203:55] Speaker 6: Sure. So here's here's what we currently got. We got some game set up here. The problem we're having is

--- Segment 21 (Starting at 200:00) ---

[200:00] having it, everything was going to the top of the page and that's why we had that last last prompt saying to just use Sha's end. Um, and so, looks like we have able to set up happens if we hit start a game? Still dead, actually. Let's Let's try and go back. And so this is what we have for the front. So, everything's kind of on the on the top, it seems kind of unusual, but already 9,000 players online, though. Can you look at that? All right. Let's give it up for Abraham. Next up, we have Robert. Oh, our prompt still rolling. We're on we're on minute eight of this prompt. All right, so maybe we'll give it Maybe we'll give it over to Campbell, see what she has. Here in a second, Campbell will be showing off his app. Go ahead and show it off in the best way you see fit and I'll just be here to announce what's happening. We have We have Cards Against Mobile. We have Cards Against Mobile. I have literally no idea if it works. Let's do a live test. I gave him instructions because it's been a while since I've actually played it, too. It says, read the black prompt card at the top. What's the next superhero movie about? Select a white answer card from the hand. Does anybody have what they want to pick? We have it Let's try it out. That's a nice animation there. We have nice hover animations to make it beautiful. Now, the judge picks the answer. I have to judge these four cards now. Does anybody want to be a judge? I did not verify this. Looks like Claude got creative with its cards. Then we have start a new round. I don't know how many rounds there are. I'm just going to skip through it. Hopefully, it's not too many. This is looking like some incredible functionality and user interface design. How many rounds is there? I don't think it's counting because this should go up, right? So I think it's I guess it doesn't have the rounds. It just has the play. And that's it. Good job. Let's give it up for Campbell. He has the gameplay working, the cards are being generated. You can see the players are playing with you, and he has the judge and the player selection down. Let's give it up for Campbell, everybody. All right, our final contestant, Robert. Even after that demo, Campbell is a little bit self-critical. He's saying there are still things he could have done better. But I think it was incredible. How would you improve, Campbell? I'll give at least another one to two minutes to actually verify it. Like, testing is not fun, but it's critical because it has the core functionality, but if it doesn't work in the end then if someone has the end-to-end functionality working, like uh Robert, then I would consider that a win, if it actually works. Well said. All right, well Claude says this works. Yeah, Claude, yeah. Claude says this is running and it tests on the command line, but when I run the actual page, um, get nothing, so I don't know. That's uh I have a like, it was doing all kinds of multiplayer stuff and all It's grand, it would be fancy. Yeah. That's the funny thing about doing these speed builds. Is there's a whole app in there, but if you can't get it running, it's not impressive to show you guys. And in 5 minutes, we could get it running and uh we'll see a an entire app that's hiding there. Yeah. Um Oh. So, uh do you want to plug it in? So we're going to plug it in here and show the blank screen and he's going to come back and actually show you the application in 5 minutes. But I think the audience would agree with me that the winner is clear. Let's give it up for the winner of our speed build challenge, Mr. Campbell Cow, ladies and gentlemen. Now, it takes a lot of guts to get up here and to just start building in front of a 7,000 people. So please give it up for all of our contestants and please give them a hand for giving up, getting up here and trying. They're going to be posting this application on their X account completed for you in the next 10 minutes. So if you want to see that, feel free to follow Joint Gauntlet AI. All right, thank you guys. All right, we're going to be doing one more round. So, I'd like to bring up our next set of contestants. We have Lamar and Gary. We have Jared and Jefferson, the Lambert brothers. And we have Brian and Joshua. Hello, hello, hello, hello, hello. Mic number one. Hello? All right. So, we have a request from the X audience that we're going to be taking. And that request is to rebuild the app that we all hate, Jira. So, we're going to be taking 15 minutes on the clock and we're going to try to see how far they can get to build Jira. Now, each of these individuals have their own hype man, similar to the round we had earlier. They'll be letting us know what prompts they're using, what tools they're using, what process they're following. At the same time, we'll connect some computers to the screen so you can see the process that they're going to take. Does everybody understand what they have to build? I I would just I would just like to clarify that this is very much not possible, so we can make you a nice page. A nice page coming right up. Skill issue. Granted. All right, let's go. Ready? When do we start? All right. Are we all ready? All right, Jefferson. Let's get the timer on the screen. It's going to be 15 minutes. All right, 5, 4, 3, 2, 1 speed built. All right. I know some of us are not as familiar with Jira. I think there are two one or two contestants who have never used Jira. Um, Jera is just going to step into cursor real quick and uh he's been taking a quick look at Jira, what the different offerings are. Um, and we're just getting the first steps of the project set up. So Lamar doesn't type anything ever when he writes his code and I'm really surprised that when he was whispering while he was talking, it actually still worked. It's really interesting. I love what Brian's doing here. He just straight up told Claude, hey, we're building Jira, do it quickly. All right. Yes, so Lamar is often uses uh Chat GPT. He's currently using O3 Mini High for this PRD. Looks like Brian's got Claude 3.5. Low R is now putting into Claude. And why? Tell us tell us about some of their prompting. What are they saying? Uh right now, it's hard to tell what I'm doing, but I'm actually using

--- Segment 22 (Starting at 210:00) ---

[210:01] Speaker 2: um Claude by Anthropic um to build the app.
And I'm actually using another app called Replit to build the app and then I'm also using another app called Manis to build the app.
And I'm also using another app called ChatGBT to build the app.
And so now we're using all four of those.
So at the end I'm going to have four different apps and I'm going to simultaneously while I'm waiting for one prompt, be working on the next program.

So, this is going to be like kind of hard to follow.
That's why I'm talking instead of Gary.
But um there's all kinds of stuff going on right now and at the end we should have at least one app that works.

[210:57] Speaker 3: Uh Jared just got Claude code running in the terminal of cursor.
This is pretty helpful for being able to look at the code a little bit more closely and make more precise edits using the cursor app while simultaneously making some large uh swaths of editing with Claude code.
Um and it's rocking and rolling, it's going to be the best one.
Don't worry.

[211:24] Speaker 4: It looks like Brian's a little old fashioned.
He's using plain cursor, but it looks like he already has a Jira board up and running.
I see him adding a task to the board and now it looks like he's thanking Claude saying great job.

[211:42] Speaker 5: So Lamar, why do you like Replit?
It seems like it's making some good progress compared to the other ones.

[211:47] Speaker 2: Oh, I like Replit because it's like a good um tool to build fast and the agent is really good.
Replit has done a really good job uh becoming like one of the top uh tools for AI building.
Though it's like overlooked because it kind of started um not as good as some of the others like cursor, but now it's um uh one of the top ones in my opinion.
Uh if you don't have like a required text stack because it uses the same text stack every time.
Uh well, almost.

[212:22] Speaker 5: Yeah, it has a different default than some of the other platforms.

[212:25] Speaker 3: One of the most important steps when using AI to build is making sure from the get go you have a pretty good set of uh requirements for your app and going over that as well.
So right now Jared is taking a look at a bunch of different things as far as Jera is concerned, making sure we've got the right pages and the right way to prompt uh so that those pages get created correctly.

[212:51] Speaker 4: I would also like to point out that earlier Brian said that this was impossible and just 30 seconds ago he told me, oh wait, actually this is definitely possible.

[213:05] Speaker 6: So It's only been four minutes so far.
So Brian, let me know why you changed your mind.

[213:09] Speaker 7: I uh did not know what Jira was and I skimmed the Wikipedia page and it read a lot more complicated than how it actually is.

[213:19] Speaker 7: So.
I'll just make it look nice for the next 10 minutes.

[213:24] Speaker 6: All right.
Lamar, where are you at?

[213:27] Speaker 2: Uh one of the four programs has made like a really nice looking Jira app um with ticketing and a dashboard.
Um so it looks pretty good.
So we can actually spend a lot of the time if we want to quit the other ones and just focus on this one.
Um and then we can continue building it and uh make sure that it actually works.
And then we um should be good to go.
But um the progress on, oh, um I I thought people could see it, but yeah, so that's where we're at.

[213:56] Speaker 3: It's looking good, it's looking good.

[214:03] Speaker 4: Brian just told Claude to make the theme dark because that's how he is preferable.

[214:09] Speaker 3: All right, we're about to hit the 10 minute mark and it looks like we already have the UI running.
It looks like that we have Lamar making four applications, Jared's on a console log error, but that's about to go away.
And Brian's already looking at colors and theming for his application.

[214:53] Speaker 3: Lamar may have four apps building, but Jared has seven sub processes running hacking Lamar and he will be shutting down three of those apps at least within the next few minutes.

[215:12] Speaker 8: Oh Lamar, you have to click mirror your screen.
Mirror your screen.

[215:16] Speaker 2: Got it.
All right, cool.

[215:17] Speaker 3: Oh, dope.

[215:19] Speaker 2: So, so this is the first first one that we're building.
Uh it's kind of behind um and we'll see what happens with this one.
Continue.
Boom.
And then uh we'll go to Replit.
So this is the one that's going going the best so far.
So in speed builds, I like to try all kinds of different things.
Um so if you ever check out my Twitter, you'll see like all kinds of different stuff going on with the videos like this.
Um but uh this one's looking good.
So we're going to come back to this one and work on it.
Um and then we have another one going in Manis, which is uh China's version of Open AI's operator.
Um and this one's uh going pretty good.
And um uh this one actually I have the most uh confidence in this one.
So right now we're just seeing the code, but um eventually we'll see uh some cool UI.
Um and then ChatGBT is still thinking or something.
And then um I got bored so I made another one.
So now this is the fifth one with a different program called V0.
And so um in between prompts, it's kind of seems like inefficient.
But in between prompts, I can just easily switch through because each prompt takes like uh one or two minutes to execute and go to the next part.
So instead of sitting here being bored, I just um uh move on to something else.

[216:48] Speaker 3: While Lamar has been bored, Jared has been working extremely hard and building an even better app that looks much, much better.
You won't believe it when you see it.

[216:58] Speaker 4: Stay focused.

[217:03] Speaker 3: Well, how's Brian doing over there?

[217:06] Speaker 4: It looks like he's doing pretty good.
I see Claude is thinking very hard.
Would you mind showing me the app so I can describe it?
Ooh, it looks sleek indeed.

[217:27] Speaker 4: Yep, he's got tickets.
He can move them between boards.
Looks like anybody who uses this app is going to be super productive.

[217:44] Speaker 3: Lamar is trying to figure out how to make ChatGBT show like an artifact.
You good?

[218:11] Speaker 3: All right, we have Jared screen now.
We'll have Jefferson walk through what he's working on and how far he is.

[218:18] Speaker 9: So, the best practice here is to use bun in opposition of NPM.
Uh so we we've got that going.
The app is about to be running in just a little bit here.
Um and we're just figuring out a few things here, but then you'll be seeing the app.

[218:50] Speaker 3: You're pretty impressed with Manis?

[218:52] Speaker 2: Yeah, it's like doing um a lot of end points right now and doing like different things that we probably don't need.
But it's actually going to be pretty cool if we can get it going, get it done in uh five minutes.

[219:06] Speaker 3: All right, let's do a time check.
Where are we at at the timer right now?

[219:09] Speaker 9: 5 minutes and 35.

[219:11] Speaker 3: We're about to hit the 5 minute mark.
And in 5 minutes, what what's going to happen is we're going to have each one of them connect and showcase their applications.
And then we're going to have the audience decide who's the best builder of Jira in 15 minutes?

[219:32] Speaker 5: So often times when you prompt Replit etc, they'll have mock data and not working API endpoints.
So one thing that Lamar is trying to figure out is how do we quickly get a real API endpoint so we can file a real ticket.

[219:55] Speaker 2: No, no.

[220:09] Speaker 9: So we've been getting some wacky errors coming from uh the terminal here.
So we're uh looking at re opening another directory and moving some files over, creating them there.
Uh but we'll have that up up and running in a little bit.

[220:34] Speaker 5: So the cool thing about Manus, which is like Lamar said, Chinese version of Open AI operator.
Um it's basically just been doing its own thing with almost no no intervention.
Or did you intervene at all since you prompted it?

[220:50] Speaker 2: Uh no, not yet.

[220:52] Speaker 5: So Manus has been just going for 10 minutes straight without any 11 minutes straight without any intervention.

[221:05] Speaker 4: So it looks like Brian's just trying to

--- Segment 23 (Starting at 220:00) ---

[220:00] Speaker 2: make it extra useful. He's working on ticket customization for figuring out how to basically add useful fields to your tickets.
Right now he's implementing giving it a title and he's already got priority and due dates and descriptions up and running.

Looks like he's alternating between doing that and working on more of the theme.
[220:00 - 221:45] (Silence)
[221:45] Speaker 2: All right, let's do a time check. How far are we, how much time do we have left?
All right, we're three minutes left. Let's give it up for all of our contestants. It takes guts to get up here and just start coding in front of an audience. Let's give it up for all of them.

And in just a second, we'll have Brian showcase his application. We'll have Jared showcase his application and then we'll have Lamar showcase whichever number of applications he'd like to share.
All right, whenever you're ready.

Still have two minutes, huh? You're already ready to go?
Claude's thinking about it. Okay.

How's Jared doing? Did you resolve that error, Jefferson?
There is a very strange error popping up persistently that is very mysterious. Very mysterious error.
I want everybody to know that when we did this at South by Southwest, we did have a champion and that was Jared Lambert. He beat out all the other contestants live here at South by, so he's defending his title for the speed build competition at Gauntlet.
How's Lamar doing, Gary?
No comment.
Looks like there must be an error there.
No, it's beautiful code.
All right, minute 30 left.
Again, it's very important to understand that each Gauntlet challenger has their own AI programming methodology. They pick and choose the tooling that works for them. They decide if they're going to use voice, they're going to use text, they're going to use prompting, which models they're using, and over the 12 week program, they use that methodology to build these applications and ensure that they can build it quickly, efficiently, and properly.
My computer just died so.
Okay, yeah, you can take the podium. Looks like Brian's computer is dying so we have to do a last minute uh uh move for the charging cable.
All right, 40 seconds left, and Brian will be the first one to present just in case his computer dies.
I would like to clarify that my computer has already died, and I have 33 seconds to see if it uh can come back on and hopefully it's saved.
It's interesting. With Lamar, Mannis has still not produced a single viewable output. It's it's just working, working, working, but may not be the right solution for a 15 minute window. And then Replit still working on on a database.
So we'll see if there's time. 10 seconds left.
Eight, seven, six, five, four, three, two, one! Let's give it up for the challengers of our speed build competition.
All right, so Brian's computer is rebooting, so we're going to start with Jared over here.
All right, we're going to start with Lamar.
Oh, we can't, somebody has to go, all right, here.
Yes, you could. Oh, wait, I got it, I got it. Oh, oh here you go. There you go.
All right, Brian's computer is up and running, we're going to have him walking through the features that he's completed and then we're going to go and see if he built the best version of Jira in this group of contestants. All right, Brian, over to you.

Lamar's like skip all the errors, please just show me something. Please!

All right, Brian, go for it.
All right. Uh so here's my Jira clone, Jira 2, the Jiraing. Um and as you can see we have one very important task, uh robot uprising. Let's put the due date for uh when should that one be due?
2027. All right.
11 25 2027.
And uh love Plyny.
And we can add tickets.
Any any tasks anybody needs done? That they'd like to recall.
Rally the troops.
All right. We'll rally the troops. Sounds good.

Yeah, so we've got uh descriptions, due dates, priorities. I can drag these between uh in progress and done. I think that one's still to do, not quite in progress.
And that's pretty much all I have for now, but 15 minutes in.
Good job, Brian. Let's give it up.
All right, next up we have Lamar.

All right, here we are. So, I did a lot of stuff and we'll see if any of it worked out. Um, this one didn't.
This one's still cooking. So this one's like the best option. Um, Replit, um but right now it's working on auths and a lot of stuff. So I actually would have skipped auth if I was like actually thinking about the time limit because auth takes more than like 15 minutes usually for these automated systems to set up because they try to make it more secure, especially right now since um, some of the CEOs of this uh this these um companies such as the Replit CEO and Vercel are arguing about security, so it's taking a little bit more to uh do security and make sure the apps are secure so they can back up their words. So um, that's where we're at on that one. And then Mannis is still going. So this one, this app is actually really good for like just putting in a prompt and just leaving it, and then coming back like an hour later and then it's a a fully built app. So, on my Twitter there's a a video like this where I actually ran Mannis for an hour and a half and um, I shortened that down and sped it up really fast, but it went for an hour and a half with one prompt and built an app. Um, so it can do, right now you can see there's a checklist here, and it's going through like this whole checklist. So these are all the things that it has left to finish the app. And so um, if we didn't want to build actual Jira and we just said make a simple ticketing system, then um, we could just shorten that down to like five steps and it'll do it. But, if we just leave this going, it's going to build the app. And then um, this one's is empty, so <laughing>. And then we tried uh with uh V Zero. And if I create account and sign up for it, um, this thing will go away and everything will work perfectly and it'll be the best app, but we're not going to demo that right now.
Let's give it up for Lamar, everybody.

So, I was running into some issues today. Uh so Tailwind, in case we've got any web devs in the house, Tailwind recently updated to a new version. And that's actually been really difficult with uh with the vibe coding because uh Claude loves Tailwind version 3. Um so I ran into some build errors with my first project where it set up a ton of styling with uh with Tailwind version 3.
And it just uh kind of dug itself into a whole but one of the benefits of when you're writing uh code this fast is you're not really tied to one project. You can rewrite things so quickly. And so like 7 minutes in I I threw in the towel and I started a new project. And this is what I got. Um, I used Cloud Code for this. Uh Cloud Code is pretty good at initializing like empty databases or em- empty uh empty repos and um, just getting a like a rough, a rough draft up and running. So you can see I I literally made every single single feature.
I'm not going to click on any of these because We don't have time.
But yeah, pretty impressive.
Um, any investors in the building, just uh just give me a call if if you're interested in Jira clone. Thank you. Good job, Jared. Let's give it up for Jared, everybody.

--- Segment 24 (Starting at 230:00) ---

[230:16] Speaker 2: All right, so here comes the moment of truth.
We're going to go one by one and we're going to see what the audience thinks and who the winner was.
To start out, let's make some noise for Brian and Joshua.
How about Lamar and Gary?
What about Jefferson and Jared.

Looks like the reigning champion keeps his title!
Jared Lambert, Gauntlet AI Speed Build.
Let's give it up for all the contestants, ladies and gentlemen.

All right, we have 8,000 people joining us live.
So it's time to call on Austin Allred to walk through some of the things that are going on here at Gauntlet AI and why this Speed Build challenge is so important.
All right, I think we we have an update to one of the apps that was built real quick.
Uh we're going
I think you can use any of the this one?
Sorry, we're making messes up here for all the AV people.
We apologize.

All right, we've got I don't know if there's anything Nope.
I think that worked.
Yep.
Okay, I'll let All right, so just an update from the first Speed Build.
Benji can join me to do most of the talking, if you want.
Um, so apparently all most of the errors we got were CSS errors, that was keeping us from running.
However, when I get to it, I'm still not actually able to play.
But just for kicks, while I was uh building all this in the background in my extra few minutes after the competition, I just thought, you know what, this app that we that my team built in the past few weeks, that we're going to show you later, um, I could actually maybe do this in there.
And so while I was building, or while I was fixing the errors on my speed build, I actually just put it in Dream Up, which is our like our AI game builder, and each one took I did five times, and each one took about a minute to build, and uh each created a working version of Cards Against Humanity.
So, go ahead, take it away.
I'll show it while you while you talk.
Speaker 3: Yeah, that's awesome.
Uh yeah, so it looks like uh Robert and his team built an app that can build this game better than Cursor, which I think is pretty awesome and they did it in what, two weeks?
That's right.
Speaker 3: Yeah, about two weeks.
Pretty incredible.
Speaker 2: Walk us through the functionality.
Speaker 3: Sure.
Speaker 2: Yeah, uh Of Dream Up or of the of the Cards games.
Speaker 3: Okay.
So, um as we'll demo more formally later, uh Dream Up is an AI game builder where anybody without any skill, or any code, whatever, you just come up in with an idea.
So I'll do this one again.
Build a Cards against Humanity clone.
Right?
And then, when it runs, you get a little a little menu of games you can play while you wait, and it takes about a minute, maybe like a minute and a half and depending on the game, and while we're doing that, I'll just show you the other versions of cards that we got.
Cards against Decency.
Okay, so this one put in Benji here and and another name.
And then, I don't even know what I Each one has its own unique instructions.
So I might have to read through the play instructions to actually get these working.
Let me find one that like is more intuitive.
Yeah, so see this uh this one has bot players that have been adding their cards.
I need to pick mine.
So it says, what is it good for?
Yikes.
I'll say growing apparent.
Okay.
All right.
So anyway, that's cards against friends.
There's quips against humanity, slightly different take on it and different UI.
Okay.
Right?
And then, let's see is this one Let's let that cook for a minute.
But anyway, and I haven't even iterated on any of these.
You can also put in improvements and like iteratively build a game while you're, you know, waiting and you get to play your existing version while you're waiting.
Um And so I just did this five times.
Every single time it worked and built me something that I can play with.
Got that's a little risque.
Okay, so there you go.
That's the one we just built live.
Speaker 3: For the record, the audience wanted to make Cards Against Humanity, so.
Speaker 2: Yes, exactly, yeah.
Speaker 3: We need to we need to sanitize this a little bit content-wise.
Speaker 2: Awesome.
Speaker 3: But anyway, yeah, so that's the update on the speed build and uh what you can do with Dream Up.
Speaker 2: All right.
Good job.
Awesome.
Uh thank you guys and thank you to all of the builders who've been uh thrust on stage to build stuff really quickly for us.
Um lunch is just arriving, so we'll be setting that up momentarily.
Um, before I get here, I wanted to point out a couple of things.
First, obviously, speed building is fun games.
It's all super, super fast.
Um, a lot of the important stuff of engineering comes when you have more complex applications, uh, bigger code bases.
Uh, you know, code is not necessarily going to all be one-shot anytime soon.
There's a whole lot of work that goes into building real, stable, deployable applications.
But, I think, uh, you know, that's that's an example to show you if you learn how to wield AI, you know, this is probably intermediate level, not quite advanced level.
Um, it gets very, very powerful.
Um, another thing that is pretty interesting to me, if you talk to the challengers about AI in its current state versus when they joined almost three months ago, each of them will tell you that it is just wildly better today than it was three months ago.
Three months ago, a lot of what they just showed you wasn't possible.
Some of those products didn't exist, many of those models didn't exist.
So a lot of what we try to do at Gauntlet AI is, yes, build, uh, yes, understand enough so that you can layer in AI where it makes sense.
But again, I I can't emphasize this enough, a lot is just continuing to stay on the edge of uh what is possible and that takes a significant amount of dedication and learning.
Um, somebody was saying a new Gemini model came out since we've been talking and I know people downstairs are already trying to build with it.
Um, so that's kind of, that gives you a sense of what we're we're trying to just keep riding that wave at Gantlet AI.
Um, we're going to take a break uh for a few reasons, both for lunch for the people who are here and because X has streaming limits that we're about to run into.
Um, so we'll actually stop this stream and then in about uh two hours, we'll start another stream um for the last little bit of the demos.
Um, for those of you that are here for that part, um uh 2:00.
2:00 is is the the time.
Um then we'll have dinner and drinks and all sorts of time to hang out, meet challengers, talk about whatever is of interest to you all.
Um, and yeah, if you're uh coming remotely, you can check out demodate.gauntletai.com and that will show you all of the projects, all the challengers, some of their history.
Um you can leave feedback for them, you can leave feedback for us.
Um, and then if you're interested in a future gauntlet cohort, uh either as a challenger challenger or a prospective hiring company, uh that is at gauntletai.com.
You can apply for either of those.
Um, so thank you so much and we'll uh lunch is still just getting here, but we'll uh we'll take a break and keep going.
Thank you.

## Part 1 




--- Segment 1 (Starting at 00:00) ---

[0:00 - 0:13] (Silence)
[0:13] Speaker 1: I
[0:13 - 0:18] (Silence)
[0:18] Speaker 1: Uh
[0:18 - 0:28] (Silence)
[0:28] Speaker 1: The
[0:28 - 0:40] (Silence)
[0:40] Speaker 1: Mhm.
[0:40 - 0:47] (Silence)
[0:47] Speaker 1: Mhm.
[0:47 - 0:50] (Silence)
[0:50] Speaker 2: Okay.
[0:50 - 0:58] (Silence)
[0:58] Speaker 1: Uh
[0:58 - 1:06] (Silence)
[1:06] Speaker 1: Yeah.
[1:06 - 1:13] (Silence)
[1:13] Speaker 2: I
[1:13] Speaker 1: Mhm.
[1:13 - 1:16] (Silence)
[1:16] Speaker 2: Okay.
[1:16 - 1:18] (Silence)
[1:18] Speaker 1: All
[1:18 - 1:46] (Silence)
[1:46] Speaker 1: Yeah.
[1:46 - 1:54] (Silence)
[1:54] Speaker 1: Yeah.
[1:54 - 2:01] (Silence)
[2:01] Speaker 2: Okay.
[2:01 - 2:09] (Silence)
[2:09] Speaker 1: Mhm.
[2:09 - 2:20] (Silence)
[2:20] Speaker 1: Yeah.
[2:20 - 2:30] (Silence)
[2:30] Speaker 1: Uh
[2:30 - 2:42] (Silence)
[2:42] Speaker 1: Yeah.
[2:42 - 2:49] (Silence)
[2:49] Speaker 1: Mhm.
[2:49 - 2:54] (Silence)
[2:54] Speaker 2: Okay.
[2:54 - 3:03] (Silence)
[3:03] Speaker 2: Okay.
[3:03 - 3:10] (Silence)
[3:10] Speaker 2: So
[3:10 - 3:17] (Silence)
[3:17] Speaker 2: Okay.
[3:17 - 3:20] (Silence)
[3:20] Speaker 1: Yeah.
[3:20 - 3:28] (Silence)
[3:28] Speaker 1: Uh
[3:28 - 3:37] (Silence)
[3:37] Speaker 1: Mhm.
[3:37 - 3:41] (Silence)
[3:41] Speaker 1: Yeah.
[3:41 - 3:53] (Silence)
[3:53] Speaker 2: Okay.
[3:53 - 3:57] (Silence)
[3:57] Speaker 1: Uh
[3:57 - 4:04] (Silence)
[4:04] Speaker 2: Okay.
[4:04 - 4:26] (Silence)
[4:26] Speaker 1: Yeah.
[4:26 - 4:31] (Silence)
[4:31] Speaker 2: Okay.
[4:31 - 4:35] (Silence)
[4:35] Speaker 1: Yeah.
[4:35 - 4:42] (Silence)
[4:42] Speaker 1: Oh.
[4:42 - 4:44] (Silence)
[4:44] Speaker 2: Okay.
[4:44 - 4:53] (Silence)
[4:53] Speaker 1: Yeah.
[4:53 - 4:59] (Silence)
[4:59] Speaker 2: Okay.
[4:59 - 5:08] (Silence)
[5:08] Speaker 1: Uh
[5:08 - 5:16] (Silence)
[5:16] Speaker 2: Okay.
[5:16 - 5:25] (Silence)
[5:25] Speaker 1: Mhmm.
[5:25 - 5:31] (Silence)
[5:31] Speaker 2: Okay.
[5:31 - 5:38] (Silence)
[5:38] Speaker 2: Yep.
[5:38 - 5:43] (Silence)
[5:43] Speaker 1: Yeah.
[5:43 - 5:47] (Silence)
[5:47] Speaker 2: Okay.
[5:47 - 5:55] (Silence)
[5:55] Speaker 1: Yeah.
[5:55 - 6:03] (Silence)
[6:03] Speaker 2: Okay.
[6:03 - 6:13] (Silence)
[6:13] Speaker 1: Mhmm.
[6:13 - 6:23] (Silence)
[6:23] Speaker 1: Yeah.
[6:23 - 6:30] (Silence)
[6:30] Speaker 2: Okay.
[6:30 - 6:39] (Silence)
[6:39] Speaker 1: Yeah.
[6:39 - 6:45] (Silence)
[6:45] Speaker 2: I
[6:45 - 6:48] (Silence)
[6:48] Speaker 2: Okay.
[6:48 - 6:54] (Silence)
[6:54] Speaker 1: Mhm.
[6:54 - 7:01] (Silence)
[7:01] Speaker 2: Okay.
[7:01 - 7:04] (Silence)
[7:04] Speaker 1: Yeah.
[7:04 - 7:11] (Silence)
[7:11] Speaker 2: I
[7:11 - 7:17] (Silence)
[7:17] Speaker 2: Okay.
[7:17 - 7:23] (Silence)
[7:23] Speaker 1: Mhmm.
[7:23 - 7:29] (Silence)
[7:29] Speaker 1: Yeah.
[7:29 - 7:35] (Silence)
[7:35] Speaker 2: I
[7:35 - 7:40] (Silence)
[7:40] Speaker 2: Okay.
[7:40 - 7:50] (Silence)
[7:50] Speaker 1: Mhm.
[7:50 - 7:56] (Silence)
[7:56] Speaker 1: Yeah.
[7:56 - 8:03] (Silence)
[8:03] Speaker 2: Okay.
[8:03 - 8:07] (Silence)
[8:07] Speaker 1: Uh
[8:07 - 8:15] (Silence)
[8:15] Speaker 2: Okay.
[8:15 - 8:23] (Silence)
[8:23] Speaker 1: Mhm.
[8:23 - 8:28] (Silence)
[8:28] Speaker 1: Mhm.
[8:28 - 8:38] (Silence)
[8:38] Speaker 2: Okay.
[8:38 - 8:47] (Silence)
[8:47] Speaker 1: Yeah.
[8:47 - 8:56] (Silence)
[8:56] Speaker 1: Oh.
[8:56 - 9:03] (Silence)
[9:03] Speaker 2: Okay.
[9:03 - 9:10] (Silence)
[9:10] Speaker 1: Yeah.
[9:10 - 9:25] (Silence)
[9:25] Speaker 1: Uh
[9:25 - 9:33] (Silence)
[9:33] Speaker 2: Okay.
[9:33 - 9:40] (Silence)
[9:40] Speaker 1: Mhmm.
[9:40 - 9:48] (Silence)
[9:48] Speaker 2: Okay.
[9:48 - 9:57] (Silence)
[9:57] Speaker 1: Yeah.
[9:57 - 10:04] (Silence)
[10:04] Speaker 2: Okay.
[10:04 - 10:08] (Silence)
[10:08] Speaker 1: Yeah.
[10:08 - 10:14] (Silence)
[10:14] Speaker 2: Okay.
[10:14 - 10:22] (Silence)
[10:22] Speaker 1: Uh
[10:22 - 10:30] (Silence)
[10:30] Speaker 2: Okay.
[10:30 - 10:36] (Silence)
[10:36] Speaker 1: Mhmm.
[10:36 - 10:40] (Silence)
[10:40] Speaker 1: Yeah.
[10:40 - 10:47] (Silence)
[10:47] Speaker 2: Okay.
[10:47 - 10:53] (Silence)
[10:53] Speaker 1: Mhm.
[10:53 - 11:00] (Silence)
[11:00] Speaker 2: Okay.
[11:00 - 11:07] (Silence)
[11:07] Speaker 1: Yeah.
[11:07 - 11:15] (Silence)
[11:15] Speaker 2: Okay.
[11:15 - 11:24] (Silence)
[11:24] Speaker 1: Yeah.
[11:24 - 11:32] (Silence)
[11:32] Speaker 2: Okay.
[11:32 - 11:38] (Silence)
[11:38] Speaker 1: Mhmm.
[11:38 - 11:46] (Silence)
[11:46] Speaker 2: Okay.
[11:46 - 11:55] (Silence)
[11:55] Speaker 1: Yeah.
[11:55 - 12:03] (Silence)
[12:03] Speaker 2: Okay.
[12:03 - 12:06] (Silence)
[12:06] Speaker 1: Yeah.
[12:06 - 12:14] (Silence)
[12:14] Speaker 2: I
[12:14 - 12:20] (Silence)
[12:20] Speaker 2: Okay.
[12:20 - 12:27] (Silence)
[12:27] Speaker 1: Mhm.
[12:27 - 12:32] (Silence)
[12:32] Speaker 1: Yeah.
[12:32 - 12:39] (Silence)
[12:39] Speaker 2: Okay.
[12:39 - 12:44] (Silence)
[12:44] Speaker 1: Uh
[12:44 - 12:53] (Silence)
[12:53] Speaker 2: Okay.
[12:53 - 13:00] (Silence)
[13:00] Speaker 1: Mhm.
[13:00 - 13:06] (Silence)
[13:06] Speaker 1: Yeah.
[13:06 - 13:13] (Silence)
[13:13] Speaker 2: I
[13:13 - 13:18] (Silence)
[13:18] Speaker 2: Okay.
[13:18 - 13:25] (Silence)
[13:25] Speaker 1: Mhm.
[13:25 - 13:31] (Silence)
[13:31] Speaker 2: I
[13:31 - 13:35] (Silence)
[13:35] Speaker 2: Okay.
[13:35 - 13:42] (Silence)
[13:42] Speaker 1: Yeah.
[13:42 - 13:49] (Silence)
[13:49] Speaker 1: Oh.
[13:49 - 13:56] (Silence)
[13:56] Speaker 2: Okay.
[13:56 - 14:02] (Silence)
[14:02] Speaker 1: Yeah.
[14:02 - 14:09] (Silence)
[14:09] Speaker 1: Uh
[14:09 - 14:16] (Silence)
[14:16] Speaker 2: I
[14:16 - 14:21] (Silence)
[14:21] Speaker 2: Okay.
[14:21 - 14:30] (Silence)
[14:30] Speaker 1: Mhm.
[14:30 - 14:35] (Silence)
[14:35] Speaker 2: Okay.
[14:35 - 14:43] (Silence)
[14:43] Speaker 2: I
[14:43 - 14:47] (Silence)
[14:47] Speaker 2: Okay.
[14:47 - 14:53] (Silence)
[14:53] Speaker 1: Mhm.
[14:53 - 14:58] (Silence)
[14:58] Speaker 2: Yep.
[14:58 - 15:01] (Silence)
[15:01] Speaker 1: Mhm.
[15:01 - 15:07] (Silence)
[15:07] Speaker 2: Okay.
[15:07 - 15:12] (Silence)
[15:12] Speaker 1: Yeah.
[15:12 - 15:20] (Silence)
[15:20] Speaker 2: Okay.
[15:20 - 15:28] (Silence)
[15:28] Speaker 1: Uh
[15:28 - 15:35] (Silence)
[15:35] Speaker 2: Okay.
[15:35 - 15:42] (Silence)
[15:42] Speaker 1: Yeah.
[15:42 - 15:47] (Silence)
[15:47] Speaker 2: I
[15:47 - 15:51] (Silence)
[15:51] Speaker 2: Okay.
[15:51 - 15:58] (Silence)
[15:58] Speaker 1: Mhm.
[15:58 - 16:03] (Silence)
[16:03] Speaker 1: Mhm.
[16:03 - 16:09] (Silence)
[16:09] Speaker 2: I
[16:09 - 16:14] (Silence)
[16:14] Speaker 2: Okay.
[16:14 - 16:22] (Silence)
[16:22] Speaker 1: Yeah.
[16:22 - 16:28] (Silence)
[16:28] Speaker 2: I
[16:28 - 16:31] (Silence)
[16:31] Speaker 2: Okay.
[16:31 - 16:38] (Silence)
[16:38] Speaker 1: Yeah.
[16:38 - 16:47] (Silence)
[16:47] Speaker 2: Okay.
[16:47 - 16:54] (Silence)
[16:54] Speaker 1: Yeah.
[16:54 - 17:01] (Silence)
[17:01] Speaker 2: Okay.
[17:01 - 17:05] (Silence)
[17:05] Speaker 1: Mhm.
[17:05 - 17:12] (Silence)
[17:12] Speaker 2: Okay.
[17:12 - 17:20] (Silence)
[17:20] Speaker 1: Mhm.
[17:20 - 17:26] (Silence)
[17:26] Speaker 1: Yeah.
[17:26 - 17:33] (Silence)
[17:33] Speaker 2: Okay.
[17:33 - 17:37] (Silence)
[17:37] Speaker 1: Yeah.
[17:37 - 17:45] (Silence)
[17:45] Speaker 2: Okay.
[17:45 - 17:52] (Silence)
[17:52] Speaker 1: Uh
[17:52 - 17:59] (Silence)
[17:59] Speaker 2: I
[17:59 - 18:03] (Silence)
[18:03] Speaker 2: Okay.
[18:03 - 18:11] (Silence)
[18:11] Speaker 1: Mhmm.
[18:11 - 18:16] (Silence)
[18:16] Speaker 2: Okay.
[18:16 - 18:22] (Silence)
[18:22] Speaker 1: Yeah.
[18:22 - 18:30] (Silence)
[18:30] Speaker 2: I
[18:30 - 18:33] (Silence)
[18:33] Speaker 2: Okay.
[18:33 - 18:39] (Silence)
[18:39] Speaker 1: Mhm.
[18:39 - 18:44] (Silence)
[18:44] Speaker 2: I
[18:44 - 18:47] (Silence)
[18:47] Speaker 2: Okay.
[18:47 - 18:55] (Silence)
[18:55] Speaker 1: Yeah.
[18:55 - 19:02] (Silence)
[19:02] Speaker 2: Okay.
[19:02 - 19:09] (Silence)
[19:09] Speaker 1: Mhmm.
[19:09 - 19:15] (Silence)
[19:15] Speaker 1: Uh
[19:15 - 19:22] (Silence)
[19:22] Speaker 2: I
[19:22 - 19:25] (Silence)
[19:25] Speaker 2: Okay.
[19:25 - 19:32] (Silence)
[19:32] Speaker 1: Mhm.
[19:32 - 19:37] (Silence)
[19:37] Speaker 1: Yeah.
[19:37 - 19:44] (Silence)
[19:44] Speaker 2: Okay.
[19:44 - 19:50] (Silence)
[19:50] Speaker 1: Uh
[19:50 - 19:58] (Silence)
[19:58] Speaker 2: I
[19:58 - 20:01] (Silence)
[20:01] Speaker 2: Okay.
[20:01 - 20:08] (Silence)
[20:08] Speaker 1: Mhm.
[20:08 - 20:14] (Silence)
[20:14] Speaker 1: Yeah.
[20:14 - 20:22] (Silence)
[20:22] Speaker 2: Okay.
[20:22 - 20:30] (Silence)
[20:30] Speaker 1: Uh
[20:30 - 20:38] (Silence)
[20:38] Speaker 2: I
[20:38 - 20:41] (Silence)
[20:41] Speaker 2: Okay.
[20:41 - 20:48] (Silence)
[20:48] Speaker 1: Mhm.
[20:48 - 20:54] (Silence)
[20:54] Speaker 2: Okay.
[20:54 - 21:02] (Silence)
[21:02] Speaker 1: Yeah.
[21:02 - 21:10] (Silence)
[21:10] Speaker 2: Okay.
[21:10 - 21:14] (Silence)
[21:14] Speaker 1: Mhmm.
[21:14 - 21:22] (Silence)
[21:22] Speaker 2: Okay.
[21:22 - 21:30] (Silence)
[21:30] Speaker 1: Yeah.
[21:30 - 21:37] (Silence)
[21:37] Speaker 2: Okay.
[21:37 - 21:41] (Silence)
[21:41] Speaker 1: Yeah.
[21:41 - 21:49] (Silence)
[21:49] Speaker 2: Okay.
[21:49 - 21:56] (Silence)
[21:56] Speaker 1: Uh
[21:56 - 22:03] (Silence)
[22:03] Speaker 2: I
[22:03 - 22:06] (Silence)
[22:06] Speaker 2: Okay.
[22:06 - 22:15] (Silence)
[22:15] Speaker 1: Mhm.
[22:15 - 22:21] (Silence)
[22:21] Speaker 2: Okay.
[22:21 - 22:26] (Silence)
[22:26] Speaker 1: Yeah.
[22:26 - 22:34] (Silence)
[22:34] Speaker 2: Okay.
[22:34 - 22:42] (Silence)
[22:42] Speaker 1: Mhm.
[22:42 - 22:48] (Silence)
[22:48] Speaker 1: Yeah.
[22:48 - 22:56] (Silence)
[22:56] Speaker 2: Okay.
[22:56 - 23:03] (Silence)
[23:03] Speaker 1: Uh
[23:03 - 23:10] (Silence)
[23:10] Speaker 2: Okay.
[23:10 - 23:18] (Silence)
[23:18] Speaker 1: Mhm.
[23:18 - 23:24] (Silence)
[23:24] Speaker 1: Yeah.
[23:24 - 23:30] (Silence)
[23:30] Speaker 2: Okay.
[23:30 - 23:37] (Silence)
[23:37] Speaker 1: Mhm.
[23:37 - 23:44] (Silence)
[23:44] Speaker 2: Okay.
[23:44 - 23:52] (Silence)
[23:52] Speaker 1: Yeah.
[23:52 - 23:58] (Silence)
[23:58] Speaker 2: Okay.
[23:58 - 24:05] (Silence)
[24:05] Speaker 1: Yeah.
[24:05 - 24:13] (Silence)
[24:13] Speaker 2: Okay.
[24:13 - 24:17] (Silence)
[24:17] Speaker 1: Uh
[24:17 - 24:26] (Silence)
[24:26] Speaker 2: I
[24:26 - 24:29] (Silence)
[24:29] Speaker 2: Okay.
[24:29 - 24:36] (Silence)
[24:36] Speaker 1: Mhm.
[24:36 - 24:43] (Silence)
[24:43] Speaker 2: Okay.
[24:43 - 24:49] (Silence)
[24:49] Speaker 1: Mhm.
[24:49 - 24:57] (Silence)
[24:57] Speaker 2: Okay.
[24:57 - 25:04] (Silence)
[25:04] Speaker 1: Yeah.
[25:04 - 25:12] (Silence)
[25:12] Speaker 2: Okay.
[25:12 - 25:19] (Silence)
[25:19] Speaker 1: Uh
[25:19 - 25:26] (Silence)
[25:26] Speaker 2: Okay.
[25:26 - 25:34] (Silence)
[25:34] Speaker 1: Mhm.
[25:34 - 25:39] (Silence)
[25:39] Speaker 2: Okay.
[25:39 - 25:47] (Silence)
[25:47] Speaker 1: Mhm.
[25:47 - 25:53] (Silence)
[25:53] Speaker 2: I
[25:53 - 25:57] (Silence)
[25:57] Speaker 2: Okay.
[25:57 - 26:04] (Silence)
[26:04] Speaker 1: Yeah.
[26:04 - 26:12] (Silence)
[26:12] Speaker 2: Okay.
[26:12 - 26:20] (Silence)
[26:20] Speaker 1: Mhm.
[26:20 - 26:26] (Silence)
[26:26] Speaker 1: Yeah.
[26:26 - 26:34] (Silence)
[26:34] Speaker 2: Okay.
[26:34 - 26:39] (Silence)
[26:39] Speaker 1: Uh
[26:39 - 26:47] (Silence)
[26:47] Speaker 2: I
[26:47 - 26:50] (Silence)
[26:50] Speaker 2: Okay.
[26:50 - 26:58] (Silence)
[26:58] Speaker 1: Mhm.
[26:58 - 27:03] (Silence)
[27:03] Speaker 2: Okay.
[27:03 - 27:10] (Silence)
[27:10] Speaker 1: Yeah.
[27:10 - 27:18] (Silence)
[27:18] Speaker 2: Okay.
[27:18 - 27:25] (Silence)
[27:25] Speaker 1: Mhm.
[27:25 - 27:33] (Silence)
[27:33] Speaker 2: Okay.
[27:33 - 27:39] (Silence)
[27:39] Speaker 1: Yeah.
[27:39 - 27:47] (Silence)
[27:47] Speaker 2: Okay.
[27:47 - 27:54] (Silence)
[27:54] Speaker 1: Mhm.
[27:54 - 28:02] (Silence)
[28:02] Speaker 2: Okay.
[28:02 - 28:08] (Silence)
[28:08] Speaker 1: Yeah.
[28:08 - 28:16] (Silence)
[28:16] Speaker 2: Okay.
[28:16 - 28:25] (Silence)
[28:25] Speaker 1: Uh
[28:25 - 28:32] (Silence)
[28:32] Speaker 2: I
[28:32 - 28:35] (Silence)
[28:35] Speaker 2: Okay.
[28:35 - 28:42] (Silence)
[28:42] Speaker 1: Mhm.
[28:42 - 28:49] (Silence)
[28:49] Speaker 2: Okay.
[28:49 - 28:57] (Silence)
[28:57] Speaker 1: Mhm.
[28:57 - 29:02] (Silence)
[29:02] Speaker 1: Yeah.
[29:02 - 29:09] (Silence)
[29:09] Speaker 2: Okay.
[29:09 - 29:16] (Silence)
[29:16] Speaker 1: Yeah.
[29:16 - 29:24] (Silence)
[29:24] Speaker 2: I
[29:24 - 29:27] (Silence)
[29:27] Speaker 2: Okay.
[29:27 - 29:34] (Silence)
[29:34] Speaker 1: Mhm.
[29:34 - 29:41] (Silence)
[29:41] Speaker 1: Yeah.
[29:41 - 29:48] (Silence)
[29:48] Speaker 2: I
[29:48 - 29:51] (Silence)
[29:51] Speaker 2: Okay.
[29:51 - 29:58] (Silence)
[29:58] Speaker 1: Mhm.
[29:58 - 30:05] (Silence)
[30:05] Speaker 2: Okay.
[30:05 -

--- Segment 2 (Starting at 10:00) ---

[10:00 - 10:05] (Silence)
[10:05] Speaker 1: Yeah.

[10:06 - 10:12] (Silence)
[10:12] Speaker 2: Um, Okay.
[10:13] Speaker 1: Yeah.
[10:13 - 10:17] (Silence)
[10:17] Speaker 1: Yeah, um...
[10:18] Speaker 2: Mhm.
[10:19] Speaker 1: Uh, yeah.
[10:19 - 10:28] (Silence)
[10:28] Speaker 2: Yeah.
[10:29 - 10:36] (Silence)
[10:36] Speaker 1: Uh-huh.
[10:36 - 10:41] (Silence)
[10:41] Speaker 2: Yeah.
[10:41 - 10:50] (Silence)
[10:50] Speaker 1: Yeah.
[10:51 - 11:00] (Silence)

--- Segment 3 (Starting at 20:00) ---

[20:00 - 20:14] (Silence)
[20:14] Speaker 1: All right. Uh, we are excited to kick off this second part of Gauntlet AI demo day. Thank you for joining us. Uh, it's been a wild day with some demos, some speed builds and uh it's good to uh have a break for lunch and let everybody meet each other. Uh, we're going to keep on rolling. Um so, next up, we have Abraham Julian.

--- Segment 4 (Starting at 30:00) ---

[30:00] Speaker 1: and Nathan uh with night vision.
So we'll turn it over to them.

[30:18 - 30:28] (Silence)

[30:28] Speaker 2: Um hello everyone.
My name is Ibrahim.
Uh this is Odio and this is Nathan.
And our app is called Night Vision.
Uh first of all, do we have any chess players in the house?
Just kind of raise your hand.
All right.
So, if you've ever played chess over the board and you want to analyze your game, um manually put in the pieces into like uh stockfish or whatever is a tedious process and um you know, it could be difficult to do.
And if you want to improve your chess game, it's kind of important to analyze your games, right?
So, we, that was the problem we identified over um the course of this program.
And our solution was to create a computer vision model that could look at a physical chess board and map that out to um map map that out to a fen notation.
And then we're able to analyze the game that way.
Okay, so um to turn a computer vision model, you do need high quality data set and um manually labeling the any a video or a picture of a chess set, it's also a tedious process as well.
So the way we generated our data set was that we um used blender to render a chess board um and chess pieces as well.
And with Blender's Python interface, we're able to get several positions and generate a high quality data set.
Um this is what our data set would look like. Uh we have the corners of the board.
We have the pieces and we have their occupancy as well.
Um I don't see anything displayed here.
Is that fine?
Okay, all right.
So I'm going to do a live demo where we're going to run inference on a chess board image.
So going to open this file here.
And we can see this is from Black's perspective.
So I'll run recognition on this.
Just give it a second.
And then here, uh you could see the bounding box for the board.
We can see I'll also zoom in.
You can see the pieces correctly identified.
And we can also compare it here on the digital chess board.
You can see the castle king, you can see the knight, you can see all the other pieces and you could do an analysis on this.
This is one part of Night Vision.
I'll hand it over to my colleagues to do the rest.

[33:53] Speaker 3: Hi, I'm El.
I want to describe our two main features of our app.
We offer a tutor and an analysis engine for analyzing your games, whether that be from a live game board or from one of your play your favorite platforms like chess.com.
Our tutor gives even moves that in real time match the skill of you as you play against it.
As you can see here and now you can using PGns, you can upload your games from any platform, chess.com or the live games.
We provide analysis here.
I'm going to hand it off to Nate, who's going to give you guys a nice live demo.

[34:52] Speaker 4: Yeah, all right.
So we'll go over to our app here and this is a game that's already been analyzed.
So we still have the annotations that you would expect uh for many other chess engine like a blunder, but we also have tactical annotations.
So we can see here we have a fork as the queen is attacking the pawn and the king and we also have pins as this bishop is poning is pinning the uh pawns to the queen there.
So basically with Night Vision, you can get your chess game whether it's over the board from any chess website or from our tutor and you can get in-depth analysis that's instructive and can be understood at any level of play.
Thank you.

[36:14 - 36:16] (Silence)

[36:16] Speaker 1: Awesome, thank you guys.
Next up, we have portals.
We got Adam Paul and Spencer Owens.

[37:02 - 37:06] (Silence)

[37:17] Speaker 5: All right.
How we all doing?

[37:24] Audience: Good.

[37:25] Speaker 5: Woo.
AI has kicked off a revolution in gaming.
Uh if your Twitter feed looks anything like our Twitter feed, probably looks a lot like this.
Um a lot of games.
This is not normal.
This wasn't normal uh even just a few months ago.
AI has empowered developers to make uh games that used to be reserved for like AAA Studios.
Um we can now do that with just JavaScript and AI.
Um Adam and I caught the bug uh over the course of Gauntlet.
We started developing some games as well.
Uh but we ran into an issue of distribution.
So we built some fun games, but how do we get this out to people?
How do we get people's eyes on this?
Um and so to solve for that, we built the platform that we think can build the future of gaming.
Bring the future of gaming together.
It's portals.

[38:28] Speaker 6: Yeah, so we uh we each built a game and it was a lot of fun.
Our games are fun to play.
But honestly, we were done building them in like less than a week.
And so we were sitting there thinking, what are we going to do with these games now that we've built them?
Uh maybe we'll combine them into one big game, maybe we'll, I don't know, put them on Twitter and they'll be missed and forgotten after a day.
And we realized we can use AI actually to create a hub where we can put our games on the web, everyone can find them.
And that's called Portals.
It's not just a hub for our games, it's a hub for anyone's games.
So creators can go here, game creators and they can upload their games and what we're really trying to do here is recreate the magic of like that early 2000s web game era.
Anyone who was around for that, it was a lot of fun.
Um we had, you know, addictinggames.com, newgrounds.com.
If you guys remember that.
And so that's what we're trying to recreate here.
And the idea is you go into into portals, game dashportals.com.
It's live right now.
Uh and it's it's to start with just uh a basic link aggregator.
You know, so maybe we go to all games.
We can see we've got plenty of games live on the site now.
People are organically adding their games every day.
Uh we've got over a dozen people so far uploaded their games, lots of Gauntleters.
But in addition to being just a standard link aggregator, the website itself is also a game.
So we have this 3D game world that is backing the whole website.
Look at all of these portals.
You can fly around in your rocket.
These are all user generated.
You can customize, add them yourself and each of these portals, if we were to say fly into one of them, what do we see?
Boom, now we're entering the world of Monster Hunter, which is a game created by ater here, one of us.
Super fun game, you can access it on portals.
So, this is kind of what we're thinking will be the future with the animation layer of the uh with 3js, which is the new animation layer for the web like flash used to be.
So I'll just quickly show you this will just run through some fast features on the website.
Uh this is how a creator would go and add a portal to their world.
Uh you know, put your URL, you can choose where in the game world you place it.
And you can customize the color, all these things and you just submit it.
And pretty quickly your portal is up and running.
You can go and fly into your game world from our website.
But we also think that not only can creators just add their games, they can actually do a lot more with something like this.

[41:03] Speaker 5: So we think Portal solves three big problems in uh in game development.
The first is distribution, as we've talked about.
Um if you're a game developer, you need to get your your game out there.
We already have users who are engaged and willing to play games.
Um and that's huge because unless you have a huge social media following, it's not going to happen for you.
Um number two is we think it can solve the issue of money and revenue for an indie game developer.
Currently, if you're, if you have a massive platform, you can vibe code a game, um and then put some ads on there, you get revenue.
Um that's great, but for the rest of us, um we've created an interoperable uh game asset layer.
So you can create things like skins.
Um you know, unlockable characters, um secret levels, things like that.
And instead of just, you know, your so the website, it is uh, you know, it's trusted.
It's through the platform.
And finally, we hope to help EdTech solve their motivation problem.
So, learning platforms exist.
Um we're going to create the reward layer and um this reward layer, you know, students can work in their learning app, gain a virtual currency and instead of having, you know, a pizza party or something that they don't care about.
What kids care about?
They care about games.
And so we provide the infrastructure for them to um actually invest in something that they care about.
Ontologically,

--- Segment 5 (Starting at 40:00) ---

[40:01] Speaker 1: we're pretty similar uh to uh Steam and um you know, similar Roblox.
But um yeah, this is a bet on the Open Web and a bet on AI.
[40:24] Speaker 2: Yeah, and so even right now it's live.
You can go to game-portals.com.
If you've got a game, upload it, you can play other games and we really think portals is going to be a lot of fun.
Thank you.

[40:50] Speaker 3: Awesome.
Thank you guys.
Um up next, we've got Aiden, Thomas, and Palmer um with an app that I will let them describe uh called Ivoni.
[41:45 - 41:55] (Silence)
[41:55] Speaker 4: All right.
I'm Palmer.
This is Sterling and this is Thomas and we're going to talk to you guys today about Ivoni, uh your new collaborative knowledge base.
So, when working with AI, we identified a problem very very early on.
Data is a mess, it takes a ton of time to like find all the context that you need for what you're building and it kills your productivity.
We have tons of sources that we gather information from every day, especially when building with AI.
You talk to Chat GPT, you talk in cursor, you talk to cloud directly, perplexity searches, right?
It all takes so much time and they're all in separate places and we want to consolidate it.
Furthermore, your LLMs don't know anything about you.
So we want to add all of this context to the LLM and give it information about what you're actually building.
You know, get something that's specific to your project.
So, our solution that we have for you is Ivoni.
It's your team's unified knowledge repository.
We want to bring all your data into a secure centralized hub where you have everything that you need that's pertinent to your project, your research, everything that you're building all in one place.
Um and again, this is because everything is so fragmented and scattered right now.
Um and this is a problem that we had very early on in every ideation session.
Everyone goes off, does their own research and then you got to gather it.

[43:11] Speaker 5: So uh, we have this uh demo here um, we have like you're researching a topic basically, and like you find a good paper, you don't really know what's going on here.
Um basically all you need to do is press command K and uh our Mac OS app realize shows up, it realizes like what's you don't need to do.
Takes the link automatically, sees it's a PDF document.
All you need to do is press press enter and uh it gets ingested into our uh your database.
And now what this allows it to to do is basically chat with the uh paper, ask anything about it.
And uh yeah, you can see on the right side here, uh all the sources it used to formulate that answer.
And uh this is like just a tiny part of our platform.

[44:23] Speaker 5: It's impossible to go out of the video and continue to the next slide.

[44:29] Speaker 5: There you go.
So uh, basically we spent go back.
[44:41] Speaker 5: How do I
[44:42 - 44:45] (Silence)
[44:45] Speaker 5: There we go.
We spent most of our time actually building like our ingest engine.
So you can basically integrate any any kind of app you have into it.
So basically you have the command K anywhere.
And for example, for YouTube, you're watching a good YouTube video, you want to save it.
All you need to do is press command K.
It realizes, oh, this is a YouTube video.
saves the link to your database.
Um embeds the whole transcript of the YouTube video, and you can then chat to it using our platform.
We also like allow you to organize your stuff into workspaces.
You can share your workspaces with like your friends, uh colleagues, so you can like they can use it however they want.
Um and you can work together on project as well.
[45:46] Speaker 6: So, there are a bunch of different solutions for this that we've already found, but what makes Ivoni unique?
Um, it's secure.
So, we have a platform that will allow you to take your data and keep your data because of the fact that we're using Tinyvec, which is a locally embedded uh vector database.
So it's always yours and you just have to download it.
Um offline flexibility because of this as well and it's scalable.
So, it's enterprise ready.
We have workspaces for people to collaborate and it's also swift and seamless, incredibly fast, again, thanks to Tinyvec and some of the LLM providers we're using.

[46:36] Speaker 4: So, you're probably wondering what's next for us.
Um, as mentioned, enabled by Tinyvec, we can expand both the depth and the breadth of the features that we have in Ivoni.
Um, things that we want to add immediately, we want to hook it up to an MCP server.
This will give your LLM or this will give any LLM or agent access to your knowledge base.
So you don't even have to go in there and do the manual work.
It'll do it for you.
Um, we want to do iOS and Android mobile applications, um, because then you can access and update your information on the go.
I don't know about you guys, like I think of stuff while I'm driving all the time that I would love to be able to just add like little facts and opinions to my knowledge base, um, especially about projects I'm working on.
And then lastly, there's a lot of potential here for secure enclaves.
Um, when you talk about like really, really sensitive data, being able to have it offline is incredibly important.
And so that's uh kind of another, you know, larger goal that we have for Ivoni down the road.
Um, but yeah, you can go to Ivoni XYZ Ivoni.XYZ right now if you want to try it out.
Thank you guys for your time.
[47:57] Speaker 3: Awesome.
Thank you guys.
All right, up next we have Alesha and John with The Last Ember.
[48:36] (Testing, testing)
[48:39] Speaker 7: Does this one work?

[48:48 - 48:53] (Silence)
[48:53] Speaker 7: Sure, let's stand over there.
[49:16 - 49:22] (Silence)
[49:22] Speaker 7: Sorry, just doing the sound.
All the way all the way.
All the way up.
Okay.
Hello everyone.
I'm John and this is Alesha and we're the team behind The Last Ember.
So, it's no secret that games are really expensive.
One of the most awaited games coming out, Grand Theft Auto 6 costs an estimated 2 billion dollars to make.
And another one, Elden Ring, Game of the Year, cost 200 million, which is considered cheap.
Uh, and we've seen how AI can meaningfully disrupt enterprise SAS.
That's kind of the definition of Gant AI where one or two person teams can actually compete with gigantic SAS companies.
And we've seen how AI can do that with games too on Twitter, but how about AAA games?
AI just can't really do realistic graphics, vast worlds, fast paced combat, or can it?
So for our capstone, we decided to see just how far we could push AI and if we could make a meaningful dent in the world of AAA gaming studios.
And we hope you enjoy.
[50:59] Narrator: Our world was broken by fire and scale.
We lost everything.
Even our hope from an act of defiance.
Our village burned.
All were erased.
He lost his home and his family.

--- Segment 6 (Starting at 50:00) ---

[50:00] Speaker 1: Years later,
the cycle returned.
But this time
no more cruelty,
no more destruction.
The cycle ends with him.

[50:36 - 50:41] (Silence)
[50:41] Speaker 2: We have adopted a lot of AI approaches during our game development from voices, videos, sounds, all aspects. For the trailers that you just saw, is we use Leonardo AI to generate images that fit with our game styles.
And then we feed those images to runway AI to give it camera control.
So, we can better prepare the player to embargan journey with us.
And if you see closely, you'll see someone familiar on the screen.
How we did it is we took a lot of photos of Austin's face and we used those images to mold the 3D model and then use the images to bake it on the model and put his head on the night.
So before, people have to go into professional studio to do a full body scan to have the 3D model. But now with the AI, we can just render this in less than two hours.
On top of that, we also use Blender MCP to help us troubleshooting because sometimes there are some problems that we just cannot solve by searching on internet, on YouTube, but the Blender MCP can help us look at the problems that we have, the settings and then help us troubleshoot.
And without further ado, let's see the game.
[53:16] Speaker 3: Yeah, there's not really much of a choice here.
We have to pick Austin.

[53:25] Speaker 3: Yeah, so welcome to the world that we've built.
Um just to tell you all, this is all built in pure JavaScript running on the browser.
This is not unity or unreal.
And the grass field that I'm running on, these are individually rendered.
Millions of grass blades.

And if I turn Austin around here and I zoom in.
And of course a lot of this code was written by AI as well.
So uh we've really tried to use AI to kind of tackle every aspect of game development here and I think to finish it off, I can show you some combat.
Do you guys want that?
So I think I have a pretty strong enemy.
might even be a boss.
Uh I should probably power up before I fight him.

That's modeled after Austin's voice by the way, using AI.

And yeah, this is complete pure JavaScript running on my MacBook Air.

[55:22] Speaker 4: Well done.
[55:24] Speaker 3: Yeah, so uh we think that in a few years, AAA Gaming Studios making millions uh could be run by one or two people and we hope after this presentation, you'll think the same.
Thank you.
[55:55] Speaker 4: Thank you.

[56:07] Speaker 5: All right, thank you.
I can't describe quite the feeling of what it is to see a demo and then see that they've rendered your face in 3D and your voice on on a video game character.
That's that's a first for me.
Um, all right, next we have Amir with Nor.

[57:26] Speaker 6: All right.
Hello everyone.
My name is Amir and I have Nor.
Picture this, you're back in college, sitting down in your lecture.
You're so frantically writing down every single word the professor says that you end up missing out on the entire lesson.
And this is just the reality.
70% of college students in US have reported that they are not able to engage in their lectures because they're so focused on taking down notes.
Traditional note taking causes college students to miss out on all the key insights throughout their classes.
This is because they're so focused on taking down notes that they end out end up missing out on the entire understanding.
And after class, they're juggling multiple tools, whether it's having their notes on Google Drive, having flash cards on Quizlet, or accessing random websites to take practice quizzes.
Everything is fragmented which leads to inefficient study sessions as well as wasted time.
This is where Nor steps in.
Nor is an AI driven study platform as well as a note taker that can transfer any type of content including PDFs, slides, recordings, or even YouTube links and generate detailed notes, flash cards, quizzes, and mind maps for those visual learners.
Nor is available on both iOS and web, which enhances a user experience allowing them to first start off by recording these lectures or processing them on their phone and then later up following up on their web to be able to review and study on a bigger screen.
By freeing students from manual note taking and allowing them to fully engage and absorb and absorbs all the information in their lectures, they will have a better understanding throughout their classes and have a fully set of study materials ready to go all centralized in one app.
Now this is a quick web demo of Nor.
Users can record their lectures, upload audio or documents or even process YouTube links.
They can go ahead and add a title, choose what class this lectures for and add tags to enhance searchability throughout those uh classes.
Then they can follow along with the processing to keep track of what's going on.
And then when it's ready, they have a personal dashboard created to be able to review their notes and then also their flash cards, mind maps, and their quizzes.
They also do have the feature to export these notes to PDF and generate a shareable link to class uh share with their classmates.
They also do have a calendar to be able to keep track of those lectures and know when they were recorded.

[1:01:15] Speaker 6: So far, we've gathered over 50 test users across three of the biggest college campuses in North Texas and here in Austin.
And we've overall received positive feedback as well as suggestions of new features to implement, one of those being the YouTube link processing.
We're planning to officially launch on April 1st and we're solely relying on user generated content to promote on our Tik Tok and Instagram as well as word of mouth and campus ambassadors to expand our user base.
If interested, you could visit us here at Nor.online or scan this QR code to download on test flight.
Thank you.

[1:02:18] Speaker 5: Awesome.
Thank you.
Next up, we have Amelia with Study B.

[1:03:05] Speaker 7: All right, hello everyone.
I'm Amelia and I'm very excited to share my project Study Be with you all.
So let's talk about a problem that a lot of students are facing right now.
It's traditional study methods.
A lot of students rely on these, but they can often lead to passive learning and

--- Segment 7 (Starting at 60:00) ---

[61:08] Speaker 1: limited, um, retention.
Students are just not aware of more creative approaches to studying.
I know when I was in high school, not even two years ago, I was unaware of um these more creative study methods.
So I felt trapped by these traditional methods such as flash cards or just rereading your notes.
Um, but more diverse and active study methods can be a lot more engaging and effective for students.

So the solution is Study Bee.
Study Bee provides students with a structured way to experience more creative methods to learn.
It doesn't only um provide these suggestions, but it will help the students um use them and walk them through these study methods.

So I've implemented six methods into my project Study Bee.
I'm not going to go over all of them right now, because I'll be showing some in the demo.

All right, I'm going to walk you through the user experience here.
So first you have your subjects.
So this student chose history and philosophy, you upload documents um that are relevant to these um subjects for you, and you can go to your calendar and create tests.
And with these tests, you can link your um documents that you uploaded and generate some suggestions for studying.
So um we have these for American Government, um study guide, and we can even create these um new events based on what the AI suggested, so just like that.
But the real material is going to be in the study materials page.

So we're going to see our active materials here, and it looks like we have one that is an SQR4 method.
Let's dive a little deeper into that.
So what is SQR4?
So the app is going to walk you through all of that.
So first you're going to survey your document.
So you're going to read the titles, headlines, look at the images, not go too deep into the information.
Um, and you're just going to take some brief notes and you are going to provide some questions that you have, because you haven't thoroughly read the document.
So you might have some questions on um, the information.
And then you're going to read the document super thoroughly, um and answer the questions that you ask yourself.
Take some extra notes.

And then you are going to recite, reflect, and review.
And then complete your study session, um and it'll be in your completed materials.
Now, let's create a new study material, and we're going to use the Feynman technique for this.
Um, this is going to be on an American Government study guide.
So this technique is really cool, because um the student kind of becomes the teacher in this, because we learn a lot better when we um can teach to other people.

And so the student becomes a teacher, and the AI becomes the student.
So the student's going to either record themselves doing a little lecture of the um information in the document they connected to this study method, or they're going to just type out some text.
So for example, there are three branches of the US government.
And they're going to submit their text or their audio recording, and the AI is going to ask them a question, such as what are the primary functions of each of the three branches of the US government?
And so the student's going to have a little conversation with the AI.
So they're going to provide an explanation, and then the AI is going to question them on that explanation, and it's going to be a good back and forth for the student to learn.
Another study method is um chunking your time.
So focusing 25 minutes on studying, and then five minutes, take a break.
25 minutes studying, um and repeat that process.
So we have a little timer up there to help the student um take care of that.
And these are just three of the six uh study methods I've implemented, but I'm working on more, and uh this is Study Bee.
Thank you.

[69:53 - 70:01] (Silence)
[70:01] Speaker 2: All right.
Up next, we have Arpan with Doctor Contract.

[70:08 - 70:16] (Silence)

[70:19 - 70:28] (Silence)

[70:31 - 70:53] (Silence)
[70:53] Speaker 3: Test, test.
All right.
So who's ready to talk about B2B SAS?
Let's go!
All right.
So this is Doctor Contracts.
Uh, I'm Arpan by the way.
Uh, this is Doctor Contract.
Doctor Contract is designed to save your organization hundreds of thousands if not millions of dollars.

So if you're a business owner, you probably have this issue.
You have tens or hundreds of contracts scattered across your organization, and it's a nightmare to keep track of renewal dates, of cancellation dates, of how much you're paying in total, of number of seats, all that stuff.
Doctor Contract handles all of this for you.

So the way we work is that we crawl through every single email inbox in your company in a privacy preserving way.
We analyze all of the contracts, all of the documents, we pull out key terms, important information, and then we upload this to our platform and present this to you in a beautiful dashboard.
So, let me show you how it works.

[72:09 - 72:12] (Silence)
[72:12] Speaker 3: So first we have our email crawler.
This runs on your desktop, so it is privacy preserving.
You drop in your Google Workspace credentials or Microsoft Outlook, whatever.
And then we will pull out every single email in your organization.
You can select which ones you want us to crawl.
If you don't want us to crawl an email, then you deselect it and we will not look at any of that, any of those emails.

And then we can go ahead and analyze them.
We will search by subject lines.
So you have complete control over what we look for, what subject lines we search for, what we're analyzing.
You can pull out subject lines if you don't like them or if you want to hide them from us.
All good.
And then we can start the crawl.
We go through every single email inbox, search for those subject lines, and pull out the contracts.

Or if you want, um we also have another option where you can drag and drop PDFs.
So if you have a bunch of PDFs, we'll analyze which ones are contracts and then we'll upload them to our platform.
I just dropped like 50 PDFs and we'll analyze them in under a minute.

And then we'll analyze them and store them in our contract.
All your contracts are encrypted, it's totally secure.
This is our dashboard.
You can see typical dashboard stuff.
And then you can go to our contracts page, where you will get a list of all of the contracts that we've analyzed and pulled out.
Um, you can see the vendor name, renewal dates.
You can choose what column headers you want.
So if you want to see the cancellation notice date, you can see that.
You can filter the contracts, maybe you only want to see contracts above $50,000 a year, you can do that.
Uh, you can sort them by whatever you want.
Uh, you can export this data as a CSV.

And then you can also click on an individual contract to get more information.

So we'll click on one.
And then you can see the details we pulled out, the contract summary, start date, yearly cost, billing cycle, whatever, number of seats you've purchased.
All of this we've pulled out from the contract with OCR, with LMs.
And then you can also see the PDF on the right-hand side, so you can get a sanity check, look deeper, whatever.
And then we also have a deep research feature.
So we will research all the terms in the contract, and then we'll tell you how is this vendor?
What's the ratings?
What are competitors?
Are you overpaying?
Are you underpaying?
And this is super useful information if you want to negotiate your contract.

And then we also pull out all the dates, all the important dates from all the contracts we analyze, and we plot them in a calendar, so you can keep track of your cancellation dates, of your renewal dates, things like that.
So you don't accidentally forget to cancel a contract and lose 50 grand.

And then we have a category breakdown.
This helps you visualize and understand your spend.
So we assign tags to all the contracts we analyze, consulting, security, infrastructure, DevOps, whatever.
And then you can see each contract and uh see how much you're spending on infra, or how much are you spending on DevOps and get a much much better understanding of how your company is spending money.
So we launched our beta last week and we are close to closing our first $20,000 in revenue.
Thank you.

[78:15 - 78:18] (Silence)
[78:18] Speaker 2: All right.
Thank you, Arpan.
Uh, now we've got Brett and Will with Echo AI.

--- Segment 8 (Starting at 70:00) ---

[70:00] Speaker 1: Hello?

[70:05] Speaker 1: All right, hi everyone.
My name is Brett and this is Will and we're not showing our screen yet, so let me try and get that going.
Let's give us a second here.

[70:48] Speaker 1: Suspense hangs in the air.

[70:53] Speaker 1: Oh, we're on.
Yeah.
Oh, we're good.
Okay.
Someone should have told me that.

[71:04] Speaker 1: All right, so I'm Brett and this is Will and this is Echo AI.
Um, so one of the things that I personally struggled with was where we had, um my wife and I with our newborn, we're watching the Australian Open and we had subtitles on to not wake him up and I just noticed how terrible the captioning was on that.
It was 15, 20 seconds late.
It was the players names were wrong.
It was just an utterly broken experience.
And that's something that we can fix through AI.
Um, so when you're watching watching live events, be it sports or news or the there's a long delay, the accuracy is typically like 13 percentage points below what it should be, uh, for domain specific terminology.
And uh the current ADA and FCC regulations are unmet and they're actually not enforced because the the current state of the art is so bad.

[72:28] Speaker 1: So our solution is to develop a AI first uh close captioning and transcription service that uh delivers transcriptions in 750 milliseconds.
That's 20 times faster than the the current state of the art in the industry.
And we have a native integration with all the professional audio and video workflows.
You know, some of the things that we're seeing here at the presentation and anything being done uh on your favorite cable news station.
Uh and we also are able to extend this technology through a plugin architecture where you can enable and disable and change the configuration on the fly of any of the plugins that are working through um through the system.

[73:28] Speaker 1: So in just two weeks I was able to have uh some very impressive conversations with some stakeholders at various companies such as the NBA, Twitch, Assist, and upcoming next week the um MLB Advanced Media.
Each of these conversations led to two additional leads and a referral onto uh investors into the company.

[73:59] Speaker 1: The total uh media and broadcasting and enterprise education, as you get into captioning and uh live production is about $80 billion.
Of that, the accessibility and close captioning is close to 30 billion and I think that we would be able to capture a third of the market within 18 months uh on being able to integrate all of these uh large contracts into us and and go down the long tail of all the second, third and and amateur type of sports and news productions.

[74:47] Speaker 2: Yeah, so on the technical side of things, just what makes us so much different than everyone else is it really does just come down to AI, both on the building our product.
You know, we're able to build it incredibly quickly due to using AI.
And also just we actually believe in the technology.
A lot of the competitors in the space, um, they're very set on keeping a human in the loop and that's where their 15 to 20 seconds comes from.
They don't see the present where it is working without a human in the loop and it's only going to get better from here.
Um, so that's our biggest technical differentiator.

[75:53] Speaker 2: Uh, but to introduce I'm Will, um, I've been a software developer for four or five years-ish, um, done some work at smaller companies, done some work at bigger companies and now excited to try this out.
Yeah, I think Will is definitely underselling himself.
He uh is quite an amazing partner to work with um and I'm proud to, you know, have him as a founding engineer on Echo AI.
Uh so I've worked uh I've got a few more years of experience of that and I've worked mostly for the last 12, 15 years in the uh VC gig economy space.
As you can see some of the things there.
Uh I also started a nonprofit for the Ukraine war and we raised $77 million in the first year.
Um, we have a few strate- one strategic advisor who uh is already on board and two more that are unannounced as of yet.

[76:59] Speaker 1: So the ask for us is uh to help us raise a seed round.
That would allow Will and I to work full-time on this and we believe we would have our first customers within weeks and months, uh not necessarily anything longer than that.
We are UE positive from the day one.
Our lean structure allows us to have uh superior gross margins where an average SAS company now maybe 75, 80%.
We think we're able to get closer to 90, 92% margins, uh which are just unreal.
Uh and that goes through the use of AI and our lower uh staffing and other costs.
Um, there are four different revenue streams that we're looking to capture.
Uh one obviously being the enterprise captioning and translation I've been talking about, but there are some future revenue streams that are already identified.
Uh license licencing fees from third party plugins that get added to the system, uh a subscription based analytics, both for existing customers and for external parties looking to understand what is being talked about and and what is the the sentiment around some of those conversations.
And then, um obviously the data brokering as you get into uh having petabytes of very interesting data, uh you definitely need to be able to monetize that.
So I would appreciate uh anyone checking out our website, uh echoai.studio and we have a QR code there and we look forward to uh helping you.
Thank you.
Thanks.

[78:46] Speaker 3: All right, thank you.
Uh next up, we have Kale with Primer.

[79:03 - 0:79:24] (Silence)

[79:24] Speaker 4: And yes.
Allow.
Let's go.
Wait.

[79:48 - 079:57] (Silence)
[79:57] Speaker 4: All right.
Uh, thank you for having me.
Um, so my name is Kale.
Um, and I am a big Neil Stevenson fan.
Um, I don't know uh how many people have read it, but he had a book in the 90s called the Diamond Age or the Young Ladies Illustrated Primer.
And it revolved around this girl from a very hard scrabble beginning who had come across this kind of AI powered book that would anticipate her needs and kind of teach her lessons through storytelling, through kind of videos and images and whatnot.
I thought that was a really cool idea uh and so I decided to build it.
Oh.
And then I decided to get rid of that.
All right, so how the primer works is there are adaptive stories.
The AI um will kind of anticipate what the child needs to learn at any given time.
Um, and it will teach them those lessons through um a blend of storytelling and images, videos, games.
Um, and the primer kind of grows up with the kid and um, yeah, it kind of it gets more complex, the questions, the lessons get harder as they go.
Um, and as they every there we go.
Sorry about that.
Um, so yeah, uh the primer will generate all of these kind of age appropriate stories and lessons and then it adapts and grows with them.
And this is why I'm a little bit flustered uh because I managed to break the app an hour ago uh by running get reset hard in the wrong terminal.
Um, so instead of throwing away the half hour of work that I had done trying to tweak the voice pipeline, I instead threw away three days worth of work on the app itself.
Um, so yeah, uh drop your commits early and often.
But anyways, unfortunately, uh I won't be able to demo it today, but um it's not going to take too much longer to fix.
Uh I just I I broke it an hour ago and and didn't have time to put it back together in time for the demo.
Anyways, um, so the features of it.
It has a very personalized um relationship with the user and it interacts with the user in a very um, just kind of natural and holistic way.
Um, it becomes their friend and it becomes their mentor.
Um, everything is generated in real time.
It's not pulling uh information from databases or what not.
It's it's kind of coming up with it on the fly as appropriate.
Uh, and yeah, it kind of seamlessly integrates text and images and video games.

--- Segment 9 (Starting at 80:00) ---

[80:01] Speaker 1: uh would be kind of the cool thing that I want to do next on it.
[80:10] Speaker 1: Um, and by doing this, so not only can you teach the children really well, but you make them love uh learning because of that connection that they have with the primer.
[80:31] Speaker 1: Uh it's a big part of the story and it was a big part of why I wanted to do it.
[80:42] Speaker 1: Um, yeah, so uh technical architecture. It uses PipeCat uh PipeCat Cloud uh for the voice pipeline. uh that does kind of
[81:00] Speaker 1: speech to text and text to speech and then there's an AI agent in the middle of it, um that can do all of the tool calls and whatnot that you want.
[81:12] Speaker 1: Uh, and it is quite cool.
[81:15] Speaker 1: So, my roadmap and next steps. Number one, fix it.
[81:20] Speaker 1: That would be a good idea.
[81:22] Speaker 1: Uh and then after that, um currently it just uses uh GPT's um history function to track the progress.
[81:35] Speaker 1: Uh I wanted to put in an actual vector database and then get it into the hands of kids.
[81:42] Speaker 1: Anyways, thank you very much.
[81:45] Speaker 1: That's all.
[81:47 - 82:34] (Silence)
[82:34] Speaker 2: Alright, thank you.
[82:37] Speaker 2: Uh next we have the feed up team consisting of David, Thomas, and Harmony.
[83:07 - 83:20] (Silence)
[83:20] Speaker 3: Okay.
[83:21] Speaker 4: All right.
[83:22] Speaker 5: Hello, I am Harmony Morris.
[83:26] Speaker 6: I'm Thomas Davidson.
[83:29] Speaker 7: I'm David Johnson and we've been working on Feed Up, which is an organizational feedback platform for big organizations.
[83:48] Speaker 7: So let us tell you about our problem in an anecdote.
[83:51] Speaker 7: Um, I worked at Target for about two years before I joined the Gauntlet just in a store and in all that time I was asked for feedback one time and I can pretty much guarantee you that nobody ever looked at it.
[84:08] Speaker 7: Um, there was definitely no action taken based on my feedback.
[84:14] Speaker 7: Uh, so what that means, um, given that this is not in any way a unique uh situation, viable insights just fall through the fingers of decision makers.
[84:26] Speaker 7: There's just too much data, there's not enough time to do anything with it.
[84:30] Speaker 7: And so no action gets taken.
[84:34] Speaker 7: And employees correctly identify that nobody's hearing them, and so they grow to resent um resent their employers.
[84:45] Speaker 7: And annual surveys just don't cut it anymore.
[84:48] Speaker 7: So our solution is to put gamified daily surveys that take 30 seconds or less into the hands of millions of employees across the country.
[85:00] Speaker 7: uh enabling their managers to produce viable action for them.
[85:04] Speaker 8: Um, so we tried to differentiate ourselves from previous solutions through two main ways, mostly by um consolidating the large amount of data into easily digestible, only the actual items uh request to send up to managers and uh we realized that mandates weren't quite enough to ensure compliance among these surveys.
[85:41] Speaker 8: Um, so the ease of use and gamification will be really number one in getting them there.
[85:48] Speaker 7: Absolutely.
[85:50] Speaker 7: And here is our uh just quick little demo of our mobile app.
[85:58] Speaker 7: Um, that would be for the respondents.
[86:01] Speaker 7: We have the reward system here.
[86:05] Speaker 7: Here you see a survey, nice and easy to take.
[86:10] Speaker 7: Animation looks good.
[86:13] Speaker 7: Um, and it's just a very simple process.
[86:19] Speaker 7: Um, you can view all of your old responses.
[86:24] Speaker 7: Um, and you can also um just yeah, you can view all of your old responses and then you see you have we have a streak function.
[86:38] Speaker 7: You can change your work days so that you're not um losing your streak just because uh you were off work that day.
[86:48] Speaker 7: And we also have just an always on feedback box type thing where you can tag uh create tags, choose to submit anonymously, just uh allowing more channel as many channels as we can for feedback.
[87:10] Speaker 7: And we also allow users to completely control the aesthetics of the app just because we think that'll help people use it more.
[87:18] Speaker 5: All right, here we have the manager's page.
[87:20] Speaker 5: We can see that there's a dashboard with a bunch of information taken from the respondent's survey responses.
[87:29] Speaker 5: There is a easy way to manage groups and users and we can easily create surveys to give out to your employees.
[87:40] Speaker 8: Cool.
[87:41] Speaker 8: And um how we built it, mostly it was to have the front ends interact with a uh single backend that is able to uh get all the data.
[87:57] Speaker 8: Um and also have a uh scheduling server so that uh we can provide the summaries uh very quickly to managers when they need them, but also um having reliable scheduling will allow for anonymized surveys to make sure that employees don't feel on the spot when they give answers.
[88:25] Speaker 5: All right, so how did we use uh AI in stage one to develop our planning docs and anthology.
[88:32] Speaker 5: We used Open AI's 01 and then stage two coding, we used cloud 3.7 and 3.5 and then from now and continuing, we'll use Open AI's 40 until there's a better newer model.
[88:54] Speaker 7: So what's next for us? uh we're hoping to find some pilot organizations and revolutionize their employee feedback collection and utilization pipeline and as well improve their employees's experience, retention and morale and we hope to expand to more and more types of organizations till we can get everybody's voice hurt.
[89:30] Speaker 7: Thank you all very much.
[89:31] Speaker 8: Thank you.
[89:33 - 90:39] (Silence)
[90:39] Speaker 2: All right, up next, we've got Drew with Sim Circuit.
[91:48] Speaker 9: Yeah, it's just not connecting.
[91:53] Speaker 9: Uh potentially.
[92:18] Speaker 9: Sorry for the technical delay.
[92:22] Speaker 9: Um my name is Drew Soterquist and I've built or I'm working on Sim Circuit, which is a educational mobile app um that is targeted to teach basic circuitry to K through eighth grade.
[92:50] Speaker 9: So what's the goal in Sim Circuit?
[92:54] Speaker 9: It is to bridge the gap in STEM education.
[92:59] Speaker 9: So I'm targeting K through 8 to learn the basics of circuitry.
[93:04] Speaker 9: I'm a bit biased, why is learning circuits important?
[93:08] Speaker 9: Well, as our world moves more and more into technology, um understanding the basic paths and flows and embedded technology is very important.
[93:22] Speaker 9: So my overall goal with Sim Circuit is to have 20 plus circuit components and three plus learning paths for the kids to grow and build their knowledge.
[93:37] Speaker 9: So the current challenge in circuit education is that there's no great one size fits all solution.
[93:46] Speaker 9: Nothing fully takes you from step zero to one on your journey.
[93:53] Speaker 9: There's a lot of abstract concepts in circuitry that I feel a lot of simulators or other apps do not touch on.
[94:02] Speaker 9: There's some age appropriate tools such as um online web app simulators that if you know what you're doing or you know the basics, you can go in, create your own components, your own circuits and see the flow.
[94:20] Speaker 9: There's limited hands-on learning, but there's a great kit.

--- Segment 10 (Starting at 90:00) ---

[90:00] Speaker 1: that I showed here of some snap on circuits. But the issue with these is accessibility. If you have this at a school, a couple kids in a classroom, these are very restrictive in the amount of components they have. If a kid loses a part, but how can this all be solved? My solution is SIM circuit.
So where I've built a custom drag and drop interface with intuitive design for wired connections, a component library and all of this with accessibility in mind.

So here's a quick demonstration of my app, starting first with the onboarding process of what you would see when you first download the application.

After signing in, we take you straight to the dashboard, which shows some gamification of a home screen with some basic levels going over um the intro or basics to circuitry. Right here, we clicked into level two, so where we can see the drag and drop design, adding in components such as a battery and an LED, connecting them with wires.

All of this with very intuitive design. And if you wish to delete a component, you can hold down, they shake, they get a little scared and you press the X to delete.

So as shown in the home screen there, my goal is to have three plus learning paths. The first three would be circuit basics, which I've completed so far. Next would be light and sound and energy and power.

So what's next for me? Um detailing a bit of my next development steps would be adding in real-time simulation, which I've started progress and working on, um which would show interactive voltage and current flow, and as well as multiple, or sorry, more educational content. So going down those other two learning paths that I've laid out. Thank you all for your time.

[93:24 - 93:27] (Silence)
[93:27] Speaker 2: All right, up next, we have Francisco with Autoplayer.

[94:03 - 94:49] (Silence - music plays)
[94:49] Speaker 3: Hi, uh my name is Francisco and uh I've made a TFT Autoplayer. I think after spending a lot of time uh building with AI, I would introduce my favorite game to it. So what exactly is TFT? It is uh basically uh eight player free for all game where everybody completes in a series of 1v1 rounds and it takes place on this grid-based board. Uh it's a little complicated with 63 units and 14 uh origin and 12 class synergies and there's also an economy component to it. Um but my real reason for making it is the grinding is incentivized in these rank games. There's a total of 31 different rank divisions and basically you start at the bottom and uh for me personally I belong like 19 divisions up, which takes about 20 hours of consistent perfect play, which is basically impossible.

So I wanted to create this thing to save me some time. Um so how does it work? Uh so I basically created an agent and uh gave it a bunch of tools. Uh the first one I fine tuned vision model, so it can recognize all of the characters. I uh gave it the game data so it could understand the basics of the game and I gave it tools to move uh buy units, sell units and place them on the board. And I shared the my strategies so it would play like I would. And so here's the first step where it scans the board, it takes a screenshot and it looks at the game state, it looks at the characters on the board and it decides the best moves to make. And then it translates it to a text text representation. uh just because it's not really a vision agent, it needs a text representation to understand what's happening. So I created this text format for it to understand and it uses that to uh make the moves from there. And then it makes the moves. Uh here's a little low-quality GIF, but it basically takes a screenshot, looks at what's going on and it buys from the shop and places them on the board as you would want them to. Um some optimizations that I made, uh since scanning takes about 2.3 seconds to um see the screenshot and analyze it, I did uh some manual maintaining and updating of the board state. Uh so that goes on behind the scenes and sometimes it'll chain moves without scanning again. And then from there waits for the next planning phase so it can uh continue playing the game. But uh yeah, that's the basics and uh so hopefully you can get where you belong without sinking too much time in. Thank you.

[99:26] Speaker 2: Good job.
[99:31] Speaker 2: All right. If you're just joining us for the Gauntlet AI Demo Day, please make sure that you go to demoday.gauntletai.com to see all of our challengers, all the projects that they have and to see what they're going to be presenting next. Next up, we have Gage McCoy with OTD. Your wardrobe just got smarter and he'll show you how.

[100:19 - 100:23] (Silence)
[100:23] Speaker 4: Uh I've spent more money purchasing clothes than I have on my college tuition. So I think that makes me the most qualified person in Gauntlet to say I'm the most fashionable person in Gauntlet. Um I'm Gage McCoy. I'm 19 years old and four weeks ago I met with a TikToker with 1.9 million followers. She pitched her idea for OTD or Outfit of the Day. And this is uh the app I'm working on for the past four weeks. I dropped everything to get to work. OTD is an all-in-one fashion app meant for building, sharing, finding and purchasing outfits powered by AI.

I only have a limited time with you today, so I'm just going to showcase the core features, starting off with the stylebot. Picture this. You're just asked a girl out and she said yes, crazy. You have to figure out what you want to wear for the date. You have three hours, so you snap a picture, say I'm going on a date in 3 hours. I need to know what to wear, just like in the picture. Um and your jaw drops as it immediately gets to work and our state-of-the-art uh stylebot will tell you what to wear. help you create a fresh, trendy outfit based off your style preferences and what's already in your wardrobe. This is personalization taken to the next level. It knows everything you wear and it uh will help you uh create the perfect outfit from that. The most the most interesting feature though for me to build was this uh clothing outfit analyst. So what it does is it will take any picture and it will scan it and find all clothing items inside of it and try to tag those. That way we can provide Amazon affiliate links for them. Initially, I tried using just one-shot classification and that did not work at all. My second attempt was to use Meta's SAM or Segment Anything model, and it was functional, but it was really, really slow and it was not sustainable. My third approach, I used uh MoveNet's pose detection to analyze the pose of the person. It finds key points like their shoulders, their waist, knees, and so on. and because of that, it's able to define what I call clothing regions. With the detected clothing regions, it's able to detect proper clothing items as you can see in the picture. This is fast. It's about 3 to 8 seconds per image and so it helps create a fast, smooth, and scalable experience for the user because waiting is so last season.

I'm excited to see uh where OTD goes. If you would like to reach out, uh talk to me about how OTD is going to change the way we style our lives, feel free to or if you just want to hear more about my personal story. Um thank you so much for your time.
[103:53] Speaker 2: One of the biggest unexpected parts about Gauntlet AI has been the emergence of so many game developers. The fact that we started this cohort and they hadn't built a game before,

--- Segment 11 (Starting at 100:00) ---

but now they can do it with AI.
So in that same way vein, I'd like to present Jack Mitchell and his game parkour run.

[101:15 - 101:25] (Silence)

[101:25] Alright. Hello everyone.
My name is Jack Mitchell, and for the past two weeks, what I've been working on is something that I've wanted to build for a long time, but I just haven't had the time to until now.
So I built an infinite open world parkour running game and, yeah, let's just get right into it.
So I've called my game parkour.run, and you can find it at prkr.run in any browser.
So let me just walk you through some of the core functionality.

So once you load in, you can run around, you can jump, you can slide, you've got some wall climbing, you've also got wall running for maintaining momentum and distance.
So that's all cool, but you've also got a grappling hook and double jump, which is super cool, they're unlockables.
So the core game loop is that you go around defeating enemies with your weapon, and then by doing that, you gain currency, which you can then use in shops around the world.
So that's all fine and dandy, but what I think sets apart parkour.run from the rest of the competition is the auto parkour and aimbot feature.
So it'll run around playing the game for you, basically, so then you can go do other things and then come back later and then upgrade your stuff.
So while that's going, I'd like to talk about my process.
So basically, in the past few weeks, I think there's been a massive paradigm shift in the way developers can develop games because with the release of new models like 3.7, Claude, it's become so much easier to code on a very high level.
So you no longer need to know necessarily all the low-level implementation details.
You can just say like, hey, Claude, can you add a gun to this parkour game?
And he'll just do it.
Like this took me 10 minutes to implement and now I have combat in my parkour game.
So point being that you can just like make stuff you want.
So I think at this point we've got some currency.
So the last thing I'd like to show you is the shop system.
So let's find a shop.
They spawn randomly around the world, and I think I see one over there.

So let's see if we get something cool.
Alright.
Looks like we can get a meteor tome, and it says it replaces the gun, so it's a new weapon.
Let's let's see what this does.
Let me go to a high vantage point to show you what it looks like.
And it looks like it just rains down meteors everywhere.
So if there's anything I want to leave with you from this presentation, it's that now that AI has gotten so good that you can just tell it to do stuff and it'll make stuff for you, you can create things that you used to only dream of building.
And so, yeah, I would encourage you to go out and build your dreams.
Thank you.

Alright.
Next up, we have the Lambert Brothers, Jefferson and Jared, who are bringing you the US Manufacturing Portal.
Their goal is to reinvigorate manufacturing here in the United States.
Let's welcome the two brothers.

[105:54] American manufacturing is falling way behind our foreign adversaries.
And um that is what Jared and I are here to change with US Manufacturing Portal.
So the issue is for decades we've been prioritizing low cost over manufacturing.
We've shipped all of our manufacturing capability overseas, and now we rely on China among other countries to manufacture parts that are critical for American companies and for our economy.
Our platform, uh, we hope will be the solution to a couple different problems in the United States.
We have a management platform, a marketplace, as well as instant quoting tied into all of that.
This is a little brief look into what our dashboard is looking like right now.
Uh, we want our software to be something you can drop into any machine shop in the United States and automatically start running their processes faster and better.
And the idea is that we'll have every shop API enabled.
We'll have a uh instant quoting process that'll allow us to open up a marketplace.
It'll be like the Alibaba of the United States for machine parts.
So uh right there you saw an image of what the marketplace might look like.
Somebody will be able to upload their part.
Our system will analyze it, find out how manufacturable it is, how many different operations it needs to be manufactured, and then generate a quote for the customer as well as suggesting improvements.
Uh, and like I said, this will be API enabled, so we'll have a um, you know, a marketplace where somebody uploads their part and we'll be able to get quotes from many different shops in the United States near them instantly.
So this, the manufacturing software industry is about $5 billion.
Um uh a $5 billion market.
Um of that portion of the market, we have about 16 to 17,000 different machine shops that would be able to use our software and take advantage of it.
And currently, uh with the current administration, lots of tariffs and things going on, manufacturing, there's a big push to bring it back to the United States and a lot of money has been invested into doing so.
For example, TSMC has just promised to invest $100 billion in bringing back chip manufacturing to the United States and lots of other investors are spending a lot of money, um, investing in larger uh, more automated machine shops.
Um, this is a picture of me with my father getting home from a deployment in 2008.
This is the whole reason why we're interested in this in the first place.
We care about our country.
We've been raised to care about the United States of America.
And we know that we're at a critical point right now where we have to be fixing the problem with our manufacturing and we think our our platform can be a part of the solution.
So with US Manufacturing Portal, we're going to speed up the process of manufacturing in the United States, cut quote times down to nothing.
And if you're an investor who wants to be part of that or a machine shop who's interested, please reach out.
Thank you.

Now, one of the biggest things that AI unlocks is the ability to create content, personalized content for all of you whenever you want it.
With that in mind, we have Joshua and Anthony, who are going to create AI-generated explainer videos on demand.

Hey, I'm Josh.
And I'm Anthony.
And we are Brunner, AI-generated explainer videos.
So, I imagine you've had the experience where you searched for something because you wanted a video to explain it and there wasn't really a good video that could explain it that well.
And I'm sure it would be nice if we had AI that could just generate explainer videos, but it's currently not that great.
And the reason is because making videos like that is hard.
And it's hard because there's tons of little decisions that are involved in making those videos.
Like just to give you a little ballpark, you're deciding what happens on every frame.
And for a 10-minute video at 24 frames a second, that's like 15,000 different decisions, which is a lot of time and energy.
And if you talk to maybe like an educational YouTuber, they'll tell you that it can take weeks to make videos like this.
And you might be thinking

--- Segment 12 (Starting at 110:00) ---

[110:00] Speaker 1: well, why don't we just use some of the new video generation models that are coming out.
[110:09] Speaker 1: And they're cool, like they're good at generating good graphics, but that's not the bottleneck.
[110:18] Speaker 1: We believe that the bottleneck is actually those thousands of little details that you have to get right.
[110:25] Speaker 1: For example, if you say chat GPT, make me an essay that's good that's on some subject.
[110:38] Speaker 1: Chances are the essay that it gives you won't be that good.
[110:43] Speaker 1: You have to actually get the prompt right, you have to work with it back and forth.
[110:52] Speaker 1: Eventually you'll get a good essay, but it'll take a little bit of work.
[110:58] Speaker 1: And it's the same thing with video models.
[111:03] Speaker 1: It might look cool, but with one shot it probably won't be that meaningful.
[111:09] Speaker 1: So, how do you get the details right and how do you do it quickly?
[111:16] Speaker 1: Enter Brenner, which is AI generated explainer videos.
[111:23] Speaker 1: So right now our project goes directly from prompt to video in one shot.
[111:33] Speaker 1: And in the future, we want to we want to make it into basically a pipeline.
[111:43] Speaker 1: We want it to give you like a like a like a little idea and tell you if it's going in the right direction, and if it's not, you want to correct it early and often.
[111:57] Speaker 1: So imagine like a like a discovery call with AI.
[112:03] Speaker 1: We also uh in the future are going to have it remember all of your preferences, so that way you can go from idea to video very quickly.
[112:15] Speaker 1: And to give you a little taste of that, we'll show you a little clip of what we can generate right now.
[112:24] Speaker 2: So we're going to show you here a quick 30-second clip that you can see.
[112:28] Speaker 2: This is just all text input and it's sped up a little bit for the presentation.

[112:39] Speaker 3: Welcome to our exploration of the quadratic formula.
[112:43] Speaker 3: A quadratic equation has the standard form X2 + bx + c equals 0, where A, B and C are constants and is not equal to zero.
[112:51] Speaker 3: The quadratic formula gives us the solutions.
[112:54] Speaker 3: Equals negative B plus or minus the square root of B2 - 4ac, all divided by 2a.
[113:07] Speaker 1: So that's a little taste.

[113:15] Speaker 2: So the big thing we were thinking about when we thought about this problem and solution is we thought about the learning pyramid.
[113:20] Speaker 2: A lot of you have probably seen this before.
[113:23] Speaker 2: What we realized is this is less of ranking methods of how we can actually learn, it's more of a workflow.
[113:34] Speaker 2: You I mean, you'll realize the bottom half of these, teaching others, practice, discussion, these are not ways to learn that initial content.
[113:44] Speaker 2: They're reinforcement learning.
[113:47] Speaker 2: So considering the initial content we look at, we see AI education as a amazing tool.
[113:55] Speaker 2: It's all stuck in the reading level though, 10%.
[113:59] Speaker 2: We see that if we can master AI educational videos, we can double that to 20% with audio visuals.
[114:09] Speaker 2: So looking at the progression of applicable AI modalities, as these each of these modalities come out, we see that as they reach production level quality, they expand exponentially.
[114:23] Speaker 2: Text, image and voice are all are all there getting there right now.
[114:29] Speaker 2: Um, what we're seeing is in the past, open source we didn't have open source models like Sesame, Corona and other ones as well.
[114:46] Speaker 2: And we're seeing that they are just getting the same quality too.
[114:51] Speaker 2: Um, and it's really impressive.
[114:54] Speaker 2: We're also seeing reasoning models as well, being a big help to be able to make long form content, like long form videos so they can be 5, 10 minutes and longer.
[115:06] Speaker 2: So when I was considering this this idea, the big question that I had was how much more useful is this in regular video?
[115:13] Speaker 2: Then if I just go on YouTube and I go search Khan Academy playlist.
[115:19] Speaker 2: And the more I thought about it, the more I realized that time and time again, we have seen these generalized incumbent providers switch to more individual personalized products.
[115:35] Speaker 2: And we believe the same thing is going to happen with with AI and video and audio visual education.
[115:43] Speaker 2: And so we're on the cutting edge of this.
[115:45] Speaker 2: We're ready to kind of exponentiate this and bring this everywhere and everywhere.
[115:52] Speaker 2: Thank you.
[116:11 - 116:21] (Silence)
[116:22] Speaker 4: Now, if you've ever coded with any AI coding tools, you'll know one of the hardest things to manage is versions of dependencies.
[116:33] Speaker 4: And so now for the next project, we'll have Joshua Jerbech, who's going to talk about anchoring, his solution to supercharging development with version specific um context.
[116:50] Speaker 4: Here's Joshua.
[117:05 - 117:34] (Silence)
[117:35] Speaker 5: Hello.
[117:39] Speaker 5: My name is Josh Cherbeck and I'm founding a company called anchoring.
[117:45] Speaker 5: Um it's both a desktop app and an API.
[117:51] Speaker 5: Um the current problems that AI faces with AI assisted development is that AI models uh are trained to generalize on a large corpus of data.
[118:06] Speaker 5: Um and they often fail when absolute specificity is required.
[118:13] Speaker 5: Um 66% of developers distrust AI code.
[118:19] Speaker 5: Only 10 to 30% of AI code matches correct library versions and 38% of AI generated code uses deprecated APIs.
[118:32] Speaker 5: Uh my solution to this is version specific documentation ecosystem, both with a desktop app and an API.
[118:43] Speaker 5: Um and we're going to revenue share with uh package maintainers so that they automatically sync uh up with our systems uh whenever a new version is released.
[119:01] Speaker 5: Um The business model uh AI platforms uh open AI, Anthropic pay us uh to whenever they're doing coding things, uh they request snippets based on the user's environment.
[119:19] Speaker 5: Uh so here's a little demo of the desktop app.
[119:23] Speaker 5: It's free and open source.
[119:26] Speaker 5: Uh it relies on uh web scraping versus actual documentation.
[119:36] Speaker 5: Um but you have anti patterns such as release, uh so that it doesn't recursively crawl uh other releases.
[119:48] Speaker 5: You're just going for the ones that you you want.
[119:51] Speaker 5: Um you version pin it.
[119:53] Speaker 5: You give it a a uh version um at the top left.
[120:02] Speaker 5: says Tori 2.40.
[120:07] Speaker 5: Um and here's the documentation snippets.
[120:10] Speaker 5: We do structured output.
[120:13] Speaker 5: So it gives it a title, uh description, list of concepts and the actual snippets itself all cleaned up um to give to an LLM when it's uh helping you develop.
[120:30] Speaker 5: And after this, we're going to show you a demo.
[120:35] Speaker 5: Um first without the MCP server.
[120:42] Speaker 5: Uh that's built into the desktop app.
[120:45] Speaker 5: Uh we're going to have it do a real time uh emission using Tori 2.0 uh which came out in 2022.
[120:58] Speaker 5: So I sped it up about 20X because uh rust compile times are a little slow.
[121:05] Speaker 5: Uh but it keeps trying to use deprecated syntax over and over and eventually gives up and tries to do something completely different using polling instead of real time emissions.
[121:17] Speaker 5: Then here we go with the MCP server LinkedIn and it's going to do the same exact prompt uh to give the uh it's going to do random words every few seconds uh on the React side, but it's emitting it from the rust side.
[121:41] Speaker 5: And there we go.
[121:43] Speaker 5: Uh no errors or anything.
[121:46] Speaker 5: It consistently looked up uh the documentation snippets based on the environment of the code base.
[121:54] Speaker 5: Thank you.

[122:01 - 122:10] (Silence)
[122:11] Speaker 4: All right, we're going to go back to our gaming revolution, our gamers here at Gauntlet AI and we're going to present Juan Pablo Herrera, who made Tussle Tubbies, a multiplayer online game that works on your browser.
[122:35 - 122:56] (Silence)
[122:57] Speaker 6: Okay, so the problem I'm tackling is that competitive shooters take a really long time to play.
[123:05] Speaker 6: Each game um takes like 30 minutes in Fortnite, and Valorant takes 40 minutes, and in Overwatch it takes 30 minutes.
[123:18] Speaker 6: So it just takes a really long time to play.
[123:22] Speaker 6: And people don't have time for this.
[123:25] Speaker 6: People want short content.
[123:29] Speaker 6: Like think YouTube versus Tik Tok.
[123:33] Speaker 6: Why watch one 40

--- Segment 13 (Starting at 120:00) ---

[120:04] Speaker 1: minute YouTube video when you can just watch many TikTok videos instead.
[120:12] Speaker 1: So, how nice would it be if there was a competitive shooter game that lasted that short amount of time, like three minutes.
[120:25] Speaker 1: So, that's what I want to build.
[120:30] Speaker 1: But surely there's a reason why all these competitive shooters take long, right?
[120:43] Speaker 1: Like but the reason is that um these competitive shooters build on previous games that were successful.
[120:57] Speaker 1: So think like Valorant and CS:GO.
[121:01] Speaker 1: Valorant took CS:GO and added characters like Overwatch and combined the two games.
[121:13] Speaker 1: So what if I take a game with a competitive scene and um has short games.
[121:26] Speaker 1: So that's what I did.
[121:28] Speaker 1: I took a mobile game called Brawl Stars and turned it into a competitive third-person shooter.
[121:41] Speaker 1: So, this is what it looks like but has three to five minute rounds, skill-based matchmaking, and it's a third-person shooter and you can play as different characters.
[121:54] Speaker 1: And I have a game mode called Gem Grab, which is specifically taken from the game.
[122:01] Speaker 1: So I can show a quick demo here of the matchmaking process.
[122:11] Speaker 1: So I have two different accounts here, I press find match and then it'll parry them up together.
[122:21] Speaker 1: And this is playing at 2X speed that you can see that the characters are grabbing the gems.
[122:34] Speaker 1: And the way this game works is that once one team reaches 10 gems, then a countdown starts.
[122:43] Speaker 1: And if the countdown finishes, then that team wins.
[122:48] Speaker 1: But if they die and lose those gems then the countdown resets.
[122:58] Speaker 1: So, as you can see here, I'm trying both different characters and you can see they're you can see they're moving on both of the browsers.
[123:16] Speaker 1: And the countdown has started, but then I'll kill them and the countdown resets.
[123:32] Speaker 1: So, this is made using 3GS.
[123:39] Speaker 1: I made the character models with um Blender MCP.
[123:48] Speaker 1: And I also rigged all the animations with Blender MCP.
[123:55] Speaker 1: And yeah, as you can see the blue team has won.
[124:01] Speaker 1: So yeah, that's my game.
[124:15 - 124:19] (Applause)
[124:23] Speaker 2: So, natural language to SQL presents a huge opportunity for anybody who wants to take it.
[124:31] Speaker 2: And this next group wants to take really hard to understand and get data from basketball stats and the NBA and put it in the hands of the exact people that need it, the analysts.
[124:46] Speaker 2: Presenting Cortex, Corey, Matt and Jordan.
[125:37] Speaker 3: Um, how's it going?
[125:39] Speaker 3: So for our project, we wanted to solve a real problem for real users.
[125:48] Speaker 3: So in that search, I was talking to a former colleague of mine who now works in NBA analytics and he expressed a common pain point that him and his peers experience, which is that for complex queries, they have to spend hours a day aggregating data across various sources and analyzing it just to answer a single query.
[126:13] Speaker 3: So out of that pain point, we decided to create Cortex, a natural language interface to advanced NBA data.
[126:25] Speaker 4: So we started by looking around in the marketplace to see if anyone was addressing this particular pain point and the best publicly available solution at the moment is Stat News.
[126:43] Speaker 4: You can see here, it's capable of answering a basic statistics question about LeBron from last year.
[126:53] Speaker 4: But this is something you could get off of Google or a quick chat GPT search.
[127:00] Speaker 4: If we use Cortex, we can bump up the complexity a little bit.
[127:07] Speaker 4: Ask something like what Warriors lineup had the highest plus minus over the 2022-23 regular season.
[127:31] Speaker 4: So while Cortex is thinking about the problem, I'll give a quick shout out to Daryl who owns the website PBP stats which services this niche community of data analysts.
[127:47] Speaker 4: He gave us read access to a legacy version of the database that has statistics going back to the year 2000.
[127:58] Speaker 4: Um, you can see that Cortex is having to think about the problem, process it, but on the other hand, even though latency is next to zero for Stat Muse, it doesn't understand even what we're asking.
[128:16] Speaker 4: Here we give back full response about the lineup, extra statistics just saved an analyst, you know, a couple hours out of their morning.
[128:29] Speaker 5: Uh, so that's the basic use case, but let's talk about design for just a moment.
[128:35] Speaker 5: Um, we naturally approached this problem from a very high level, uh, point of view.
[128:44] Speaker 5: We broke it down into three subsections: understanding the problem, and then, uh, uh, researching the problem in the database and answering the question.
[128:57] Speaker 5: Um, in the first step, we break the question down into smaller questions that could be found in a database.
[129:06] Speaker 5: Um, and then we pass it off to another agent that has the ability to go dig deep into the data to see how it's formatted, to figure out its internal relationships, um, and then to assert with certainty that it can find the data that we're looking for.
[129:28] Speaker 5: And then it passes all of the insights that it gained about the raw data to a separate code writing agent that's responsible for writing, um, executing and iterating on any possible things that can go wrong, any errors or implausible results.
[129:52] Speaker 5: Um, and then once it's able to justify its answer and gets a reasonable result, uh, it returns it to the user in natural language.
[130:02] Speaker 3: Yeah, so you can see we asked an even more complex question here, um, with a nod to expected value, which is a concept that analysts often need answers to.
[130:14] Speaker 3: And uh, Stat Muse has no concept of it at all.
[130:18] Speaker 3: It just gives a completely irrelevant response, uh, just listing the player from the season who had the most points, not the answer to the question.
[130:28] Speaker 3: Whereas Cortex, after it did some thinking, it found and provides an actually useful answer.
[130:37] Speaker 3: So, if we kind of zoom out, um, this is like a strong proof of concept that we think has good vertical scalability within the NBA world.
[130:48] Speaker 3: Um, NBA teams and analysts have use for this kind of thing.
[130:52] Speaker 3: But the interesting thing that emerged while we were building this is that the only reason while while we have some NBA uh and database specific logic in our pipeline, that was more of a function of the intense time constraint we were under than any capabilities.
[131:15] Speaker 3: And especially as AI capabilities increase, we see a clear path toward building a truly domain and database agnostic system that can just plug straight into any existing database and provide a natural language interface to answer complex queries about user data.
[131:41] Speaker 3: Thanks.
[131:43 - 131:45] (Applause)
[132:06] Speaker 2: So, if any of us have tried to read a research paper before, it's not that digestible.
[132:14] Speaker 2: Well, the next team has taken research and taken AI to make the insights coming out of a research paper something everybody can access and understand.
[132:26] Speaker 2: This is Danny and Lucas with Paper Mastery.
[132:31 - 132:34] (Applause)
[133:39] Speaker 6: Test.
[133:51] Speaker 7: Cool.
[133:52] Speaker 7: All right.
[133:53] Speaker 7: Uh, hi everyone.
[133:55] Speaker 7: I'm Danny, that's Lucas.
[133:58] Speaker 7: Uh today we'll be presenting Paper Mastery.
[134:02] Speaker 7: Uh Paper Mastery is a research learning platform that breaks down complex papers into digestible content and provides our users a path towards mastery.
[134:14] Speaker 7: Our goal is to make research approachable for everyone.
[134:21] Speaker 7: Picture this, you find a paper that with an interesting title but are left confused on what to do after reading the abstract section.
[134:33] Speaker 7: Wouldn't it be nice to have a

--- Segment 14 (Starting at 130:00) ---

[130:00] Speaker 1: tool to help guide you through this process.

[130:05] Speaker 1: Well, Paper Mastery does just that.
[130:10] Speaker 1: Uh, by utilizing various LLMs to create educational content and provide the user a starting point to their learning journey.

[130:22] Speaker 1: And if the learning material we've generated is not enough, we've even come up with solutions to help you go the extra mile.

[130:33] Speaker 1: Before I pass it off to Lucas to talk about that, a little bit of context on why we built this.
[130:41] Speaker 1: We have a special interest in this area from our own experience reading white papers and being frustrated when confirming whether or not we've achieved mastery on the subject.
[130:53] Speaker 1: We developed this tool with us in mind, but after scoping out the products in this space, we realized it could benefit far more people.

[131:10] Speaker 2: All right, so a little bit on the technical side.
[131:13] Speaker 2: Uh, you start by uploading a paper. You can do that through a PDF upload or you can do it through a journal link, archive link, or any other PDF URL.
[131:24] Speaker 2: After that, paper is going through our system.
[131:26] Speaker 2: We have a bunch of LLMs that generate a variety of learning content for you to actually understand the paper and test yourself on that.
[131:35] Speaker 2: And lastly, we've developed a pipeline that will reach out to paper authors and other experts in the field, uh, so that you can actually test with them whether or not you understand what you're talking about.

[131:50] Speaker 2: When we started looking into the space, uh, there were basically two existing products, Explain Paper and Scholarcy, both had really limited feature sets and paywalled very heavily.
[132:05] Speaker 2: Uh, so we were able to use much cheaper models and give away all of their features for basically free.
[132:13] Speaker 2: Uh, and the only thing that we might have to charge for is unlimited content regeneration and consultations with the researchers or experts in the network.

[132:43] Speaker 1: So why do we think this platform will work?
[132:47] Speaker 1: Uh, to start, it's mostly free.
[132:50] Speaker 1: And if that's not convincing enough, we think users will see our competitors are too expensive and lagging behind the latest AI developments.
[133:01] Speaker 1: We're also committed to matching our competitors bar for bar with any new features they might add in the future, as well as introducing our own.
[133:09] Speaker 1: For initial activity on the platform, in addition to hopefully everyone watching signing up, we tapped our network networks to provide users with high quality experts ready to help.
[133:22] Speaker 1: We're also targeting PhD students uh to start because as users, they need cheaper alternatives to our competitors.
[133:31] Speaker 1: And as experts on our platform, they'll have the opportunity to consult on research they already love and get paid for it.

[133:44] Speaker 1: Our platform doesn't just stop with PhD students, though.
[133:47] Speaker 1: We can expand to master students, undergrads, high school students, journal clubs, weekly digest, uh, pretty much virtually every corner that research touches.
[134:01] Speaker 1: Uh, we do this again with the goal of making research approachable for everyone.

[134:16] Speaker 2: All right, we're going to do a quick uh video demo.
[134:20] Speaker 2: Let's put this on full screen.
[134:22] Speaker 2: So when you log in, this is what the dashboard looks like.
[134:26] Speaker 2: You can see a graph of your research papers connected by various nodes that they both reference.
[134:32] Speaker 2: Paste in an archive link and we're going to pull the PDF, begin processing it.
[134:40] Speaker 2: We'll go into this page.
[134:41] Speaker 2: We're going to generate a bunch of different summaries at various complexity levels, so you can kind of ramp up on the paper smoothly.
[134:49] Speaker 2: And then once you finish each of these various learning sections, you mark them as done and we mark off some progress at the top of the page.
[134:56] Speaker 2: We uh generate queries and grab them grab YouTube videos that are going to be helpful for understanding things.
[135:05] Speaker 2: Make some quizzes so that you can test your knowledge immediately.

[135:13] Speaker 2: Flash cards are right here.
[135:15] Speaker 2: You can do them in the app or you can export to Anki and do your own uh space repetition.
[135:22] Speaker 2: If you're confused about anything in the paper, highlight it, hit explain or summarize, that'll send it to the chat interface and we'll explain it to you.

[135:37] Speaker 2: And then lastly is the consulting view.
[135:40] Speaker 2: So when you click book a session and you're a premium user, that'll kick off the pipeline and we'll try to find the researchers' uh contact info, reach out to them on your behalf.

[136:23] Speaker 3: There are millions of dollars every year that are given to researchers and professors to do research.
[136:31] Speaker 3: These are through grants that are provided by the government, but why is it so hard to apply to them?
[136:38] Speaker 3: So now we have Mark with Flumflow AI, who's going to make it easy for everyone to get money to do research.

[137:37] Speaker 4: All right, welcome to Flumflow AI.
[137:40] Speaker 4: Streamlining the grant application for maximum microphone.
[137:46] Speaker 4: Mark, Mark, use the mic.
[137:52] Speaker 4: Welcome to Flumflow AI, streamlining the grant application for maximum impact.
[138:06] Speaker 4: The total grant applications that were funded by the US government was just under 200 billion in the year 2024.
[138:16] Speaker 4: Grants enable critical research, essential services and educational innovation, driving advancement that would otherwise be unattainable.
[138:29] Speaker 4: They are vital for fueling progress.
[138:32] Speaker 4: The problem, grant writing, is time-consuming, sifting through guidelines, gathering data, and crafting a persuasive narrative demands significant effort.
[138:51] Speaker 4: Diverting resources from the core mission.
[138:56] Speaker 4: The solution is Flumflow AI, and the core of our application is a centralized document store that every applic every document that is uploaded to the grant application goes into it.
[139:09] Speaker 4: It's vectorized and available to all the assistance.
[139:14] Speaker 4: And also the shared thread.
[139:16] Speaker 4: That's a conversation thread that all the assistants use, so what you say to one assistant is available to the other assistants.
[139:26] Speaker 4: And then we have our three assistants.
[139:28] Speaker 4: The first research assistant will help you craft a stronger grant by asking you questions about your grant, and also answering questions about the grant.
[139:40] Speaker 4: And then we have the writing assistant that will write the grant, and the review assistant that will check it for spelling errors, logic errors, and other errors, and also check for compliance with the grant funding requirements.
[140:03] Speaker 4: How do we do this in 21 days?
[140:06] Speaker 4: We coded it with Claude and Cursor, uh, did 100% of the code base, did all the refactoring.
[140:13] Speaker 4: If we hit a problem that Claude couldn't solve, we use Gemini to get an answer and we were able to get past it.
[140:22] Speaker 4: And we use Grock to write all the documentation and the technical requirements.

[140:37] Speaker 4: So we'll go through a quick demo.

[140:57] Speaker 4: The first thing you do is you select your opportunity and enter in basic grant information, and then select your optional sections.
[141:11] Speaker 4: What you what it does when it generates the grant application is it will automatically attach to the grant application all the requirements that the funder has for that grant application.
[141:30] Speaker 4: Then what you can do is attach your own app uh documents.
[141:36] Speaker 4: In this case, I attached some research papers that support the thesis and I go out to the research assistant.
[141:44] Speaker 4: It asks me four questions, I give it answers and it will come back and ask more questions or answer the questions that I asked.
[141:54] Speaker 4: You can see on the right hand side that it has a history that you can review.
[142:00] Speaker 4: We will then generate our first pass of the grant and you can go in after that and review the sections uh where the text is.
[142:09] Speaker 4: Uh, we had a bit of an issue with the internet connection here, but it popped up uh and then if you don't like the content, you can edit it directly in the editor there or you can ask the AI to edit it.
[142:33] Speaker 4: And when it's done and you're satisfied with your application, you go out to the main page and you hit submit.
[142:43] Speaker 4: Or cancel.
[142:46] Speaker 4: In either case, uh submit will download your documents uh in word format to your to your drive, uh, so you can upload them to the grant application, uh, site on government grants.gov.
[143:06] Speaker 4: Uh, or cancel it and in either case, both your assistance, your thread, and your document store will be destroyed up on the open A server, Open AI servers so that it's not sitting there costing money.

[143:48] Speaker 5: What if I told you, what if I told you that you could write a book for just 15 minutes a day in just a year.
[143:59] Speaker 5: We think there's writers everywhere and with the next application unfold, you can just write your book in 15 minutes every day.
[144:12] Speaker 5: Here's Michael and Unfold.

--- Segment 15 (Starting at 140:00) ---

[140:00 - 141:45] (Silence)
[141:45] Speaker 1: All right. Hi, I'm Michael and I built Unfold. It's an AI writing assistant that uh doesn't do the writing for you, but it does help you complete all the non-writing parts of the process. And with this, you can complete a book by only working 15 minutes a day, so long as you can keep it up for a whole year.

The problem I'm trying to address here is that 80% of Americans have said at one point in their life that they'd like to write a book. They have a story in their head, they'd like to share it. But only 1% of those people will even attempt to try it, and of that 1%, only 3% will even finish their book.
Uh, my contention is, I don't think the best stories that uh people have heard are in the minds of professional authors. I think they might be in the heads of people who haven't even had a chance to try it yet. Uh, and so I built unfold to address this uh, basically the core impediments that are keeping people from putting pen on paper or hands on keyboard.

So, how Unfold solves this?

Uh
Yeah.
We uh basically the core issues keeping people from writing are uh issues with time. You get off work, you don't want to spend four hours writing a book. Motivation. I mean, once you uh once you start writing a book, how do you stay on it? Habits are notoriously hard to keep.
And then the structure. Like if you aren't a professional author, how would you even know how to start writing it to begin with? Uh, we address the time thing by sectioning off sessions into uh basically 15-minute writing sessions, much like how Duolingo would do their uh language learning process. Uh and we have AI in the background organizing everything for you.
Uh, motivation, we have scheduled notifications and uh progress tracking and achievements in app for all the work that you're doing. And then structurally, AI is in the background analyzing everything that uh you're feeding it so that uh it provides the guidance and uh doesn't do the writing for you, but will give you coaching and tips along the way.

So, how's this all work? Uh, basically, Unfold acts as our writing assistant. Uh, it keeps the state of the story in the background for you, uh uh in yeah, all of yeah, keeps track of all the associated entities that you're developing.
What you do is you dump in your brainstorms, your core ideas. Uh and what Unfold ends up doing is it takes all that content and forms an ontology of all of the story entities. Basically, the characters, the settings, and the plot points all get formed into models that uh get stored for the AI to be able to access efficiently in the future.
Uh next step, all of that information, those core entity models, are pulled in and and turned into outlines. Uh, that is outlines are structured and grow and grow and grow. Uh, in writing, you might call this the snowflake method.
Basically, this grows to the point uh to where all that's left is sort of the meat of the story, and this is the part that you write once your AI assistant has handled all of the background information for you. Uh, the meat of the story, you can call story beats. They're sort of the core entity of the story uh that you write. You might think of it as a paragraph, you might think of it as uh simply just a state uh any particular state change in the story.
Uh, Unfold basically sits in the background, provides you the background information that you've already fed it. It feeds you your ideas and then prompts you to write the next best part of the story.
Uh and then finally, it provides you uh basically a story map and progress updates so that you can keep track of everything you've been doing and keep you motivated to continue.

So, if you'd like to join Unfold and share your story, uh go ahead and scan the QR code here. I'd love to have you all and see what stories unfold.

I think everybody watching and everyone here at HQ has built a form before. And we all hate it. Drag and drop all these things, put it the form together. But the next team has a solution where AI does everything for you. This is Form Locker, Nicholas, Tim, and Wilson.

Bro.
Oops.
Test.
Okay, hey everyone. Uh my name is Nicholas and then we have Tim and Wilson. And the three of us we are Form Locker. So, Form Locker is an all-in-one uh form builder platform where we basically try to supercharge form building with AI.
So, Form Locker believes that forms suck. It's absolutely tedious for form creators to create forms. There's too much branching, too much conditionals and it's just way too much to control when forms are too big.
It's also boring for form responders because it's just too one dimensional. Like they don't want to answer too many questions, they don't want to go through five minutes just to answer a form.
81% of every responder actually drops out of the forms halfway through. So, why don't we just make AI do everything?
Why don't we just make an in between between a static form and a chatbot? Make it AI forms.
So, instead of making it too boring or make it too robotic like a chatbot, we thought of making it crafted personally for every responder using AI.
So, we think static forms is the basic level form, like a level zero. A more advanced one is the dynamic forms with the branching. We think the next one is actually AI assisted forms where where AI creates the questions for you, creates the flow for you. And finally, we have a product called Z forms.

And so with using Form Locker, building a traditional form is made super easy. With AI, a single prompt like this can turn into this. AI generate generates for you a pages pages of forms complete with input fields and relevant relevant information. Um, combine or what usually takes hours can now take seconds.
Combined with the drag and drop interface and configuration options, the forms are fully customizable. Um, you can add any input field like text, numbers, and signatures and files, whatever.

Yeah.
So, the second product that we offer is called Z forms. So, what is Z forms? Um, we believe that AI is better in humans at gathering user information. So, instead of creating the form questions yourselves, we allow you to define the exact information that you want to get from your users.
Then we let AI figure out how to best do that. So, the form creator only needs to provide some context and define the exact output schema that they want. And then the form respondent is able to enjoy a personalized and tailor-made form experience.
So, we have upcoming features like hybrid form models between AI assisting you creating questions and also the Z forms. We have more customization options coming up soon with like more casual tone or formal tone for AIs, analytics, multi-language support.

--- Segment 16 (Starting at 150:00) ---

[150:00] Speaker 1: and integration with other tools such as HubSpot, Google Sheets, etc. for your marketing needs.
[150:14] Speaker 1: So, why not we revolutionize forms with AI?
[150:20] Speaker 1: Come join Formlocker.
[150:24] Speaker 1: Thank you.

[150:33] Speaker 2: All right.
[150:36] Speaker 2: Next up, we have another gamer here at Gauntlet AI, who's been working on Monster Hunter and believes that the browser is the next big game engine.
[150:47] Speaker 2: This is Norman with Monster Hunter.
[150:53] (Applause)
[150:56] Speaker 3: Okay.
[150:57] Speaker 3: Okay, okay, right there.
[151:00] Speaker 3: Okay, okay.
[151:03] Speaker 3: Yeah, yeah, okay.

[151:11] Speaker 3: Connecting people all around the world, okay?
[151:16] Speaker 3: 7 billion people in the world, bring them all online and getting them together.
[151:24] Speaker 3: When I talk about this and ask people to think about a company that's associated with this slogan, the first thing that people think about, oh, is Facebook, right?
[151:35] Speaker 3: Bringing everyone together, connecting the world's people.
[151:43 - 151:48] (Silence)
[151:48] Speaker 3: When I think about connecting the world's people, I think about a game that I used to play when I was a little kid, okay?
[152:02] Speaker 3: When I think about connecting the world's people, I think about Club Penguin, okay?
[152:08] Speaker 3: A game that I used to love.

[152:12] Speaker 3: In the year 2025, nobody is on Facebook because they want to be on Facebook.
[152:20] Speaker 3: You're on Facebook because that's the way things have always been done, because you are afraid to miss someone's messages and because you're just simply trapped there.
[152:32] Speaker 3: Uh, nobody wants to be on a platform and hang out with, uh, people who might be robots, right?
[152:40] Speaker 3: People, uh, who might be hijacking your attention for malicious intent and a place that's frankly a wasteland that's filled with advertisements.
[152:54] Speaker 3: Believe it or not, people used to want to be on games like Club Penguin and Maple Story and World of Warcraft, okay?
[153:08] Speaker 3: Kids would come home and they would, every day they would come home and they would look forward to logging on and hanging out with their friends and doing cool things together with their friends online.
[153:22] Speaker 3: It was a big part of many people's lives, and it was a big part of my life too.
[153:26] Speaker 3: In fact, many people even found their husbands and wives on games like this, okay?
[153:34] Speaker 3: On games as humble, as you might think, as Club Penguin.
[153:40] Speaker 3: And the players loved the game so much that whenever one of them was shut down, bought out by a company or canceled, they loved the game so much that they would spin up their own private servers just so they could get the chance to play, okay?
[153:56] Speaker 3: And so I have a minute left.
[153:59] Speaker 3: And so, you all today are invited to uh, do an interactive demo.
[154:06] Speaker 3: So monsterhunter.io is the game that I'm releasing today.
[154:13] Speaker 3: Um, so everyone can log on and play together.
[154:20] Speaker 3: Uh, So this is an authentic, uh, real-time multiplayer game that runs in the browser, okay?

[154:43] Speaker 3: And, uh, there you go.
[154:47] Speaker 3: So here's two different players and everyone is uh invited to log on and play.
[154:56] Speaker 3: And so this is something that's been on my mind for a long time.
[155:01] Speaker 3: But we're also going to make it successful.
[155:05] Speaker 3: And the way to make it successful is the fact that this thing runs in the browser, everybody.
[155:11] Speaker 3: When you're in the browser, you have built in distribution and you can build it once and run it on iOS.
[155:19] Speaker 3: You can run it on Android, you can run it on Windows, you can run it on MacOS.
[155:24] Speaker 3: We're going to integrate Monster Hunter everywhere you can run a browser.
[155:28] Speaker 3: You're going to be able to play Monster Hunter and that's how we're going to build, right?
[155:33] Speaker 3: Something that people are actually going to love.
[155:37] Speaker 3: A place that people are going to want to be in and hang out with their friends with and do cool things online.
[155:45] Speaker 3: All right, Monster Hunter, everybody.
[155:48] Speaker 3: Thank you.

[156:01] Speaker 2: Next up, we have Peter Chen with Assemble.
[156:07] Speaker 2: Have you tried to build any deep hardware and had no idea where to start or what parts to buy?
[156:14] Speaker 2: Well, Peter's solving that problem with AI and his new app Assemble.
[156:21] (Applause)
[156:43] Speaker 4: Thanks Ash.
[156:45] Speaker 4: Uh, yeah, so I'm here to uh demo Assemble.
[156:51] Speaker 4: So Assemble is a uh AI powered web app that helps you select parts and create block diagrams for electronics.
[157:01] Speaker 4: Um, so a little bit of background.
[157:05] Speaker 4: There's been a few times, uh, in my life where I've wanted to create some hardware, something I thought was really cool.
[157:12] Speaker 4: Um, but I'd always get bogged down by like trying to look through the thousands and thousands of different components that are available to use.
[157:24] Speaker 4: And uh, yeah, it just made me really uncertain and uh kind of killed my momentum.
[157:32] Speaker 4: And um, after talking with some friends I've been in the industry for a pretty long time, um they say that it it can take a while even for experienced engineers and uh it can also get really expensive if you make a mistake.
[157:46] Speaker 4: Um, so with Assemble, hopefully uh the process can become much faster and much less error prone.
[157:55] Speaker 4: Um, yeah, so let's dive into the demo.

[158:00] Speaker 4: Um, so I'll just create a new project.
[158:12] Speaker 4: Yeah, so when you first open Assemble, it'll ask you uh what you want to build.
[158:19] Speaker 4: Uh, and then so I'll just put, let's say we want to build a smartwatch.
[158:30] Speaker 4: After you put that in, it'll use AI to generate additional follow up questions to really dial in what you're looking for.
[158:38] Speaker 4: Um, just for the sake of time, I'm not going to go through this whole flow.
[158:42] Speaker 4: Um, I'm going to skip it for now.
[158:45] Speaker 4: Um, and then you land in kind of the main interface.
[158:50] Speaker 4: Um, so you can manually add a part, uh, like a microcontroller.
[158:59] Speaker 4: It is much harder to type with one hand.
[159:02] Speaker 4: Uh, and you can just drag and drop your component onto your block diagram, kind of like lucid chart.
[159:09] Speaker 4: Um, but you can also ask AI to help you out.
[159:12] Speaker 4: So I can ask it to find me
[159:34] Speaker 4: Uh and then it'll start responding and it'll start looking stuff up.
[159:39] Speaker 4: Uh, And for the sake of time, I'm just going to jump over to uh another project where this conversation has already played out.
[159:51] Speaker 4: Um, I'm going to kind of show you what the AI can do.
[159:54] Speaker 4: Um, so it'll look up different parts for you.
[159:58] Speaker 4: Um, it'll give you the specs, it'll give you uh the link to the product page so you can look at it with more detail if you wanted to.
[160:08] Speaker 4: Um, out of the all the products that it looks up, uh, it'll make suggestions and pick the best one for uh what you're trying to build.
[160:17] Speaker 4: Uh, it'll allow you to add these products um with all this data, uh, into your projects.

[160:33] Speaker 4: And it'll also update your block diagram to actually contain these and it'll make the connections between the different interfaces that uh need to be connected.
[160:45] Speaker 4: Um, and you can always, you can also use it just like any other AI agent.
[160:49] Speaker 4: You can ask it questions, um, and you can, you know, ask it for updates as well.
[160:55] Speaker 4: Say like the microcontroller is too expensive, you want something cheaper.
[161:00] Speaker 4: It'll look up more stuff for you and it'll generate a table.
[161:15] Speaker 4: Um, and then it'll it can update your parts again.
[161:22] Speaker 4: Um, and then after you're done with all that, it'll give you a nice summary of uh everything that you're using, the price and the availability.
[161:30] Speaker 4: So you can go ahead and just go to the vendor, uh distributor in this case Digi Key and go buy it.
[161:38] Speaker 4: Um, and yeah, that's uh Assemble for now.
[161:42] Speaker 4: Uh it's still in the early stages.
[161:46] Speaker 4: Still a lot of features that we have planned such as exporting this, uh, so that you can import it into CAD software and you can build on top of it.
[161:56] Speaker 4: Uh, and yeah, and here's a QR code for the wait list and we'll let you know when it's ready to launch.
[162:05] (Applause)

[162:31] Speaker 2: All right, so over 13,000 people have joined us on this stream throughout the entire day.
[162:38] Speaker 2: But if you haven't gone to demo day.gauntletai.com and checked out all of our challengers, all of the projects that they've worked on so far or even a little bit of a recap, feel free to go there right now and make an account.
[162:54] Speaker 2: For our next team, we have Hero Hurder.
[162:59] Speaker 2: We have old school game engines like Unity being combined with new school tools and AI.
[163:08] Speaker 2: This is Brian and Riley with Hero Hurder.

--- Segment 17 (Starting at 160:00) ---

[160:00 - 161:45] (Silence)
[161:45] Speaker 1: Hello.
[161:46] Speaker 1: My name is Raley Bird.
[161:50] Speaker 1: My partner here.
[161:51] Speaker 2: I'm Brian Jensen.
[161:55] Speaker 1: And what we what we can is is a small experiment to see how older systems such as Unity might still work with newer systems such as Cursor.
[162:08] Speaker 1: So that they together can make game development perhaps reach the proverbial 10 times faster.
[162:20] Speaker 1: Uh, we decided that it'd be best if we just tried to make a game with them.
[162:38] Speaker 1: Originally, the idea was perhaps a little smaller than now.
[162:46] Speaker 1: But we looked around, there are plenty of powerful, well-established game engines, but few are meaningfully integrated with with with their AI tools that are available nowadays.
[163:03] Speaker 1: Some of them, such as Unity, have a handful, uh, but often they're not a direct part of the engine and you have to go hunt them down.
[163:18] Speaker 1: While some progress is being made, it often feels like the innovation of the AI tools is lagging behind their potential.
[163:34] Speaker 1: Part of the issue right now is that AI tools are often siloed with no standard way to connect them to engines or pipelines.
[163:47] Speaker 1: Teams must rely on work arounds and custom solutions which slows progress and limits creative potential.
[163:55] Speaker 1: Many game companies aren't even taking the time to learn how to make the new tools work with their old, simply because they have so many custom ones that that are all in-house and they can't be bothered to take the time because that would slow down development of their next big project and take too much money.
[164:17] Speaker 1: So we took matter into our own hands.
[164:19] Speaker 1: We said to test how modern AI could enhance classic engines by building a game with them.
[164:26] Speaker 1: As mentioned, Unity was the engine we decided to make the game in.
[164:31] Speaker 1: Chat GPT helped us brainstorm.
[164:34] Speaker 1: Cursor helped us to code, debug and streamline collaboration.
[164:40] Speaker 1: Dolly rapidly generate visual concepts and replacements for any assets we couldn't find online.
[164:48] Speaker 1: Combining the old tools with the new ones, created momentum.
[164:52] Speaker 1: Development was faster, creative decisions came easier, and pretty much tasks were reduced.
[165:00] Speaker 1: Giving the AI tools the context of our of our unity setup and our game design dock helped them to suggest actionable steps for implementation in the spots where they weren't themselves able to make the code or the items or the objects, et cetera.
[165:25] Speaker 1: The result, a working game that shows that when you stop waiting and start building, you too can make things.
[165:34] Speaker 1: This wasn't just about making a game, it was a test whose results show that AI could supercharge even legacy systems.
[165:44] Speaker 2: So as mentioned our nice title screen here was initially designed by Dolly and then cleaned up by other AI tools to get all those fancy artifacts that it loves putting in everything out.
[166:00] Speaker 2: Um, this background was initially generated by Grock and then fed into a Pixel diffusion engine.
[166:12] Speaker 2: And uh, a lot of our assets here like that nice sky background and these adventurer sprites all came from a free uh, free to use collection made by this very nice Japanese guy who just does that all day I suppose.
[166:40] Speaker 2: And um, yeah in the in the in the age of AI the value of free assets has really gone up quite a bit as the cost of software development is rapidly dropping.
[167:00] Speaker 2: And there's our game where you possibly get burned by a dragon.
[167:05] Speaker 1: The game itself, uh, is Hero Herder.
[167:10] Speaker 1: Play as a as the guild manager of a new found adventures guild and your job is to rebuild the guild, hire new adventures, make the buildings to give you more abilities and a possible build a team that can in fact slay the dragon lest it do the most terrible thing imaginable, come down and burn you to a crisp.
[167:45] Speaker 1: Again, my name is Raley Bird.
[167:47] Speaker 2: I'm Brian Jensen.
[167:48] Speaker 1: And we made Hero Herder.
[168:02 - 168:38] (Silence)
[168:38] Speaker 3: Now, before when we wanted to make a game, we needed a bunch of developers, we needed a bunch of money.
[168:51] Speaker 3: We needed all these other things than just our imagination.
[168:56] Speaker 3: But now with dreamup.gg, you can make a game in just one minute and bring your imagination to life.
[169:07] Speaker 3: Presenting Sebastian, Matt, Robert, Andre, and Chris.
[169:16] Speaker 3: This is dream up.
[169:20] Audience: Woo!
[169:20 - 172:29] (Silence)
[172:29] Speaker 4: Check, check.
[172:34] Speaker 4: All right, hello everyone.
[172:37] Speaker 4: We're the team behind dream up, the Replit for games.
[172:43] Speaker 4: We call ourselves the dream team.
[172:47] Speaker 5: I'm Rob.
[172:51] Speaker 6: I'm Matt.
[172:53] Speaker 7: Sebastian.
[172:58] Speaker 8: I'm Chris.
[173:00] Speaker 9: I'm Andre and I was going to build a live game for you guys, but actually after this lunch's speed build, I'm going to build two live games.
[173:19] Speaker 4: Okay.
[173:21] Speaker 4: All right, so everyone has an idea for a game.
[173:25] Speaker 4: Uh as kind of went through this, but I'll go through it again.
[173:35] Speaker 4: Um, but in the past, you couldn't just go and build it.
[173:40] Speaker 4: You had to spend all kinds of time uh learning skills like game development, uh going to school for years, or hiring expensive teams.
[173:55] Speaker 4: But now all you need is your imagination.
[174:02] Speaker 4: So we built dream up to allow you to go onto our site, put in your wildest crazy idea for a game, and watch as AI builds it for you in a minute or two.
[174:19] Speaker 6: So once you describe your game in text or voice, our game development agent will build it for you.
[174:26] Speaker 6: Uh it will plan out the rules if there's any ambiguity, pull in relevant assets and write the game code for you.
[174:34] Speaker 6: Uh if the result is not what you envisioned, you can edit your game with further requests and our game agent is based on the latest Cloud 3.7 Sonnet, which makes far more impressive games than any model that came before it.
[174:51] Speaker 9: As we instead of just showing things or describing things in abstract, we decided to have this to demo live.
[175:04] Speaker 9: I can create the game by typing.
[175:11] Speaker 9: And while you wait, we have some featured games to to pass time.
[175:18] Speaker 9: The other day, I thought, you know what?
[175:21] Speaker 9: I want a retro game for educational content.
[175:27] Speaker 9: So we created fraction invaders.
[175:32] Speaker 9: And you can play around.
[175:36] Speaker 9: The cool thing here is once you played and you thought this was nice, maybe you can share with a friend and you can keep editing it.
[175:50] Speaker 9: So I have a bunch of edits.
[175:53] Speaker 9: I shared with my friend Matt.
[175:56] Speaker 9: He decided to play a little more and make it a little nicer looking.
[176:04] Speaker 9: So he went and did a couple edits and had a better ship.
[176:18] Speaker 9: Now, one of the things we can do is click here, share with my friend Chris as well.
[176:25] Speaker 9: So Chris, why don't you play it out?
[176:29] Speaker 8: All right, thanks Andre.
[176:32] Speaker 8: And with his link, I'm able to jump right in to play the game and I could play and edit the game's himself, but I already know how to reduce fractions.
[176:46] Speaker 8: So I wanted to make something like a cross up between Golden Eye 007 but with ninjas.
[176:57] Speaker 8: So I was able to come up with that and get a pretty good game that I uh I was pretty happy with right off the bat, but I had some suggestions, uh tweaks, improvements that I want to make.
[177:15] Speaker 8: Uh, so I was able to keep chatting.
[177:20] Speaker 8: Uh, and the game editing ontology we use is to send up some of the previous conversation, the last version of the game code and the user's new query.
[177:36] Speaker 8: And that goes up to our agent, which is able to pull out outside libraries, like in this instance, 3js uh for 3D rendering for a 3D shooter, cannon JS for physics.
[177:52] Speaker 8: And but I didn't need to know any of that.
[177:54] Speaker 8: I just needed to keep chatting in order to create the game that I envisioned.
[178:00] Speaker 10: Since our launching, we have more than 100 weekly active users and more more than 1,000 games were developed in our platform.
[178:13] Speaker 10: Now, we're launching our premium features, which is for $29 a month, you can have more prompts, you can have better assets, and we develop a AI agent that transform any single player game to a multiplayer game.
[178:33] Speaker 10: So Chris, if you mind, can we play Golden Eye together?
[178:39] Speaker 8: Yeah, absolutely absolutely.
[178:42] Speaker 8: Let's jump in.
[179:01] Speaker 7: Sebastian, where are you at?
[179:09] Speaker 7: Oh, I see you.
[179:12] Speaker 7: Oh you do?
[179:13] Speaker 7: Where are you?
[179:14] Speaker 7: Are you shooting me?
[179:16] Speaker 7: I'm behind you.
[179:19] Speaker 7: Ah, there you are.

--- Segment 18 (Starting at 170:00) ---

[170:00 - 170:03] (Silence)
[170:03] Speaker 1: Cool.
[170:05] Speaker 1: And our games are ready.
[170:11] Speaker 1: So, enchanted flight you can choose an owl, a butterfly, a dragon, or a parrot and start flying.
[170:21] Speaker 1: And if you guys want to play cars against friends, you can just go to dreamhub.gg or scan this QR code.
[170:33] Speaker 1: Thank you.
[170:33] Speaker 2: Thank you.
[170:39 - 170:47] (Silence)
[170:47] Speaker 3: For our next challenger.
[170:51] Speaker 3: He believes that software is being democratized.
[170:55] Speaker 3: If there's a SAS out there, you can just rebuild it with AI.
[170:59] Speaker 3: That's why he's created Assassins and he's going after Docusign.
[171:03] Speaker 3: This is Robert Cardwell.
[171:07 - 171:13] (Silence)
[171:13] Speaker 4: Uh, no.
[171:14 - 171:17] (Silence)
[171:17] Speaker 4: Yeah.
[171:17] Speaker 4: Uh, please work.
[171:23] Speaker 4: Uh, do we have signal?
[171:32 - 171:35] (Silence)
[171:35] Speaker 4: Um, there we go.
[171:36] Speaker 4: Okay.
[171:37] Speaker 4: Uh, very good.
[171:38] Speaker 4: So, uh, the assassins, I'm going to kill DocuSign and uh, and others and you're going to make a killing.
[171:54] Speaker 4: So, this is what the world would look like if people had never decided that you could charge $10 a month simply to sign some documents online.
[172:06] Speaker 4: Um, AI has destroyed everyone's Moat and this is not just a pitch.
[172:17] Speaker 4: This is also a, uh, financial, not financial advice.
[172:22] Speaker 4: So, people are not pricing in Gauntlet.
[172:28] Speaker 4: Consider how often people are surprised when Austin comes over and he does his demos and everyone's like, oh, I want to hire gauntlet students.
[172:39] Speaker 4: So the market has figured out that AI is a thing.
[172:43] Speaker 4: It has not figured out how it is a thing.
[172:47] Speaker 4: Um, Now there, DocuSign is a particular example because a, their product has way too many people, and b, it sucks.
[173:01] Speaker 4: So, I'm not the only one who's doing this, but uh, this is actually a good thing because it's essentially more, think of it as more shots on goal.
[173:14] Speaker 4: The more free products there are, the more people are going to be like, oh, the price that I should be paying is zero dollars, and they're not going to pay for signing their documents.
[173:32] Speaker 4: And my plan is essentially to make a big, splashy public launch with this, um, ideally get investment, which I can use to advertise because advertising is is fierce, um, post to post to Reddit, post to Twitter, uh, Wall Street bets, basically just really get people thinking like, oh, the price for signing a document should be zero dollars.
[174:09] Speaker 4: Um, Turns out it's actually really easy to build DocuSign.
[174:16] Speaker 4: This is what it looks like after about 2 hours.
[174:19] Speaker 4: You have the basics.
[174:21] Speaker 4: Um, this is what it looks like after about five days.
[174:28] Speaker 4: We have a little bit more than the basics, um, but still like a nice clean interface.
[174:36] Speaker 4: And this is what it looks like after 10 years of zero interest rates.
[174:49] Speaker 4: The DocuSign, if you go to click, like the very first thing you see is the login screen.
[174:57] Speaker 4: It's bad.
[174:58] Speaker 4: I don't need to explain more how it's bad.
[175:00] Speaker 4: Um, so my quick search found like three different people who have made perfectly good products, all of which are pretty similar.
[175:12] Speaker 4: Um, mostly they're not getting a huge amount of attention.
[175:16] Speaker 4: This is not something, this is not the first um thing that you're going to go after when you have the power of AI.
[175:26] Speaker 4: Like, first you're going to build something that makes you money and then well, everything will have been have been built already, so you make something that'll make you a little bit less money.
[175:40] Speaker 4: Um, Essentially what I'm looking for is investment or people who wanted to coordinate this with me, uh, essentially publicity blitz, you can short the stock.
[175:55] Speaker 4: Um, you can get people very scared because it's a new thesis and then you can cover.
[176:03] Speaker 4: Uh I leave you with this meme and I leave you with my uh contact info and where you can actually go and see the demo website I have up.
[176:17] Speaker 4: All right.
[176:17] Speaker 5: Yeah.
[176:23 - 176:27] (Silence)
[176:27] Speaker 3: Now, if you want to start playing Dota, there was a steep learning curve.
[176:33] Speaker 3: But with the new Dota 2 draft assistant and Ryan's new application, you can start learning now and get better at drafting.
[176:41] Speaker 3: This is Ryan Harron.
[176:43 - 177:06] (Silence)
[177:06] Speaker 6: Howdy.
[177:08] Speaker 6: I am a degenerate gamer and I play Dota 2.
[177:15] Speaker 6: That's my favorite game.
[177:17] Speaker 6: Dota 2 is one of the most popular games right now.
[177:21] Speaker 6: It's got about 600,000 average viewers a day, uh, players, excuse me.
[177:29] Speaker 6: It's got several tens of millions of active players throughout a month.
[177:35] Speaker 6: Uh, but the unfortunate thing about Dota is that it's very complicated.
[177:39] Speaker 6: It's got a very strict rule set.
[177:42] Speaker 6: You can do a lot of things in it and there's 126 unique characters that all interact with each other in a rock paper scissor sort of format, which means that one hero will counter another while in turn countered by a third.
[178:02] Speaker 6: When I started playing Dota, I got very quickly into the competitive aspect of the game, which involves drafting at the start of every match where you would pick and ban heroes to create your perfect team against the enemy.
[178:20] Speaker 6: You do this with knowledge of what the enemy plays and what you play, so they would also know what you play and it would complicate things in terms of mains, counters, synergies, and so on and so forth.
[178:35] Speaker 6: And I sucked at drafting.
[178:40] Speaker 6: I was terrible at it because I didn't know anything about the game and you have to know far too much.
[178:51] Speaker 6: When I wanted to practice my drafting, I had no one to practice with.
[178:56] Speaker 6: Other captains didn't want to do it because they would be teaching me things that would help me to beat them.
[179:05] Speaker 6: So why not make a robot that does it better than them anyway and practice against that.
[179:11] Speaker 6: I have here the Dota 2 Draft assistant.
[179:17] Speaker 6: As you can see, you can start a draft at any point in time and you will be recommended picks and bans as well as explanations for why you should pick and ban certain heroes based on the meta of the professional games of Dota 2.
[179:37] Speaker 6: So the top level games.
[179:39] Speaker 6: You can also choose to select the opponent's level, which means that you can choose to pull in additional information from a certain rank of the, uh, opponent to better focus against who you'd be playing against.
[179:59] Speaker 6: So you still have the professional meta, but now you're playing against people from your own rank and you're drafting against them too.
[180:13] Speaker 6: The other things that this app includes are a player profile to track your games.
[180:25] Speaker 6: It will track them in real time, updating once every hour, as well as a live assistant, which is currently in development.
[180:41] Speaker 6: With this app, I hope to level up the uh, the drafting game of both professional and amateur teams as well as to teach players how to draft using the recommended picks and bans and analyses.
[181:06] Speaker 6: Thank you.
[181:11 - 181:16] (Silence)
[181:16] Speaker 3: All right, next up we have Shahiir Ahmad, who has his email agent and uses browser use envision for task management.
[181:25] Speaker 3: Here's Shahiir.

--- Segment 19 (Starting at 180:00) ---

[180:00-180:02] (Silence)
[180:02] Speaker 1: Hey there everyone, my name's here.
[180:04] Speaker 2: Thank you. I'll I will use the microphone.
[180:08] Speaker 1: Sorry.
[180:09] Speaker 2: Hey there, my name is here, Ahmed, and this is email agent.
[180:14] Speaker 1: So, currently, a lot of LLMs are, you know, they're trained on a ton of internet data, right?
[180:20] They have the entire corpus of the internet, a ton of text.
[180:23] Next token prediction has allowed them to make huge strides in intelligence and break through the scaling wall, right?
[180:29] Um, however, when it comes to actually making, you know, action taking actions within the real world, they may not necessarily have the context or knowledge of the abstractions that we take for granted on a day-to-day basis.
[180:42] For example, when you're training an LLM, um, something like GPT 3.5 or four, it's not looking at images of the of a website, it's looking at just the code of the website.
[180:52] So then when you're trying to have it access the web, it's not going to have nearly the same performance as when you ask it to write a PhD paper, for example.
[181:00] And so if we truly want agents to participate in the global economy, they need to interface at least for some period of time with the same systems that we do on a day-to-day basis.
[181:09] An email agent is the perfect way to start testing out this application because we can have our agent take actions for us and actually provide value.
[181:22] So, over here, I decided to use browser use an open source library and have it take actions on my email account, so it could send emails, summarize content from emails, and also unsubscribe from emails.
[181:34] So, as you can see over here, I did have to give it my login information, so if you're not running this on local host, you might have to think about your privacy concerns, but as you can see over here, it's accessing the Google login page.
[181:50] It puts in the information and then it's going to look through my email.
[181:56] Um, as you can see, right, it's has to scan through the page and get a better understanding, hey, what's going on here.
[182:03] And then it's looking for keywords to understand what to do.
[182:08] And so, as you can see here, I'm asking my friend uh about that thing I sent him.
[182:14] Um, There are also multiple other examples.
[182:18] Uh, for example, there's this summarization process where it has to look through my email and then um look uh look for certain senders and then summarize the information.
[182:32] Right here.
[182:33] So, this would be perfect, let's say if you if you have a ton of articles from substack in your inbox and you want to get a better understanding of what you're missing out.
[182:43] And then this is the one one of my favorite examples is unsubscribing from emails, right?
[182:50] So, currently, you know, what do you call it?
[182:52] This can be a huge concern for a lot of people.
[182:55] You know, we have a ton of spam, right?
[182:58] And let's say you're tired of getting Groupon emails, you could have your agent do all the work of unsubscribing for you.
[183:07] Um, And yeah, as you can see here, it finds the unsubscribe button and is able to unsubscribe.
[183:16] So, if we want agents to participate in the economy, right, and actually take actions outside of like an MCP circle MCP server with predefined, you know, context and actions, then they need to start interfacing with the web and, you know, maybe even use some of the tools that we're creating here at Coled AI.
[183:36] And thank you.
[183:37] That's all I got.
[183:46-184:04] (Silence)
[184:04] Speaker 3: All right.
[184:06] Next up, we have a team that is changing the way that we test software engineering.
[184:13] Now, with the advent of AI, our traditional software engineering tests aren't going to work anymore.
[184:21] People use AI to solve all those algorithms instantly.
[184:25] So this is Vibe Decoder with Spencer and Tim.
[184:34-184:38] (Silence)
[184:38] Speaker 4: Use this.
[184:38] Speaker 5: Sure.
[184:39] Speaker 4: Use this.
[184:45-184:55] (Silence)
[184:55] Speaker 6: Yo, y'all, my name is Tim.
[184:57] This is my beautiful partner, Spencer here, and coming at you live from Austin, Texas.
[185:02] We're here to introduce our solution, Vibe Decoder.
[185:07] But before we talk about our solution, we need to address the problem in the room.
[185:12] And the problem is twofold.
[185:16] The current technical evaluation process for hiring a software engineer is both broken and obsolete.
[185:24] Well, why is it broken?
[185:27] Well, currently, companies hiring software engineers typically give home a technical aptitude test that involves either a take-home exam or a hacker rank challenge.
[185:39] Both prohibit the usage of AI.
[185:42] Inevitably, people cheat on those exams by either memorizing the algorithm for hacker rank or using AI on the side.
[185:50] And why is it obsolete?
[185:52] Well, I think we're all an attestation to the fact that building software today looks radically different than it did a year ago.
[186:03] And we believe, our thesis is, the individual's ability to orchestrate a result or set of results using one or more AI tools is going to be the predominant um skill that's going to be very important in hiring software engineers.
[186:18] With that being said, we're happy to introduce Vibe Decoder, which is a way that we reimagine what the technical hiring process for an engineer looks like.
[186:27] Instead of discouraging or prohibiting AI usage, we allow you to do it however you see fit.
[186:35] The participant opens up the the practice exam.
[186:43] They're given a problem and we collect video and we collect audio and we have a live AI voice agent that not only passes down the requirements of the assignment, but also ask engaging questions because fundamentally we believe that collecting and extracting how the user thinks while they're orchestrating a result with AI tools is more just as important as them successfully completing the exam itself.
[187:14] We want to extract their mental model.
[187:17] We want to identify their brain their decision-making tree while they make decisions.
[187:21] What tools do they use?
[187:23] We believe that this all helps us collect data to make a candidate viable for hiring partners.
[187:29] And with that, I pass it off to my beautiful partner, Spencer.
[187:34] Speaker 7: Thank you.
[187:36] Next slide.
[187:40] Here's what it looks like for the candidate or the test taker.
[187:44] The first time they log into the test, it asks you to share your screen and share your audio.
[187:51] That's because it's very different from other coding tests.
[187:56] We're going to use AI to analyze your entire screen.
[188:04] Once you're into the test, you see the rules, you're allowed to do pretty much anything you want.
[188:09] You can use AI, you can use documentation.
[188:13] The only thing you cannot do is talk to other humans.
[188:17] And then we give you the task and this is the task that was given to people.
[188:26] Now, here's what it looks like on the hiring manager side.
[188:29] We gave this test to 10 Galet litterers to see who is the best at using AI.
[188:37] We have the results scored from top to bottom and you also see that we have a transcript at the bottom.
[188:45] If the hiring manager would like to see the entire video, that option is available as well.
[188:54] Now, if we click the drop down for AI usage, this shows you why the score was assigned and also evidence supporting that score.
[189:04] We wanted to make sure that we were assigning accurate scores, so this is critical information for the hiring manager.
[189:11] They need to know the why and they need to know the evidence.
[189:15] That can give them the confidence that the score is correct.
[189:21] So, how did we get these scores?
[189:23] And how are we using AI in this application?
[189:27] The first step is that we pass the video to Gemini and generate that entire transcript that you saw on the last slide.
[189:36] The second step is that we have more prompts to actually assign the grade.
[189:41] Each company may have their own preferences or their own ontology of what the grade should be, so we allow them to customize the rubric.
[189:53] With all that being said, we hope that this app helps company helps companies decode who are the best vibe coders.
[189:59] That's it.
[190:00] Yeehaw, baby.
[190:01] Galet 2025.
[190:04-190:15] (Silence)
[190:15] Speaker 8: All right, AI gives us the opportunity to create personalized learning aids and education for all of our children around the world.
[190:26] What if we could supercharge reading fluency with AI?
[190:31] That's exactly what Suman built.
[190:33] This is Reading Beast.
[190:36-190:39] (Silence)
[190:39] Speaker 9: You should have to take out the Speaker 9: Uh screen.
[190:58] Speaker 10: your screen.

--- Segment 20 (Starting at 190:00) ---

[190:00] Speaker 1: Okay, this is what the child sees.
[190:05] Speaker 1: Sorry guys, see?
[190:10] Speaker 1: The audio is not working.
[190:13] Speaker 1: So this is audio.
[190:15] Speaker 1: Here is the library which consists of a collection of AI generated short stories.
[190:22] Speaker 1: Just want to make sure the audio is working.
[190:25] Speaker 1: These short stories can be filtered by various levels and then you have to change it to the speakers in which we're going to dial.
[190:30] Speaker 1: The child then picks the one that would like to read.
[190:33] Speaker 1: Where is it at?
[190:35] Speaker 1: The task is to read the passage aloud one minute.
[190:38] Speaker 1: Is it working?
[190:39] Speaker 1: And AI will analyze the reading.
[190:40] Speaker 1: Okay, wait a second.
[190:42] Speaker 1: Thank you.
[190:42] Speaker 2: Space Adventure.
[190:44] Speaker 2: Maya wanted to be an astronaut.

[190:57] Speaker 1: Sorry just you can use a mirror now instead.
[191:18] Speaker 1: He's right here.
[191:20] Speaker 1: I don't see it there.
[191:22] Speaker 1: I'll put a new one right there.
[191:27] Speaker 1: Okay, hi everyone.
[191:29] Speaker 1: This is Suman here and I want to introduce you to reading Beast.
[191:34] Speaker 1: Reading Beast is an AI tool that helps your children to improve in their reading fluency.

[191:40] Speaker 1: So this is the problem.
[191:42] Speaker 1: One recent study showed that 32% of fourth graders are reading below their reading proficiency.
[191:52] Speaker 1: This means that you know, they have problems, the poor reading skills have problems like academic setbacks, social isolation, and an emotional challenges.
[192:07] Speaker 1: So I built a solution, which is reading Beast and this is uses AI, advanced AI to assess reading abilities and it's gamified in such a way that the children find the practice as not as a chore, but as a fun quest to obtain badges and points.
[192:32] Speaker 1: So I prepared a pre-recorded demo for you so that it's all going to be easy for you.
[192:38] Speaker 1: I hope the audio works.

[192:56] Speaker 1: Okay, this is what the child sees after they log in.
[193:01] Speaker 1: And here is the library which consists of a collection of AI generated short stories.
[193:14] Speaker 1: These short stories can be filtered by various levels.
[193:24] Speaker 1: The child then picks the one that he would like to read.
[193:27] Speaker 1: Let's pick this one.
[193:35] Speaker 1: The task is to read the passage aloud for one minute and AI will analyze the reading.
[193:45] Speaker 2: Space Adventure.
[193:47] Speaker 2: Maya wanted to be an astronaut.
[193:50] Speaker 2: She read books about planets and stars every night.
[193:56] Speaker 2: Her room was decorated with pictures of space.
[194:02] Speaker 1: This is my son reading the passage.
[194:11] Speaker 2: We couldn't wait to explore space someday.
[194:20] Speaker 1: And here are his results.
[194:24] Speaker 1: So here are the results generated by the AI.
[194:29] Speaker 1: On the main screen, you get to see the number of words that are correct per minute, reading accuracy, areas of strength, and areas that needs some work along with the errors.
[194:46] Speaker 1: In the next step, you see a more detailed analysis, including fluency, prosity, confidence.
[194:58] Speaker 1: And on this step the user going to click on the words to practice.
[195:05] Speaker 2: Museum.
[195:09] Speaker 2: Exercise.
[195:17] Speaker 1: They can also practice interactively with the AI.
[195:30] Speaker 1: So the child is encouraged to practice, we take the test and improve the score.
[195:40] Speaker 1: The child's reading fluency is tracked over time here.
[195:48] Speaker 1: By month, all time, this week, the profiles tab show all the badges that the child has collected over time and what is still left to be unlocked.

[196:15] Speaker 1: Okay, so finishing off this product has a immense market opportunity with 90,000 elementary schools in the United States and 24 million K through 5 students and an edtech market that is rapidly growing.
[196:41] Speaker 1: So just to finish off, I would like you to invite to join and use reading Beast and to transform the reading experience for children.
[196:52] Speaker 1: Thank you very much.

[197:08] Speaker 1: There are thousands of people every day using chat, GPT and cloud to combat shyness and nervousness.
[197:18] Speaker 1: This next challenger built shy chat, which lets you practice and get better at public speaking and different events that might make you shy.
[197:28] Speaker 1: This is Tom Jacob.

[198:45 - 198:56] (Silence)

[199:13] Speaker 1: Oh, uh, it's okay.
[199:15] Speaker 1: Uh, I can just drag it in.
[199:22] Speaker 1: Yeah.
[199:30] Speaker 1: Oh, it's live.
[199:32] Speaker 1: Okay.
[199:33] Speaker 1: Yeah.
[199:35] Speaker 1: Slide show.
[199:40] Speaker 1: Oh, nice.
[199:43] Speaker 1: Oh, okay.
[199:44] Speaker 1: Uh, yeah, let's see.
[199:46] Speaker 1: I don't know how.
[199:49] Speaker 1: Ah.
[199:50] Speaker 1: Okay, let's see.
[199:52] Speaker 1: Yeah.

[199:56] Speaker 1: Uh, yeah, I see it.
[199:57] Speaker 1: All right.
[199:58] Speaker 1: Thank you, thank you.

[200:11] Speaker 1: I got you.

[200:37] Speaker 1: Yeah.
[200:39] Speaker 1: Okay.
[200:40] Speaker 1: Hello everyone.
[200:42] Speaker 1: Uh, thank you for coming today.
[200:45] Speaker 1: All right.
[200:45] Speaker 1: So, have you ever felt nervous or unsure about how to start a conversation or found yourself creating awkward pauses?
[200:54] Speaker 1: You're not alone.
[200:55] Speaker 1: Today, many teens and young adults are having trouble being social, they're becoming less social.
[201:01] Speaker 1: They're experiencing higher levels of social anxiety than ever before.
[201:06] Speaker 1: So, you can see from this graph here, right?
[201:10] Speaker 1: Teens are becoming less social, they're hanging out with friends less and less, right?
[201:14] Speaker 1: They're alone, they're on their devices.
[201:18] Speaker 1: You can see the outcome as well.
[201:20] Speaker 1: Anxiety is going up and it's the highest among the youngest generation.
[201:25] Speaker 1: And what are the causes?
[201:26] Speaker 1: Likely, people are just alone, right?
[201:28] Speaker 1: They're not hanging out, they're not talking to people.
[201:31] Speaker 1: So what's the outcome of that?
[201:33] Speaker 1: A lot of shy people, right?
[201:35] Speaker 1: People are afraid of going out, talking to people.
[201:39] Speaker 1: So I've noticed this problem myself, right?
[201:41] Speaker 1: After COVID especially, and so I decided to work on my social skills.

--- Segment 21 (Starting at 200:00) ---

[200:00] Speaker 1: Dios.
[200:01] Speaker 1: Um, so I found a online course, a social skills course, and honestly, it wasn't that good, right?
[200:13] Speaker 1: Um, I talked to people who were there for a long time and they didn't seem to be getting a whole lot out of it.
[200:20] Speaker 1: I talked to someone who's there for about a year and they're still taking the course.
[200:25] Speaker 1: And uh, I I don't know.
[200:27] Speaker 1: It didn't seem to be working for them.
[200:30] Speaker 1: And um, the problem that I found was that they're using a lot of information, they're giving you scripts on what to say, and they're not enough give not spending enough time actually socializing.
[200:46] Speaker 1: There's only about 15 minutes of a of a a Zoom lesson at the end where you actually end up talking to people.
[200:55] Speaker 1: So my best understanding is that the circuitry is already in your brain to learn how to socialize.
[201:02] Speaker 1: And so, the best way to learn how to socialize and learn how to talk to people is just get the reps in and talk to people.
[201:11] Speaker 1: So that's how I came up with Shy Chat.
[201:15] Speaker 1: The idea here is, I want people to be able to build conversation skills naturally.
[201:24] Speaker 1: All right?
[201:25] Speaker 1: So, my app provides an easy, low stakes way to learn socialization and conversational skills.
[201:36] Speaker 1: It uses Chat Rulette style matching to allow you to talk to as many people as possible and allow you to get your reps in.
[201:46] Speaker 1: So you get more exposure and decrease your social anxiety over time.
[201:54] Speaker 1: And we have the added benefit of um, the scripts from before, the information, right?
[202:01] Speaker 1: So you have the best of both worlds.
[202:03] Speaker 1: You can talk to people with essentially uh a guy in your ear guiding you if necessary.
[202:11] Speaker 1: And this is done via tool tip and it'll give you conversational cues based on uh the sh- of the call transcript.
[202:22] Speaker 1: So we're giving the transcript to AI and then we're getting uh information about what you might be able to say next, guiding you throughout the conversation, giving you essentially training wheels as you talk to people.
[202:39] Speaker 1: So here we have a screenshot of the app.
[202:42] Speaker 1: And you can see uh conversational tips uh that are shown up for this conversation.
[202:51] Speaker 1: You have uh um, what are you up to for the rest of the day?
[202:58] Speaker 1: What is it about bicycling that you like?
[203:02] Speaker 1: Speaking of bicycling, what's your favorite bike route in Austin?
[203:06] Speaker 1: So obviously the conversational partner was talking about bicycling and uh we have these uh tips available.
[203:15] Speaker 1: And so the idea here is, we go from uh nervous and afraid of talking to people to uh confident, comfortable and eager to engage in uh conversations.
[203:33] Speaker 1: Thank you.
[203:34] Speaker 1: It's been Shy Chat.
[203:39 - 203:49] (Applause)
[203:55 - 204:15] (Silence)
[204:15] Speaker 2: Over 15,000 individuals have joined us live on X. And if you want to apply to be a hiring partner, go to Golet AI.com, and you can also apply to be a challenger for our next cohort.
[204:40] Speaker 2: Up next, we have Tree, who's created a 3D multiplayer game called tensu.io.
[204:50] Speaker 2: Let's welcome Tree.
[204:52 - 205:10] (Applause)
[205:10] Speaker 3: So we have one mic for you, Cornel.
[205:13] Speaker 3: Hold it.
[205:14] Speaker 3: And then face him.
[205:14 - 206:10] (Silence)
[206:10] Speaker 4: Okay, hello everyone.
[206:13] Speaker 4: Uh I realized that there have been a lot of game presentations here and I personally think that they're pretty cool, but if you have gotten tired of seeing them, I apologize because I made another game.
[206:35] Speaker 4: My game is called tensu.io and it's an online multiplayer game where you play as a slime character, a magical slime character.
[206:51] Speaker 4: Originally, I wanted to create an open world game uh because it's always been my dream to create an open world game.
[207:00] Speaker 4: But I realized that with the time frame that we were given, I didn't have enough time to do the project.
[207:08] Speaker 4: It was too ambitious.
[207:10] Speaker 4: So I decided to do an online multiplayer game instead.
[207:15] Speaker 4: I decided to create an IO game because historically, they've been pretty popular and what's made them popular is that they're browser based, which makes them very accessible and uh they have short search session links, meaning that you can play a game without taking up too much of your busy schedule.
[207:45] Speaker 4: And this has led to them uh receiving a lot of traffic over the years.
[207:55] Speaker 4: Before I uh show my demo, I would like to say that tensu.io is available right now.
[208:04] Speaker 4: If you go to tensu.io or if you scan the QR code, it works on both desktop and laptop sorry, desktop and mobile.
[208:14] Speaker 4: So, I'll just give everyone a few seconds to scan it if you want to.
[208:25 - 208:38] (Silence)
[208:38] Speaker 4: Okay.
[208:40] Speaker 4: Let me show the demo now.
[208:46] Speaker 4: So like I said, tensu.io is a a play game where you play as a slime character.
[208:57] Speaker 4: And the objective is you're placed in a world with other players and you have to farm these crystals, those blue things that you see, gain XP to level up, uh battle other players and eventually make it to the top of the leader board on the top right.
[209:20] Speaker 4: Um, this game was made using 3js for the graphics engine, uh Rapier for the physics engine and for ne the network model, I used uh client authenti client authority with service side validation in order to uh have smooth movement and combat.
[209:45] Speaker 4: And as you can see for a browser game, it does pretty well.
[209:53] Speaker 4: And instead of going into the details of my game, I would like to share my opinions about the game in gaming development and AI and how AI can play a role in it.
[210:08] Speaker 4: Game development has been uh historically difficult because it requires expertise in so many different creative uh fields, from music, sound effects, uh 3D modeling and art.
[210:30] Speaker 4: just to name a few and it'd be really difficult for a small indie team to pull all these off by themselves.
[210:40] Speaker 4: But I think that with the recent development in AI tooling, these barriers are becoming lower and lower and eventually I think that almost anyone will be able to turn their ideas into reality.
[210:58] Speaker 4: And I think that uh on like contrary to a lot of people's opinions, I rather than AI hindering uh human creativity, I think that AI will actually enable human creativity because it makes it so much easier for anyone just to express their ideas and that's kind of like my spicy POV, but um I think that we should take advantage of this going forward and be as creative as we can.
[211:44] Speaker 4: Thank you.
[211:45 - 211:48] (Applause)
[211:48 - 212:00] (Silence)
[212:00] Speaker 2: All right, next up we have U and Lamar with their breakthrough coach.
[212:12] Speaker 2: It's a coach that can lead you through personal development and walk you through it using video chat.
[212:27 - 214:46] (Silence)
[214:46] Speaker 5: Okay.
[214:47] Speaker 5: Oh, okay, I got you.
[214:48] Speaker 5: I got you now.
[214:49 - 215:23] (Silence)
[215:23] Speaker 5: It's all the way.
[215:25] Speaker 5: Good?
[215:26] Speaker 5: All right, cool.
[215:28 - 215:37] (Silence)
[215:37 - 215:50] (Music)
[215:50 - 215:56] (Silence)
[215:56 - 216:03] (Music)
[216:03] Speaker 5: All right, it works.
[216:04] Speaker 5: Good.
[216:07] Speaker 6: All right, let's go.
[216:09] Speaker 6: Um, so basically, um everyone here, we all use chat GPT, Claude and other AI chat tools to increase our productivity.
[216:26] Speaker 6: So I did a survey of my friends to ask them, my non-technical friends, to ask them how they use chat GPT.
[216:37] Speaker 6: Because I was curious what they're using it for?
[216:40] Speaker 6: Are you using it to be more productive at work?
[216:44] Speaker 6: Are you using it to, um, shop for groceries or find out things that you should do in life?
[216:57] Speaker 6: And what I found out is a high percentage of my friends were using AI to develop themselves.
[217:07] Speaker 6: They're having chats about relationships, chats about boosting their confidence, chats about eliminating fear and different things like that.
[217:20] Speaker 6: So we decided to create the breakthrough coach.
[217:26] Speaker 6: So the technology to create this has only been uh available for a few weeks.
[217:36] Speaker 6: So as soon as we saw that te

--- Segment 22 (Starting at 210:00) ---

[210:03] Speaker 1: technology, we began creating the breakthrough coach and we created the breakthrough coach and we actually put it on this robot.
[210:19] Speaker 1: Um, so someone could like come up to the robot and chat with the robot, um, with the breakthrough coach.
[210:29] Speaker 1: So, Uzair will tell you guys how he created, uh, how he hacked this robot, because it was really hard to do that, to make that work.
[210:40] Speaker 2: All right, so uh, with the advent of vision LLMs and speech to text, right now if there's any like COVID era telepresence equipment, you can repurpose it to chat with AI.
[211:02] Speaker 2: Um, with the camera, the LLM can see whatever is around it, with audio to speech, that's like a natural interface, and then any other device it has like wheels or hands or whatever can be two calls.
[211:22] Speaker 2: And so the idea of having the breakthrough coach on this was that you can just stand and talk more naturally to it.
[211:30] Speaker 2: It can see you, it can also comment on your posture, if your like body language is good, on top of like you what you're saying.
[211:48] Speaker 1: All right, perfect.
[211:49] Speaker 1: So, basically we did a confidence uh, breakthrough coach session.
[211:58] Speaker 1: Um, so we did a so for this demo, we actually recorded the demo of a session so that it wouldn't take so long so you get the point.
[212:09] Speaker 1: And also because like it's really sensitive to sound because it's um, made for one on one communication.
[212:18] Speaker 1: Um, so let's um read that.
[212:26] Speaker 1: And then um, so cool, so this is an example of a conversation.
[212:36] Speaker 3: Uh, I think the main uh challenge that I'm trying to tackle is I'm doing a presentation.
[212:42] Speaker 3: Um, and I want to make sure that I'm super confident um, during that presentation.
[212:57] Speaker 4: I notice you have a great energy about you and your light green hoodie really makes you stand out.
[213:04] Speaker 4: Now, let's talk about your presentation.
[213:09] Speaker 4: What's the main topic you'll be discussing?
[213:12] Speaker 4: And what specifically makes you feel less confident about it?
[213:17] Speaker 3: Uh, I feel confident.
[213:20] Speaker 3: If you maybe if you could just give me like one tip of how to be confident during the presentation, uh, that'll be great.
[213:31] Speaker 4: One tip that can greatly help is to focus on your message rather than your self-image.
[213:37] Speaker 4: Remember, you're speaking to share information, not to impress with your speaking skills.
[213:44] Speaker 4: What do you think is the most important point you want your audience to take away from your presentation?
[213:51] Speaker 3: Uh, I just want them to see how cool the technology is.
[213:56] Speaker 3: And basically, I think you helped me out really good and um, I think that's perfect.
[214:00] Speaker 3: So thank you.
[214:05] Speaker 4: That's great to hear that I could help.
[214:08] Speaker 4: And I'm glad you're excited about the technology.
[214:14] Speaker 4: To summarize, we discuss building confidence for your presentation and I offered a tip to focus on your message.
[214:25] Speaker 4: As a next step, practice your presentation with that mindset and you'll see a boost in your confidence.
[214:31] Speaker 4: Keep pushing forward.
[214:32] Speaker 4: You've got this and I'm rooting for your success.
[214:36] Speaker 3: All right, thank you.
[214:38] Speaker 1: All right, cool.
[214:39] Speaker 1: So, this breakthrough coach is um, so the technology that we use to do that, um, that that agent can actually see me.
[214:53] Speaker 1: And the first thing it did was comment.
[214:56] Speaker 1: I'm wearing a light green uh, hoodie.
[214:59] Speaker 1: And so the the first thing it did was give me a comment.
[215:02] Speaker 1: says my energy was good and it liked that my light light green hoodie.
[215:09] Speaker 1: So it's more realistic, the avatar looks realistic and it responds in real time.
[215:15] Speaker 1: So it gives people a better interface to chat with rather than just using uh, a chat box.
[215:23] Speaker 1: And so, that's our project and if you guys want to contact us, there we go.
[215:43 - 215:49] (Silence)
[215:49] Speaker 1: Good job.
[216:09] Speaker 5: Now, what if we could invest in virality and videos that creators make?
[216:18] Speaker 5: What if you could support your favorite creators, so they can make more videos?
[216:26] Speaker 5: That's what the next app is trying to do.
[216:30] Speaker 5: This is Hype by Christian Cataneo.
[216:34 - 217:07] (Silence)
[217:12] Speaker 6: Testing.
[217:24 - 217:27] (Silence)
[217:27] Speaker 6: Beautiful.
[217:30] Speaker 6: Can everyone hear me?
[217:33] Speaker 7: Yes.
[217:34] Speaker 6: Amazing.
[217:39] Speaker 6: We made it this far.
[217:41] Speaker 6: Thank you everyone for coming.
[217:46] Speaker 6: Let's give ourselves a round of applause.
[217:56] Speaker 6: So today I will be presenting hype.
[218:05] Speaker 6: For a very long time since basically getting addicted to social media, I've wanted the ability to capitalize on my knowledge of social trends.
[218:26] Speaker 6: And it's been immensely frustrating not being able to instantly invest in a viral video or a creator that I think is going to blow up.
[218:43] Speaker 6: And after speaking with my friends, I learned that this is actually something that other young people are interested in and it also applies to music and sports.
[219:03] Speaker 6: And so, I present to you Hype, which is a way to invest in posts that generate money by promoting products and services for brands.
[219:22] Speaker 6: So that is the basis of the revenue.
[219:29] Speaker 6: And the idea is that you can scroll and micro invest instead of liking.
[219:41] Speaker 6: Now, before Gauntlet, I didn't have any mobile app experience and so this project felt like uh, an immense kind of obstacle ahead of me and I didn't know where to begin, but I was able to build this out in the past two weeks.
[220:00] Speaker 6: And it's got some kinks left before it can be, you know, on the app store or whatever.
[220:05] Speaker 6: But you can invest and you can short and even the comments are investible.
[220:19] Speaker 6: And so you can also invest in creators because you can represent accounts as ETFs of the monetized content.
[220:40] Speaker 6: And then when you invest in an account, you get shares of all of their future posts.
[220:47] Speaker 6: So this way you can track their success.
[220:50] Speaker 6: Now, personally, I've found YouTubers and TikTokers and Instagram, um, influencers or whatever, um, when they had a very low follower count.
[221:02] Speaker 6: And so this is a way that we can capitalize on our knowledge of social trends.
[221:11] Speaker 6: And there's different ways that revenue can be generated, whether creators are making an individual post to promote something or they're replying and getting filtered by follower accounts and maybe they have partnerships with brands.
[221:39] Speaker 6: And so any content that they make is uh, actually monetizable and they're getting paid.
[221:46] Speaker 6: And this way, users, consumers, investors can uh, capitalize off of the growth of emerging creators.
[221:58] Speaker 6: And there's a social status side of it, where since you can rank top creators, um, already successful influencers might be drawn to the application.
[222:13] Speaker 6: And you can also track who knows the most about social trends and maybe copy the portfolios of uh, YouTubers, for example, that know who's on the come up or whatever that may be.
[222:44] Speaker 6: And you can see your uh, your investments here.
[222:48] Speaker 6: You can also see who your tribe is, which are the people that you invest in that also invest in you.
[223:01] Speaker 6: Um, and yeah, it's basically a combination of Robin Hood and Instagram.
[223:11] Speaker 6: So thank you so much for listening and um, Gauntlet helped me create this, so thank you.
[223:20 - 223:30] (Silence)
[223:30] Speaker 5: Now, if any of you are like me, you've thought of a trade that you want to do and you just wish you could just sort of make the transaction right away and make the trade.
[223:43] Speaker 5: Well, K just built that and it's called Yolo.
[223:47] Speaker 5: This is K with his app for automatic agent-based trading.
[223:48 - 224:00] (Silence)
[224:00] Speaker 8: Alright, K.
[224:02] Speaker 8: Oh, used that one.
[224:03 - 226:12] (Silence)
[226:12] Speaker 9: All right.
[226:14] Speaker 9: Hey everyone, my name is K and I have a question for you.
[226:19] Speaker 9: Have you ever had the idea that something is definitely going to happen in the markets, but you did not know how to profit off that idea?
[226:28] Speaker 9: You got to take a position, long, shorts, you got to buy an option, calls, puts, delta,

--- Segment 23 (Starting at 220:00) ---

[220:00] Speaker 1: all does, yuck.
[220:02] Speaker 1: Now, in the age of AI, you don't have to do any of that.
[220:09] Speaker 1: Introducing YOLO, your AI investment buddy.
[220:15] Speaker 1: With YOLO, you tell the AI what you think is going to happen in natural language.
[220:23] Speaker 1: It will go through all the information I'm going to describe in a minute, find the optimal trade for you, and then execute it on your behalf.
[220:35] Speaker 1: So here is the app itself, where you can see the portfolio, all the positions I'm currently in.
[220:45] Speaker 1: And let's say I think for some reason or other that inflation is going to be really high in the next quarter.
[220:55] Speaker 1: I only tell the AI that inflation is going to be high and it's going to do the rest.
[221:07 - 221:12] (Silence)
[221:12] Speaker 1: Okay, one sec.
[221:16 - 221:33] (Silence)
[221:33] Speaker 1: headphones.
[221:37] Speaker 1: it doesn't work.
[221:39 - 221:48] (Silence)
[221:48] Speaker 1: Oh, yeah.
[221:51 - 222:00] (Silence)
[222:00] Speaker 1: I think inflation is going to be higher.
[222:04] Speaker 1: Higher inflation could lead to increased interest rates.
[222:12] Speaker 1: I recommend selling government bonds or investing in commodities or inflation protected securities.
[222:21] Speaker 1: A confirmation dialogue will appear for the recommended trade.
[222:28] Speaker 1: Analyzing now.
[222:30] Speaker 1: Please confirm the trade to buy inflation protected securities.
[222:40] Speaker 1: Trade executed.
[222:44 - 222:51] (Silence)
[222:51] Speaker 1: And that is literally it.
[222:54] Speaker 1: You tell the AI what you think is going to happen and the AI will automatically prepare the trade for you and then execute it.
[223:01] Speaker 1: So what is going on in the background?
[223:03] Speaker 1: What is the ontology of the app if you will?
[223:07] Speaker 1: Well, for one thing, when you talk in natural language to AI, it will use regular expressions to find what you want.
[223:16] Speaker 1: And then it will go through OpenBB, which is an open source Bloomberg terminal.
[223:21] Speaker 1: It will go through Yahoo News and other financial APIs and then it will use the Alpaca API to actually execute the trade for you.
[223:31] Speaker 1: So if we continue, you will see right now that we are actually in a long position on TIP, which is inflation protected securities.
[223:44] Speaker 1: And if we go to Alpaca and refresh, we will see that it has automatically executed the trade and taken a position on TIP.
[223:56] Speaker 1: Okay, so that's good enough.
[223:59] Speaker 1: But let's say we do not know what's going to happen in the market and we want a recommendation from the AI itself.
[224:06] Speaker 1: Well, the AI will go through the information that I just described again and then it will recommend a stock or a position for you to be in.
[224:15] Speaker 1: So in this case, the AI is recommending for us to buy Apple and you can swipe literally like Tinder, whether you want to be in a position or not.
[224:26] Speaker 1: So, in this case, we're not going to be in Apple and Tesla it says hold, I don't know why.
[224:35] Speaker 1: But we are going to actually buy Tesla.
[224:41] Speaker 1: Okay, that's good enough.
[224:44] Speaker 1: But let's say I wanted to do something even more complex.
[224:47] Speaker 1: Let's say I want to use a trading platform that does not have an API or I want some financial research to be done and I want a report.
[224:57] Speaker 1: Well, in this case, we've given YOLO its own virtual machine running Ubuntu and we've used Open AI computer use agent to execute whatever you like in a fully functional computer.
[225:15] Speaker 1: So in this case, we're just simply telling it to click on Firefox, enter Alpaca markets and retrieve the state of the market and prepare a financial report.
[225:26] Speaker 1: The AI will automatically do that autonomously, you don't have to sit through it and then it will come up with a report and then you can take a position in that trade.
[225:36] Speaker 1: Thank you.
[225:39 - 225:51] (Applause)
[225:52 - 226:14] (Silence)
[226:14] Speaker 2: Now, all of us love consuming content.
[226:19] Speaker 2: But what if AI could curate the best content and make it available to you on demand?
[226:28] Speaker 2: That's exactly what MineSurf is doing.
[226:31] Speaker 2: This is Silas with MineSurf.
[226:34 - 226:38] (Applause)
[226:39 - 226:43] (Silence)
[226:44 - 226:57] (Silence)
[226:57] Speaker 3: All right, test test.
[226:59] Speaker 3: Can you hear me?
[227:01] Speaker 3: All right.
[227:03] Speaker 3: So I'm here to talk to you about MineSurf today.
[227:06] Speaker 3: My name is Saterry, the creator of MineSurf.
[227:10] Speaker 3: And so let's get down to the problem.
[227:12] Speaker 3: Let's think about how does the internet exist right now today.
[227:17] Speaker 3: And it exists in these silos on each platform across the internet.
[227:23] Speaker 3: And so there's this massive amount of information that we can access, but it's all stiled across different platforms and each company has their own platform that they, you know, host their information on, right?
[227:37] Speaker 3: So MindSurf hopes to fix this by being able to curate from a large a large group of uh different platforms where the information already exists and to curate it in one place in an infinite scrolling feed that's addictive and really easy to access the information that you want to find, right?
[227:59] Speaker 3: So the way it works is you give it just a tidbit of information.
[228:03] Speaker 3: It could be a tweet, a stock ticker, an article or just a simple thought.
[228:10] Speaker 3: MindSurf creates an infinite list of supplemental information based on that idea, right?
[228:16] Speaker 3: And so you can explore that infinite feed based on just your original idea.
[228:22] Speaker 3: So the way the easiest way to think about it is perplexity plus X, right?
[228:27] Speaker 3: Perplexity is really good at giving you granular detailed information that is closely related to your initial query.
[228:36] Speaker 3: And X is really good because it contains a very broad depth of topics, right?
[228:42] Speaker 3: But neither one of them fully allow you to investigate ideas the way that you want them to, right?
[228:53] Speaker 3: So MindServe hopes to fix this by providing both the breadth and the depth of information on a topic.
[229:01] Speaker 3: So how's it work?
[229:02] Speaker 3: It creates an agent in the background that curates the feed and the agent has access to a subset of tools, whether it's Wikipedia, Archive, Medpub, scientific journals, Reddit, social media posts, substack, Twitter.
[229:22] Speaker 3: And so it find the agent decides in the background what to append to the feed based on your feedback, what it knows about you and what information it's already shown you.
[229:35] Speaker 3: So we can hop into a quick demo of the UI really quick.
[229:38] Speaker 3: And so the entire idea of the UI was that it's this infinitely scrolling feed where you can just swipe, right?
[229:46] Speaker 3: But I've also built it so that it allows power users to be able to navigate the application quickly.
[229:53] Speaker 3: And so everything can be navigated from the arrow keys.
[229:56] Speaker 3: If you want to jump in and see the entire text of a white paper, you hit right click, you know, right arrow key, right?
[230:04] Speaker 3: You hit down on the arrow key, you're back in the feed.
[230:07] Speaker 3: If you want to go to say settings, we can we can change the way that things look, change our, you know, styling and everything, but everything can be navigated from the arrow keys.
[230:17] Speaker 3: So by hitting left on the arrow key, you can pin this, you can copy the contents and we have a number of export options that we hope to integrate based on that.
[230:27] Speaker 3: Along with that, we can also upvote, downvote or regenerate, which gives the agent information that it needs to be able to curate a better feed for you, right?
[230:39] Speaker 3: And so really, MineServe is just a wrapper around an agent that curates the feed for you.
[230:47] Speaker 3: And so it becomes stronger with the tools that we give it, whether it's a second brain like notion or webflowy or obsidian or just your notes app, or whether it's, you know, we want to access Medpub or nature or science direct.
[231:03] Speaker 3: We can curate social feeds if we give it social tools.
[231:06] Speaker 3: And so it really allows this infinitely expandable list of information to be curated just based on what tools we provide it and we can select what preferences we want, whether it's more social based or more technical, right?
[231:21] Speaker 3: And so we have the ability to expand this and constantly refine and tailor what we're showing the user based on what tools we provide the agent.
[231:33] Speaker 3: So, I would say ride the swell, discover MineServe, start with a basic piece of information, just a tidbit and then find this infinite list of information that you can learn from.
[231:47] Speaker 3: So that's it.
[231:48 - 231:52] (Applause)
[231:53 - 231:58] (Silence)
[231:58] Speaker 4: Now, when we're deciding what to watch, most of the time we don't know what to pick.
[232:06] Speaker 4: But what if we could use AI to understand our preferences and psychology and let it decide what to watch.
[232:16] Speaker 4: That's exactly what Tennyson built and it's called any sage.
[232:21 - 232:36] (Silence)

--- Segment 24 (Starting at 230:00) ---

[230:01 - 231:47] (Silence)
[231:47] Speaker 1: Test test, can you hear me?
[231:50] Speaker 1: I don't know. I'm I'm just talking into it. Can anybody hear me out there?
[231:55] Speaker 1: All right, good.
[231:56] Speaker 1: Good.
[231:56] Speaker 1: So, uh, let's get started. My name is Tennyson. I created Annie Sage and uh, I'm just here to present it. So friends, uh Romans, countrymen, lend me your dollars. Uh let's go. This is your personalized anime recommendation engine.
[232:30] Speaker 1: So the problem that we're trying to solve immediately, what to watch?
[232:36] Speaker 1: There's literally a million things out there to watch, uh, anything, even if you narrow it down to just anime, even if you narrow it down to just one streaming service, you're going to have a problem with trying to find what to watch.
[232:54] Speaker 1: I call it analysis paralysis, you call it the paradox of choice, you're going to end up spending more time searching for things to watch than you're actually going to be watching things. Uh, so what do we do about this problem?
[233:12] Speaker 1: Solution, Annie Sage.
[233:17] Speaker 1: Now, steps one, two, three, answer some questions, build your profile, get recommendations.
[233:27] Speaker 1: This is the simple way to go.
[233:30] Speaker 1: So let's actually take a look at it.
[233:45] Speaker 1: So this is going to be pretty quick.
[233:47] Speaker 1: So you see, we're going through some questions here, we're going to be picking things that are oblique, not just asking you like, hey, what genre do you want to watch, or what kind of things have you seen before?
[233:58] Speaker 1: It's really just going to go on build a profile based on questions that you answer.
[234:06] Speaker 1: So just straight psychological things here.
[234:13] Speaker 1: So it's getting to the end, that was the 10th one and you can see coming up with these really nicely uh done images, posters of everything that you might want to watch that we've figured out.
[234:28] Speaker 1: So good psychological component and then finally a good little component.
[234:32] Speaker 1: You can see the profile there very quickly.
[234:38] Speaker 1: And of course, you can see on the end here, you can watch the trailer, which we're not going to put in because you get a copyright strike.
[234:53] Speaker 1: So, some key benefits here.
[234:58] Speaker 1: Uh does, as I was saying, psychological profiling without direct questions.
[235:05] Speaker 1: Uh, it will do advanced multi-dimensional recommendation, so it's actually building quite a few things with each one of these uh question steps here.
[235:14] Speaker 1: There's a lot of places this could possibly go.
[235:20] Speaker 1: Uh, there's comprehensive data integration.
[235:23] Speaker 1: I've put together lots of different uh APIs here.
[235:27] Speaker 1: So from My Anime List and Andy List, the movie database, uh the YouTube recommendations here, building all that, putting it together.
[235:41] Speaker 1: And you can find your next favorite anime here.
[235:46] Speaker 1: This is just the beginning of the Sage system.
[235:50] Speaker 1: So we've decided to go with Annie Sage as the very first thing.
[235:56] Speaker 1: Start your journey, experience smarter discovery and join the future.
[236:02] Speaker 1: Thank you.
[236:05 - 236:43] (Silence)
[236:43] Speaker 2: All right, for the next app, the next app is changing how healthcare brands talk with their customers.
[236:54] Speaker 2: The Corbit engine listens, speaks and understands each of its customers with precision.
[237:00] Speaker 2: This is Corbit with Yavuz.
[237:05 - 238:19] (Silence)
[238:19] Speaker 3: All right.
[238:20 - 238:22] (Silence)
[238:22] Speaker 3: All right, Corbit.
[238:25] Speaker 3: So 6.3 trillion uh is spent annually on health and wellness products and personalization is still missing.
[238:45] Speaker 3: It's uh based on guesswork and yeah, messaging is generic and superficial.
[238:57] Speaker 3: So the problem is targeting messages messaging is hard.
[239:03] Speaker 3: Brands rely on big buckets like skin type, age, total spend and it doesn't work.
[239:16] Speaker 3: Customers feel ignored and brands lose loyalty.
[239:25] Speaker 3: So, what if we could fix this?
[239:28] Speaker 3: What if there was a way to give the correct messaging to every customer, every time?
[239:36] Speaker 3: And that's why we're building Corbit engine and so here is how it works.
[239:52] Speaker 3: So this is on an e-commerce site of a brand X. And you put in your email address, you press this and you give a selfie.
[240:46] Speaker 3: And then the engine analyzes my skin, not just dry, like oily, but dives more in depth like visual clues like acnes, wrinkles, redness, those kind of stuff to do a deep analysis also on the location and the environment. And so under the roof we use data from sales, reviews, clinical studies, social media, competitors to create potential customer profiles for each product.
[241:33] Speaker 3: And now I get a detailed profile created for me.
[241:46] Speaker 3: Right here, conditions and location info. And I get recommended products.
[242:04] Speaker 3: I can also click here to schedule a visit with a brand partner and their input will improve our models. And this is the first step and the vision is to build a system that creates a year's marketing on email, SMS, website, crafted scheduled and updated real time for every single customer in minutes.
[242:55] Speaker 3: So imagine this brand is sending me an SMS about a moisturizer, not because it's on sale, but because uh because they see a need in my skin and it's timely like based on the local weather here in Austin right now.
[243:25] Speaker 3: So yeah, we're kicking this off with medical grade skin care brands and it's a highly growing market that craves this.
[243:38] Speaker 3: And even a slight uptick can optic in their lifetime value can make a huge difference.
[243:47] Speaker 3: And this is just the start.
[243:49] Speaker 3: Think cosmetics, pharmaceuticals, nutrition brands, the entire health and wellness market.
[244:02] Speaker 3: And yeah, this is not a marketing tool, we're changing how brands communicate with their customers.
[244:11] Speaker 3: So that's Corbit.
[244:12] Speaker 3: Thank you.
[244:15 - 245:01] (Silence)
[245:01] Speaker 2: Now, all of us have heard of vibe coding, but have we heard of Stark coding?
[245:10] Speaker 2: The next group is Rishab and Paul with special guest Uer and they're going to tell you all about enterprise grade Stark coding.

--- Segment 25 (Starting at 240:00) ---

[240:00 - 240:08] (Silence)
[240:08] Speaker 1: 
HDMI?
[240:14] Speaker 2: Oh yeah, that's HDMI.
[240:26] Speaker 3: Hi guys. My name is Rishab Kumar, and he's my friend Paul, and together we present to you Stark coding.
[240:55] Speaker 2: right here.
[240:56] Speaker 1: Audio I know
[240:57 - 241:01] (Silence)
[241:01] Speaker 4: Headphone headphones?
[241:04] Speaker 5: yeah
[241:05 - 241:08] (Silence)
[241:08] Speaker 6: and up. Ash, help me. You're our only hope.
[241:17] Speaker 6: No, it's.
[241:21] Speaker 7: on the headphones.
[241:22] Speaker 6: Good.
[241:27] Speaker 7: back to the audio device.
[241:36] Speaker 7: Try this.
[241:38] Speaker 6: Black?
[241:41] Speaker 7: Not.
[241:44] Speaker 6: It says Oculus.
[241:50 - 241:52] (Silence)
[241:52] Speaker 6: Okay.
[241:52] Speaker 7: Yeah.
[242:00] Speaker 6: Sorry.
[242:02] Speaker 7: Move your hand.
[242:15] Speaker 6: try.
[242:21] Speaker 7: It's not
[242:25] Speaker 6: Move your hand, please.
[242:30] Speaker 7: There it is.
[242:31] Speaker 6: Okay.
[242:38] Speaker 7: Ready?
[242:43] Speaker 8: Vive coding, everyone knows it. Everyone loves it.
[242:51] Speaker 9: Vive coding gets a bad rap for inefficiency. Slowly writing bad prompts then walking away from the computer while it generates garbage.
[243:04] Speaker 3: We know better than that. We use more than one coding agent at a time. By leveraging voice and hand gestures, we can rapidly speed up the coding process. No keyboard or mouse required. Virtual monitors with a VR headset makes it easy to monitor multitasking. AI is known to generate slop. It's an intern. Never blindly accept your intern's pull request. Reviewing code doesn't have to be purely manual.
Where rules files fall short, dedicated agents step in.
[243:54] Speaker 8: We have created multiple code review agents that analyze code bases for architecture and clean code patterns. These agents review every file for compliance and has been tested on projects with over a million lines of code.
[244:13] Speaker 3: Voice assistance are wonderful for brainstorming sessions. Even Tony Stark had Jarvis. A voice assistant is also a great entry point into calling a team of agents. No one likes tedious work, so why not have agents do the repeatable task? Hey Jarvis, I have an idea for a new project called Stark Coding. Can you check to see if the domain is available?
[244:40] Speaker 10: Yes, it appears the domain is available.
[244:42] Speaker 3: Okay, register it for me.
[244:45] Speaker 10: Right away, sir.
[244:47] Speaker 3: Let's make it a simple static website using HTML, CSS and TypeScript. I expect very little traffic, but make sure it can handle spikes after it goes viral in the USA. Make a new repo for it, set up ISC, use my normal GitHub bot template and get servers set up.
[245:11] Speaker 10: Yes, sir. I shall inform the DevOps engineer and cloud solution architect right away. Would you also like me to have the UX designer to create a placeholder page?
[245:21] Speaker 8: We have an army of agents, 100% coded and deployed by AI.
[245:27] Speaker 3: If your vive code isn't ready for production, sounds like a skill issue. Join Gauntlet AI to vibe with the best.
[245:41 - 245:42] (Silence)
[245:42] Speaker 8: Thank you
[245:42] Speaker 9: So this entire gauntlet, I have found ways to procrastinate doing my homework by doing probably 20 times as much work, creating agents to do my homework for me. Stark coding is trying to get it to a Microsoft level enterprise where I am can't tell a difference between, did I code it? Did I set up this project or did it?
So, my actual standard was sending all most of my projects snippets to my old coworkers and saying, hey, one of these was written by me, one is AI, which one is it? And they couldn't tell the difference.
[246:41] Speaker 10: I worked at Microsoft.
[246:48] Speaker 9: Thank you.
[247:10] Speaker 11: All right. Last but not least, the next presenter is creating Google for your computer. This is Peter with Phathom.
[247:32] Speaker 12: All right. Do both of these mics work? Okay. Let's talk about your computer. If you're like me, the first thing you probably open is a web browser because so much of our lives happens online, right? Whether it's email or streaming or Twitter, flamewars. But at the same time hard drives are getting bigger. Like Apple will happily sell you a 1, 2, 4, 8 terabyte hard drive. So people are definitely using it for something, right? They're not cheap, but people are using it. And it turns out that actually a lot of our lives still happens offline, still happens on our computer, whether you're a privacy advocate or work with sensitive information in industry or, you know, government intelligence or where you just have to be off like sometimes completely, you know, airgapped from the world. So a lot of our lives still happens in the computer.
But with that scale, like our ways of searching it is still pretty primitive. It hasn't really changed for the past 25 or 30 years. You say you're looking for cats, you have to type C and A and T and hope that you had everything everything you have actually has that exact string in it. It's very brittle. You can't talk about leopards or Jaguars or Puma, right? So, it's very narrow. It's like ironically, our local computer is like the last frontier. It's sitting right in front of us, but we can't really search it, right? And AI kind of makes this easier. Open AI just came out with a file search API that lets you kind of um do not just string matching but semantic search, right? What what's the meaning inside the text?
Um but it's still early. It costs like a 10 cents per gig a day and that works if you have a small project, but if you have like say even a 1 terabyte hard drive, that's 3,000 bucks a month. That's more than what I paid for an apartment in San Francisco and that's just storage. That's no embedding, that's no vectorizing, that's no searching. That's just holding the index. So there's an opportunity here. There's how can we leverage that technology for the scale of a local disc. And that's where fandom comes in. The idea is basically just a search engine for your own computer. It's fast because it's local and if you wear a tin foil hat, it's completely offline. So, it's pretty great. I mean, the secret sauce is pretty simple. It's the files are just already on your computer like in the sense that the files are in the computer. You don't have to store it again. It's already stored. So it's it's free. Um but

--- Segment 26 (Starting at 250:00) ---

[250:00 - 250:11] (Silence)
[250:11] Speaker 1: but we still have a size problem.
[250:14] Speaker 1: a terabyte's still a lot and we don't want to index all of that right away, right?
[250:22] Speaker 1: we don't want a table of contents to be as long as the rest of the book, right?
[250:31] Speaker 1: So we have to be smart.
[250:33] Speaker 1: We have to be smart about how we search.
[250:37] Speaker 1: We have to maybe have a a coarse way of covering the whole breath of something before we kind of drill down and that's, you know, when you have software engineers or mathematicians, it that sounds like a hierarchy in terms of searching, right?
[250:53] Speaker 1: And we have tools out of the box for that.
[250:57] Speaker 1: Mathwords, K means clustering, dominant set approximation, math.
[251:03] Speaker 1: Um, but here's kind of a brief dive into it.
[251:08] Speaker 1: You start with your massive amount of original text, whether it's PDFs, documents, uh or even non-text that we can turn into text and kind of cull uh the meaning out of.
[251:18] Speaker 1: And we break it up into a millions of pieces.
[251:22] Speaker 1: Sentence fragments, sentences, paragraphs, and then turn that into a numerical representation through language models.
[251:31] Speaker 1: So basically, into just straight up decimal numbers and that represents the meaning of those fragments.
[251:40] Speaker 1: Now, whereas most of the time we just throw all of that into one massive index and just query against it, we're going to do take one extra step.
[251:49] Speaker 1: We're going to take all those fragments and try to lump them up together based on what they mean, like topic and clump them, make these clusters.
[251:59] Speaker 1: And that's what came in clustering does.
[252:02] Speaker 1: And then in each cluster, we find the one fragment inside that is closest to the middle in semantic space, not physical space, but like that kind of contains the gist that is closest to the rest of that clump.
[252:19] Speaker 1: And instead of vectorizing everything and indexing everything, we index only the little middles.
[252:29] Speaker 1: So, how this works when it comes, you have a question about everything on your computer, you type in a query text and it gets fragmented and vectorized like before, but instead of being mashed against everything on your computer, it matches against the most middles and it finds the best middles.
[252:51] Speaker 1: From those middles, we can just look up the original text because it's already sitting on your computer.
[253:00] Speaker 1: The payoff, 2% index footprint.
[253:06] Speaker 1: So, if you have 10 megabytes, 100 megabytes of uh text, that's two mags of index.
[253:16] Speaker 1: It's zero cost to store because it's your computer.
[253:21] Speaker 1: It's zero cost to embed because the language model lives on your computer.
[253:27] Speaker 1: And it's zero to query for the same reason because it's just your computer.
[253:33] Speaker 1: Thanks.
[253:41 - 253:43] (Applause)
[253:43] Speaker 1: Am I done up?
[253:44] Speaker 2: All right.
[253:47 - 254:06] (Silence)
[254:06] Speaker 3: All right, I wanted to take a second to thank a couple of people who made today's event possible.
[254:14] Speaker 3: Starting with our head of ops, Rebecca Meders.
[254:21 - 254:24] (Applause)
[254:24] Speaker 3: And our other
[254:27] Speaker 3: head of Ops, Alicia DeMatteo.
[254:31 - 254:35] (Applause)
[254:35] Speaker 3: I wanted to thank the AV team, and I want to thank all the challenges for presenting their projects.
[254:43] Speaker 3: Rebecca, a few words.
[254:45 - 254:47] (Applause)
[254:51] Speaker 4: Now, y'all know I don't need a mic, right?
[254:57] Speaker 4: Uh also, I'm not getting on stage, so hopefully everybody can still see me.
[255:01] Speaker 4: Am I can I still be seen?
[255:03] Speaker 4: All right, great.
[255:05] Speaker 4: How do you guys feel?
[255:09 - 255:11] (Audience response)
[255:13] Speaker 4: Hell yeah.
[255:16] Speaker 4: You guys just submitted and it was great.
[255:22] Speaker 4: Y'all did an excellent, excellent job.
[255:25] Speaker 4: I'm so proud.
[255:27] Speaker 4: If you saw my uh X post, since I can't say tweet anymore, earlier today, you know that I feel like this is watching the Eiffel Tower being built from the beam.
[255:42] Speaker 4: Like that is what this feels like and you guys are the architects of all of this.
[255:46] Speaker 4: So I am incredibly proud to be a part of this organization.
[255:51] Speaker 4: Let's do a little bit of housekeeping.
[255:53] Speaker 4: All right.
[255:55] Speaker 4: Gauntlet staff, uh challenges, you guys kind of got marching orders from me, ask the people around you, so I don't have to say that out loud.
[256:04] Speaker 4: For the guests, we do have a great.
[256:07 - 256:12] (Inaudible)
[256:13] Speaker 5: You are so right.
[256:17] Speaker 4: Introducing the Gauntlet Champions.
[256:25 - 256:27] (Applause)
[256:30] Speaker 4: You're absolutely right.
[256:32] Speaker 4: So my champions, you got marching orders earlier today or earlier about 10 minutes ago about what happens next.
[256:41] Speaker 4: For our guests, we are going to serve you a great dinner.
[256:45] Speaker 4: Uh we like you to stay, talk to all of the challenges, just enjoy the celebration with us, but we want to get the tables a little bit more conversational.
[256:57] Speaker 4: So if you guys could just allow us a few minutes before we release dinner so we can do that.
[257:02] Speaker 4: Is everybody cool with that?
[257:06] Speaker 4: All right, now I'm going to put you back in the hands of the one, the only, the greatest MC to ever do it, Ash.
[257:14 - 257:17] (Applause)
[257:20] Speaker 6: So, this is 1,200 hours of work for every single champion back there.
[257:28] Speaker 6: So, I want to take a second to introduce the first graduating class of Gauntlet AI Winter 2025, all of the champions in the back over there.
[257:44] Speaker 6: Let's give it up.
[257:45 - 257:51] (Applause)
[257:56] Speaker 6: Each one of you put in well over 1200 hours of work, the the number of uh 24-hour sessions just coding, the amount of projects, the amount of challenges, you guys got through all of it and you are now the first graduating class.
[258:33] Speaker 6: It's important to acknowledge the ontology of projects that we have here at Gauntlet AI.
[258:35 - 258:37] (Applause)
[258:42] Speaker 6: It doesn't matter if it's gaming, it doesn't matter if it's finance, it doesn't matter if it's coding.
[258:47] Speaker 6: We have built product that is going to transform the way we build product over the next decade.
[258:55] Speaker 6: Each one of you has shown the power of AI and the power of picking up AI right now and using it to transform your workflows.
[259:07] Speaker 6: In the future, each one of you is going to become the individual that is going to take AI to the next level and bring coding its new workflow.
[259:22] Speaker 6: So, once again, I want to thank everybody who joined us today.
[259:26] Speaker 6: Over 15,000 people on X saw our live stream, over 15,000 people saw all of your projects.
[259:40] Speaker 6: If you haven't gone to demoday.gauntletai.com, please go and see all the projects, see all the challenges.
[259:51] Speaker 6: If you want to be a challenger, go to gauntletai.com and click apply now.
[259:59] Speaker 6: And if you want to hire these amazing graduates, click become a hiring partner on Gauntlet AI right now.
[260:14] Speaker 6: Thank you everybody for joining us.
[260:17] Speaker 6: Thank you Austin, thank you Rebecca, thank you Alicia, and thank you to the champions for making this the day that it was.
[260:26] Speaker 6: Thank you.
[260:28 - 260:30] (Applause)
[260:33] Speaker 7: I totally wigged that.
[260:36] Speaker 7: I got the feels like, oh my god, it's so worth it.
[260:39] Speaker 7: Gonna miss Ash, going to miss Rebecca.
[260:42] Speaker 7: My feels is coming in.
[260:44] Speaker 7: I can't handle it.
[260:46] Speaker 8: That was so good.
