

--- Segment 1 (Starting at 00:00) ---

[0:00 - 0:13] (Silence)
[0:13] Speaker 1: I
[0:13 - 0:18] (Silence)
[0:18] Speaker 1: Uh
[0:18 - 0:28] (Silence)
[0:28] Speaker 1: The
[0:28 - 0:40] (Silence)
[0:40] Speaker 1: Mhm.
[0:40 - 0:47] (Silence)
[0:47] Speaker 1: Mhm.
[0:47 - 0:50] (Silence)
[0:50] Speaker 2: Okay.
[0:50 - 0:58] (Silence)
[0:58] Speaker 1: Uh
[0:58 - 1:06] (Silence)
[1:06] Speaker 1: Yeah.
[1:06 - 1:13] (Silence)
[1:13] Speaker 2: I
[1:13] Speaker 1: Mhm.
[1:13 - 1:16] (Silence)
[1:16] Speaker 2: Okay.
[1:16 - 1:18] (Silence)
[1:18] Speaker 1: All
[1:18 - 1:46] (Silence)
[1:46] Speaker 1: Yeah.
[1:46 - 1:54] (Silence)
[1:54] Speaker 1: Yeah.
[1:54 - 2:01] (Silence)
[2:01] Speaker 2: Okay.
[2:01 - 2:09] (Silence)
[2:09] Speaker 1: Mhm.
[2:09 - 2:20] (Silence)
[2:20] Speaker 1: Yeah.
[2:20 - 2:30] (Silence)
[2:30] Speaker 1: Uh
[2:30 - 2:42] (Silence)
[2:42] Speaker 1: Yeah.
[2:42 - 2:49] (Silence)
[2:49] Speaker 1: Mhm.
[2:49 - 2:54] (Silence)
[2:54] Speaker 2: Okay.
[2:54 - 3:03] (Silence)
[3:03] Speaker 2: Okay.
[3:03 - 3:10] (Silence)
[3:10] Speaker 2: So
[3:10 - 3:17] (Silence)
[3:17] Speaker 2: Okay.
[3:17 - 3:20] (Silence)
[3:20] Speaker 1: Yeah.
[3:20 - 3:28] (Silence)
[3:28] Speaker 1: Uh
[3:28 - 3:37] (Silence)
[3:37] Speaker 1: Mhm.
[3:37 - 3:41] (Silence)
[3:41] Speaker 1: Yeah.
[3:41 - 3:53] (Silence)
[3:53] Speaker 2: Okay.
[3:53 - 3:57] (Silence)
[3:57] Speaker 1: Uh
[3:57 - 4:04] (Silence)
[4:04] Speaker 2: Okay.
[4:04 - 4:26] (Silence)
[4:26] Speaker 1: Yeah.
[4:26 - 4:31] (Silence)
[4:31] Speaker 2: Okay.
[4:31 - 4:35] (Silence)
[4:35] Speaker 1: Yeah.
[4:35 - 4:42] (Silence)
[4:42] Speaker 1: Oh.
[4:42 - 4:44] (Silence)
[4:44] Speaker 2: Okay.
[4:44 - 4:53] (Silence)
[4:53] Speaker 1: Yeah.
[4:53 - 4:59] (Silence)
[4:59] Speaker 2: Okay.
[4:59 - 5:08] (Silence)
[5:08] Speaker 1: Uh
[5:08 - 5:16] (Silence)
[5:16] Speaker 2: Okay.
[5:16 - 5:25] (Silence)
[5:25] Speaker 1: Mhmm.
[5:25 - 5:31] (Silence)
[5:31] Speaker 2: Okay.
[5:31 - 5:38] (Silence)
[5:38] Speaker 2: Yep.
[5:38 - 5:43] (Silence)
[5:43] Speaker 1: Yeah.
[5:43 - 5:47] (Silence)
[5:47] Speaker 2: Okay.
[5:47 - 5:55] (Silence)
[5:55] Speaker 1: Yeah.
[5:55 - 6:03] (Silence)
[6:03] Speaker 2: Okay.
[6:03 - 6:13] (Silence)
[6:13] Speaker 1: Mhmm.
[6:13 - 6:23] (Silence)
[6:23] Speaker 1: Yeah.
[6:23 - 6:30] (Silence)
[6:30] Speaker 2: Okay.
[6:30 - 6:39] (Silence)
[6:39] Speaker 1: Yeah.
[6:39 - 6:45] (Silence)
[6:45] Speaker 2: I
[6:45 - 6:48] (Silence)
[6:48] Speaker 2: Okay.
[6:48 - 6:54] (Silence)
[6:54] Speaker 1: Mhm.
[6:54 - 7:01] (Silence)
[7:01] Speaker 2: Okay.
[7:01 - 7:04] (Silence)
[7:04] Speaker 1: Yeah.
[7:04 - 7:11] (Silence)
[7:11] Speaker 2: I
[7:11 - 7:17] (Silence)
[7:17] Speaker 2: Okay.
[7:17 - 7:23] (Silence)
[7:23] Speaker 1: Mhmm.
[7:23 - 7:29] (Silence)
[7:29] Speaker 1: Yeah.
[7:29 - 7:35] (Silence)
[7:35] Speaker 2: I
[7:35 - 7:40] (Silence)
[7:40] Speaker 2: Okay.
[7:40 - 7:50] (Silence)
[7:50] Speaker 1: Mhm.
[7:50 - 7:56] (Silence)
[7:56] Speaker 1: Yeah.
[7:56 - 8:03] (Silence)
[8:03] Speaker 2: Okay.
[8:03 - 8:07] (Silence)
[8:07] Speaker 1: Uh
[8:07 - 8:15] (Silence)
[8:15] Speaker 2: Okay.
[8:15 - 8:23] (Silence)
[8:23] Speaker 1: Mhm.
[8:23 - 8:28] (Silence)
[8:28] Speaker 1: Mhm.
[8:28 - 8:38] (Silence)
[8:38] Speaker 2: Okay.
[8:38 - 8:47] (Silence)
[8:47] Speaker 1: Yeah.
[8:47 - 8:56] (Silence)
[8:56] Speaker 1: Oh.
[8:56 - 9:03] (Silence)
[9:03] Speaker 2: Okay.
[9:03 - 9:10] (Silence)
[9:10] Speaker 1: Yeah.
[9:10 - 9:25] (Silence)
[9:25] Speaker 1: Uh
[9:25 - 9:33] (Silence)
[9:33] Speaker 2: Okay.
[9:33 - 9:40] (Silence)
[9:40] Speaker 1: Mhmm.
[9:40 - 9:48] (Silence)
[9:48] Speaker 2: Okay.
[9:48 - 9:57] (Silence)
[9:57] Speaker 1: Yeah.
[9:57 - 10:04] (Silence)
[10:04] Speaker 2: Okay.
[10:04 - 10:08] (Silence)
[10:08] Speaker 1: Yeah.
[10:08 - 10:14] (Silence)
[10:14] Speaker 2: Okay.
[10:14 - 10:22] (Silence)
[10:22] Speaker 1: Uh
[10:22 - 10:30] (Silence)
[10:30] Speaker 2: Okay.
[10:30 - 10:36] (Silence)
[10:36] Speaker 1: Mhmm.
[10:36 - 10:40] (Silence)
[10:40] Speaker 1: Yeah.
[10:40 - 10:47] (Silence)
[10:47] Speaker 2: Okay.
[10:47 - 10:53] (Silence)
[10:53] Speaker 1: Mhm.
[10:53 - 11:00] (Silence)
[11:00] Speaker 2: Okay.
[11:00 - 11:07] (Silence)
[11:07] Speaker 1: Yeah.
[11:07 - 11:15] (Silence)
[11:15] Speaker 2: Okay.
[11:15 - 11:24] (Silence)
[11:24] Speaker 1: Yeah.
[11:24 - 11:32] (Silence)
[11:32] Speaker 2: Okay.
[11:32 - 11:38] (Silence)
[11:38] Speaker 1: Mhmm.
[11:38 - 11:46] (Silence)
[11:46] Speaker 2: Okay.
[11:46 - 11:55] (Silence)
[11:55] Speaker 1: Yeah.
[11:55 - 12:03] (Silence)
[12:03] Speaker 2: Okay.
[12:03 - 12:06] (Silence)
[12:06] Speaker 1: Yeah.
[12:06 - 12:14] (Silence)
[12:14] Speaker 2: I
[12:14 - 12:20] (Silence)
[12:20] Speaker 2: Okay.
[12:20 - 12:27] (Silence)
[12:27] Speaker 1: Mhm.
[12:27 - 12:32] (Silence)
[12:32] Speaker 1: Yeah.
[12:32 - 12:39] (Silence)
[12:39] Speaker 2: Okay.
[12:39 - 12:44] (Silence)
[12:44] Speaker 1: Uh
[12:44 - 12:53] (Silence)
[12:53] Speaker 2: Okay.
[12:53 - 13:00] (Silence)
[13:00] Speaker 1: Mhm.
[13:00 - 13:06] (Silence)
[13:06] Speaker 1: Yeah.
[13:06 - 13:13] (Silence)
[13:13] Speaker 2: I
[13:13 - 13:18] (Silence)
[13:18] Speaker 2: Okay.
[13:18 - 13:25] (Silence)
[13:25] Speaker 1: Mhm.
[13:25 - 13:31] (Silence)
[13:31] Speaker 2: I
[13:31 - 13:35] (Silence)
[13:35] Speaker 2: Okay.
[13:35 - 13:42] (Silence)
[13:42] Speaker 1: Yeah.
[13:42 - 13:49] (Silence)
[13:49] Speaker 1: Oh.
[13:49 - 13:56] (Silence)
[13:56] Speaker 2: Okay.
[13:56 - 14:02] (Silence)
[14:02] Speaker 1: Yeah.
[14:02 - 14:09] (Silence)
[14:09] Speaker 1: Uh
[14:09 - 14:16] (Silence)
[14:16] Speaker 2: I
[14:16 - 14:21] (Silence)
[14:21] Speaker 2: Okay.
[14:21 - 14:30] (Silence)
[14:30] Speaker 1: Mhm.
[14:30 - 14:35] (Silence)
[14:35] Speaker 2: Okay.
[14:35 - 14:43] (Silence)
[14:43] Speaker 2: I
[14:43 - 14:47] (Silence)
[14:47] Speaker 2: Okay.
[14:47 - 14:53] (Silence)
[14:53] Speaker 1: Mhm.
[14:53 - 14:58] (Silence)
[14:58] Speaker 2: Yep.
[14:58 - 15:01] (Silence)
[15:01] Speaker 1: Mhm.
[15:01 - 15:07] (Silence)
[15:07] Speaker 2: Okay.
[15:07 - 15:12] (Silence)
[15:12] Speaker 1: Yeah.
[15:12 - 15:20] (Silence)
[15:20] Speaker 2: Okay.
[15:20 - 15:28] (Silence)
[15:28] Speaker 1: Uh
[15:28 - 15:35] (Silence)
[15:35] Speaker 2: Okay.
[15:35 - 15:42] (Silence)
[15:42] Speaker 1: Yeah.
[15:42 - 15:47] (Silence)
[15:47] Speaker 2: I
[15:47 - 15:51] (Silence)
[15:51] Speaker 2: Okay.
[15:51 - 15:58] (Silence)
[15:58] Speaker 1: Mhm.
[15:58 - 16:03] (Silence)
[16:03] Speaker 1: Mhm.
[16:03 - 16:09] (Silence)
[16:09] Speaker 2: I
[16:09 - 16:14] (Silence)
[16:14] Speaker 2: Okay.
[16:14 - 16:22] (Silence)
[16:22] Speaker 1: Yeah.
[16:22 - 16:28] (Silence)
[16:28] Speaker 2: I
[16:28 - 16:31] (Silence)
[16:31] Speaker 2: Okay.
[16:31 - 16:38] (Silence)
[16:38] Speaker 1: Yeah.
[16:38 - 16:47] (Silence)
[16:47] Speaker 2: Okay.
[16:47 - 16:54] (Silence)
[16:54] Speaker 1: Yeah.
[16:54 - 17:01] (Silence)
[17:01] Speaker 2: Okay.
[17:01 - 17:05] (Silence)
[17:05] Speaker 1: Mhm.
[17:05 - 17:12] (Silence)
[17:12] Speaker 2: Okay.
[17:12 - 17:20] (Silence)
[17:20] Speaker 1: Mhm.
[17:20 - 17:26] (Silence)
[17:26] Speaker 1: Yeah.
[17:26 - 17:33] (Silence)
[17:33] Speaker 2: Okay.
[17:33 - 17:37] (Silence)
[17:37] Speaker 1: Yeah.
[17:37 - 17:45] (Silence)
[17:45] Speaker 2: Okay.
[17:45 - 17:52] (Silence)
[17:52] Speaker 1: Uh
[17:52 - 17:59] (Silence)
[17:59] Speaker 2: I
[17:59 - 18:03] (Silence)
[18:03] Speaker 2: Okay.
[18:03 - 18:11] (Silence)
[18:11] Speaker 1: Mhmm.
[18:11 - 18:16] (Silence)
[18:16] Speaker 2: Okay.
[18:16 - 18:22] (Silence)
[18:22] Speaker 1: Yeah.
[18:22 - 18:30] (Silence)
[18:30] Speaker 2: I
[18:30 - 18:33] (Silence)
[18:33] Speaker 2: Okay.
[18:33 - 18:39] (Silence)
[18:39] Speaker 1: Mhm.
[18:39 - 18:44] (Silence)
[18:44] Speaker 2: I
[18:44 - 18:47] (Silence)
[18:47] Speaker 2: Okay.
[18:47 - 18:55] (Silence)
[18:55] Speaker 1: Yeah.
[18:55 - 19:02] (Silence)
[19:02] Speaker 2: Okay.
[19:02 - 19:09] (Silence)
[19:09] Speaker 1: Mhmm.
[19:09 - 19:15] (Silence)
[19:15] Speaker 1: Uh
[19:15 - 19:22] (Silence)
[19:22] Speaker 2: I
[19:22 - 19:25] (Silence)
[19:25] Speaker 2: Okay.
[19:25 - 19:32] (Silence)
[19:32] Speaker 1: Mhm.
[19:32 - 19:37] (Silence)
[19:37] Speaker 1: Yeah.
[19:37 - 19:44] (Silence)
[19:44] Speaker 2: Okay.
[19:44 - 19:50] (Silence)
[19:50] Speaker 1: Uh
[19:50 - 19:58] (Silence)
[19:58] Speaker 2: I
[19:58 - 20:01] (Silence)
[20:01] Speaker 2: Okay.
[20:01 - 20:08] (Silence)
[20:08] Speaker 1: Mhm.
[20:08 - 20:14] (Silence)
[20:14] Speaker 1: Yeah.
[20:14 - 20:22] (Silence)
[20:22] Speaker 2: Okay.
[20:22 - 20:30] (Silence)
[20:30] Speaker 1: Uh
[20:30 - 20:38] (Silence)
[20:38] Speaker 2: I
[20:38 - 20:41] (Silence)
[20:41] Speaker 2: Okay.
[20:41 - 20:48] (Silence)
[20:48] Speaker 1: Mhm.
[20:48 - 20:54] (Silence)
[20:54] Speaker 2: Okay.
[20:54 - 21:02] (Silence)
[21:02] Speaker 1: Yeah.
[21:02 - 21:10] (Silence)
[21:10] Speaker 2: Okay.
[21:10 - 21:14] (Silence)
[21:14] Speaker 1: Mhmm.
[21:14 - 21:22] (Silence)
[21:22] Speaker 2: Okay.
[21:22 - 21:30] (Silence)
[21:30] Speaker 1: Yeah.
[21:30 - 21:37] (Silence)
[21:37] Speaker 2: Okay.
[21:37 - 21:41] (Silence)
[21:41] Speaker 1: Yeah.
[21:41 - 21:49] (Silence)
[21:49] Speaker 2: Okay.
[21:49 - 21:56] (Silence)
[21:56] Speaker 1: Uh
[21:56 - 22:03] (Silence)
[22:03] Speaker 2: I
[22:03 - 22:06] (Silence)
[22:06] Speaker 2: Okay.
[22:06 - 22:15] (Silence)
[22:15] Speaker 1: Mhm.
[22:15 - 22:21] (Silence)
[22:21] Speaker 2: Okay.
[22:21 - 22:26] (Silence)
[22:26] Speaker 1: Yeah.
[22:26 - 22:34] (Silence)
[22:34] Speaker 2: Okay.
[22:34 - 22:42] (Silence)
[22:42] Speaker 1: Mhm.
[22:42 - 22:48] (Silence)
[22:48] Speaker 1: Yeah.
[22:48 - 22:56] (Silence)
[22:56] Speaker 2: Okay.
[22:56 - 23:03] (Silence)
[23:03] Speaker 1: Uh
[23:03 - 23:10] (Silence)
[23:10] Speaker 2: Okay.
[23:10 - 23:18] (Silence)
[23:18] Speaker 1: Mhm.
[23:18 - 23:24] (Silence)
[23:24] Speaker 1: Yeah.
[23:24 - 23:30] (Silence)
[23:30] Speaker 2: Okay.
[23:30 - 23:37] (Silence)
[23:37] Speaker 1: Mhm.
[23:37 - 23:44] (Silence)
[23:44] Speaker 2: Okay.
[23:44 - 23:52] (Silence)
[23:52] Speaker 1: Yeah.
[23:52 - 23:58] (Silence)
[23:58] Speaker 2: Okay.
[23:58 - 24:05] (Silence)
[24:05] Speaker 1: Yeah.
[24:05 - 24:13] (Silence)
[24:13] Speaker 2: Okay.
[24:13 - 24:17] (Silence)
[24:17] Speaker 1: Uh
[24:17 - 24:26] (Silence)
[24:26] Speaker 2: I
[24:26 - 24:29] (Silence)
[24:29] Speaker 2: Okay.
[24:29 - 24:36] (Silence)
[24:36] Speaker 1: Mhm.
[24:36 - 24:43] (Silence)
[24:43] Speaker 2: Okay.
[24:43 - 24:49] (Silence)
[24:49] Speaker 1: Mhm.
[24:49 - 24:57] (Silence)
[24:57] Speaker 2: Okay.
[24:57 - 25:04] (Silence)
[25:04] Speaker 1: Yeah.
[25:04 - 25:12] (Silence)
[25:12] Speaker 2: Okay.
[25:12 - 25:19] (Silence)
[25:19] Speaker 1: Uh
[25:19 - 25:26] (Silence)
[25:26] Speaker 2: Okay.
[25:26 - 25:34] (Silence)
[25:34] Speaker 1: Mhm.
[25:34 - 25:39] (Silence)
[25:39] Speaker 2: Okay.
[25:39 - 25:47] (Silence)
[25:47] Speaker 1: Mhm.
[25:47 - 25:53] (Silence)
[25:53] Speaker 2: I
[25:53 - 25:57] (Silence)
[25:57] Speaker 2: Okay.
[25:57 - 26:04] (Silence)
[26:04] Speaker 1: Yeah.
[26:04 - 26:12] (Silence)
[26:12] Speaker 2: Okay.
[26:12 - 26:20] (Silence)
[26:20] Speaker 1: Mhm.
[26:20 - 26:26] (Silence)
[26:26] Speaker 1: Yeah.
[26:26 - 26:34] (Silence)
[26:34] Speaker 2: Okay.
[26:34 - 26:39] (Silence)
[26:39] Speaker 1: Uh
[26:39 - 26:47] (Silence)
[26:47] Speaker 2: I
[26:47 - 26:50] (Silence)
[26:50] Speaker 2: Okay.
[26:50 - 26:58] (Silence)
[26:58] Speaker 1: Mhm.
[26:58 - 27:03] (Silence)
[27:03] Speaker 2: Okay.
[27:03 - 27:10] (Silence)
[27:10] Speaker 1: Yeah.
[27:10 - 27:18] (Silence)
[27:18] Speaker 2: Okay.
[27:18 - 27:25] (Silence)
[27:25] Speaker 1: Mhm.
[27:25 - 27:33] (Silence)
[27:33] Speaker 2: Okay.
[27:33 - 27:39] (Silence)
[27:39] Speaker 1: Yeah.
[27:39 - 27:47] (Silence)
[27:47] Speaker 2: Okay.
[27:47 - 27:54] (Silence)
[27:54] Speaker 1: Mhm.
[27:54 - 28:02] (Silence)
[28:02] Speaker 2: Okay.
[28:02 - 28:08] (Silence)
[28:08] Speaker 1: Yeah.
[28:08 - 28:16] (Silence)
[28:16] Speaker 2: Okay.
[28:16 - 28:25] (Silence)
[28:25] Speaker 1: Uh
[28:25 - 28:32] (Silence)
[28:32] Speaker 2: I
[28:32 - 28:35] (Silence)
[28:35] Speaker 2: Okay.
[28:35 - 28:42] (Silence)
[28:42] Speaker 1: Mhm.
[28:42 - 28:49] (Silence)
[28:49] Speaker 2: Okay.
[28:49 - 28:57] (Silence)
[28:57] Speaker 1: Mhm.
[28:57 - 29:02] (Silence)
[29:02] Speaker 1: Yeah.
[29:02 - 29:09] (Silence)
[29:09] Speaker 2: Okay.
[29:09 - 29:16] (Silence)
[29:16] Speaker 1: Yeah.
[29:16 - 29:24] (Silence)
[29:24] Speaker 2: I
[29:24 - 29:27] (Silence)
[29:27] Speaker 2: Okay.
[29:27 - 29:34] (Silence)
[29:34] Speaker 1: Mhm.
[29:34 - 29:41] (Silence)
[29:41] Speaker 1: Yeah.
[29:41 - 29:48] (Silence)
[29:48] Speaker 2: I
[29:48 - 29:51] (Silence)
[29:51] Speaker 2: Okay.
[29:51 - 29:58] (Silence)
[29:58] Speaker 1: Mhm.
[29:58 - 30:05] (Silence)
[30:05] Speaker 2: Okay.
[30:05 -

--- Segment 2 (Starting at 10:00) ---

[10:00 - 10:05] (Silence)
[10:05] Speaker 1: Yeah.

[10:06 - 10:12] (Silence)
[10:12] Speaker 2: Um, Okay.
[10:13] Speaker 1: Yeah.
[10:13 - 10:17] (Silence)
[10:17] Speaker 1: Yeah, um...
[10:18] Speaker 2: Mhm.
[10:19] Speaker 1: Uh, yeah.
[10:19 - 10:28] (Silence)
[10:28] Speaker 2: Yeah.
[10:29 - 10:36] (Silence)
[10:36] Speaker 1: Uh-huh.
[10:36 - 10:41] (Silence)
[10:41] Speaker 2: Yeah.
[10:41 - 10:50] (Silence)
[10:50] Speaker 1: Yeah.
[10:51 - 11:00] (Silence)

--- Segment 3 (Starting at 20:00) ---

[20:00 - 20:14] (Silence)
[20:14] Speaker 1: All right. Uh, we are excited to kick off this second part of Gauntlet AI demo day. Thank you for joining us. Uh, it's been a wild day with some demos, some speed builds and uh it's good to uh have a break for lunch and let everybody meet each other. Uh, we're going to keep on rolling. Um so, next up, we have Abraham Julian.

--- Segment 4 (Starting at 30:00) ---

[30:00] Speaker 1: and Nathan uh with night vision.
So we'll turn it over to them.

[30:18 - 30:28] (Silence)

[30:28] Speaker 2: Um hello everyone.
My name is Ibrahim.
Uh this is Odio and this is Nathan.
And our app is called Night Vision.
Uh first of all, do we have any chess players in the house?
Just kind of raise your hand.
All right.
So, if you've ever played chess over the board and you want to analyze your game, um manually put in the pieces into like uh stockfish or whatever is a tedious process and um you know, it could be difficult to do.
And if you want to improve your chess game, it's kind of important to analyze your games, right?
So, we, that was the problem we identified over um the course of this program.
And our solution was to create a computer vision model that could look at a physical chess board and map that out to um map map that out to a fen notation.
And then we're able to analyze the game that way.
Okay, so um to turn a computer vision model, you do need high quality data set and um manually labeling the any a video or a picture of a chess set, it's also a tedious process as well.
So the way we generated our data set was that we um used blender to render a chess board um and chess pieces as well.
And with Blender's Python interface, we're able to get several positions and generate a high quality data set.
Um this is what our data set would look like. Uh we have the corners of the board.
We have the pieces and we have their occupancy as well.
Um I don't see anything displayed here.
Is that fine?
Okay, all right.
So I'm going to do a live demo where we're going to run inference on a chess board image.
So going to open this file here.
And we can see this is from Black's perspective.
So I'll run recognition on this.
Just give it a second.
And then here, uh you could see the bounding box for the board.
We can see I'll also zoom in.
You can see the pieces correctly identified.
And we can also compare it here on the digital chess board.
You can see the castle king, you can see the knight, you can see all the other pieces and you could do an analysis on this.
This is one part of Night Vision.
I'll hand it over to my colleagues to do the rest.

[33:53] Speaker 3: Hi, I'm El.
I want to describe our two main features of our app.
We offer a tutor and an analysis engine for analyzing your games, whether that be from a live game board or from one of your play your favorite platforms like chess.com.
Our tutor gives even moves that in real time match the skill of you as you play against it.
As you can see here and now you can using PGns, you can upload your games from any platform, chess.com or the live games.
We provide analysis here.
I'm going to hand it off to Nate, who's going to give you guys a nice live demo.

[34:52] Speaker 4: Yeah, all right.
So we'll go over to our app here and this is a game that's already been analyzed.
So we still have the annotations that you would expect uh for many other chess engine like a blunder, but we also have tactical annotations.
So we can see here we have a fork as the queen is attacking the pawn and the king and we also have pins as this bishop is poning is pinning the uh pawns to the queen there.
So basically with Night Vision, you can get your chess game whether it's over the board from any chess website or from our tutor and you can get in-depth analysis that's instructive and can be understood at any level of play.
Thank you.

[36:14 - 36:16] (Silence)

[36:16] Speaker 1: Awesome, thank you guys.
Next up, we have portals.
We got Adam Paul and Spencer Owens.

[37:02 - 37:06] (Silence)

[37:17] Speaker 5: All right.
How we all doing?

[37:24] Audience: Good.

[37:25] Speaker 5: Woo.
AI has kicked off a revolution in gaming.
Uh if your Twitter feed looks anything like our Twitter feed, probably looks a lot like this.
Um a lot of games.
This is not normal.
This wasn't normal uh even just a few months ago.
AI has empowered developers to make uh games that used to be reserved for like AAA Studios.
Um we can now do that with just JavaScript and AI.
Um Adam and I caught the bug uh over the course of Gauntlet.
We started developing some games as well.
Uh but we ran into an issue of distribution.
So we built some fun games, but how do we get this out to people?
How do we get people's eyes on this?
Um and so to solve for that, we built the platform that we think can build the future of gaming.
Bring the future of gaming together.
It's portals.

[38:28] Speaker 6: Yeah, so we uh we each built a game and it was a lot of fun.
Our games are fun to play.
But honestly, we were done building them in like less than a week.
And so we were sitting there thinking, what are we going to do with these games now that we've built them?
Uh maybe we'll combine them into one big game, maybe we'll, I don't know, put them on Twitter and they'll be missed and forgotten after a day.
And we realized we can use AI actually to create a hub where we can put our games on the web, everyone can find them.
And that's called Portals.
It's not just a hub for our games, it's a hub for anyone's games.
So creators can go here, game creators and they can upload their games and what we're really trying to do here is recreate the magic of like that early 2000s web game era.
Anyone who was around for that, it was a lot of fun.
Um we had, you know, addictinggames.com, newgrounds.com.
If you guys remember that.
And so that's what we're trying to recreate here.
And the idea is you go into into portals, game dashportals.com.
It's live right now.
Uh and it's it's to start with just uh a basic link aggregator.
You know, so maybe we go to all games.
We can see we've got plenty of games live on the site now.
People are organically adding their games every day.
Uh we've got over a dozen people so far uploaded their games, lots of Gauntleters.
But in addition to being just a standard link aggregator, the website itself is also a game.
So we have this 3D game world that is backing the whole website.
Look at all of these portals.
You can fly around in your rocket.
These are all user generated.
You can customize, add them yourself and each of these portals, if we were to say fly into one of them, what do we see?
Boom, now we're entering the world of Monster Hunter, which is a game created by ater here, one of us.
Super fun game, you can access it on portals.
So, this is kind of what we're thinking will be the future with the animation layer of the uh with 3js, which is the new animation layer for the web like flash used to be.
So I'll just quickly show you this will just run through some fast features on the website.
Uh this is how a creator would go and add a portal to their world.
Uh you know, put your URL, you can choose where in the game world you place it.
And you can customize the color, all these things and you just submit it.
And pretty quickly your portal is up and running.
You can go and fly into your game world from our website.
But we also think that not only can creators just add their games, they can actually do a lot more with something like this.

[41:03] Speaker 5: So we think Portal solves three big problems in uh in game development.
The first is distribution, as we've talked about.
Um if you're a game developer, you need to get your your game out there.
We already have users who are engaged and willing to play games.
Um and that's huge because unless you have a huge social media following, it's not going to happen for you.
Um number two is we think it can solve the issue of money and revenue for an indie game developer.
Currently, if you're, if you have a massive platform, you can vibe code a game, um and then put some ads on there, you get revenue.
Um that's great, but for the rest of us, um we've created an interoperable uh game asset layer.
So you can create things like skins.
Um you know, unlockable characters, um secret levels, things like that.
And instead of just, you know, your so the website, it is uh, you know, it's trusted.
It's through the platform.
And finally, we hope to help EdTech solve their motivation problem.
So, learning platforms exist.
Um we're going to create the reward layer and um this reward layer, you know, students can work in their learning app, gain a virtual currency and instead of having, you know, a pizza party or something that they don't care about.
What kids care about?
They care about games.
And so we provide the infrastructure for them to um actually invest in something that they care about.
Ontologically,

--- Segment 5 (Starting at 40:00) ---

[40:01] Speaker 1: we're pretty similar uh to uh Steam and um you know, similar Roblox.
But um yeah, this is a bet on the Open Web and a bet on AI.
[40:24] Speaker 2: Yeah, and so even right now it's live.
You can go to game-portals.com.
If you've got a game, upload it, you can play other games and we really think portals is going to be a lot of fun.
Thank you.

[40:50] Speaker 3: Awesome.
Thank you guys.
Um up next, we've got Aiden, Thomas, and Palmer um with an app that I will let them describe uh called Ivoni.
[41:45 - 41:55] (Silence)
[41:55] Speaker 4: All right.
I'm Palmer.
This is Sterling and this is Thomas and we're going to talk to you guys today about Ivoni, uh your new collaborative knowledge base.
So, when working with AI, we identified a problem very very early on.
Data is a mess, it takes a ton of time to like find all the context that you need for what you're building and it kills your productivity.
We have tons of sources that we gather information from every day, especially when building with AI.
You talk to Chat GPT, you talk in cursor, you talk to cloud directly, perplexity searches, right?
It all takes so much time and they're all in separate places and we want to consolidate it.
Furthermore, your LLMs don't know anything about you.
So we want to add all of this context to the LLM and give it information about what you're actually building.
You know, get something that's specific to your project.
So, our solution that we have for you is Ivoni.
It's your team's unified knowledge repository.
We want to bring all your data into a secure centralized hub where you have everything that you need that's pertinent to your project, your research, everything that you're building all in one place.
Um and again, this is because everything is so fragmented and scattered right now.
Um and this is a problem that we had very early on in every ideation session.
Everyone goes off, does their own research and then you got to gather it.

[43:11] Speaker 5: So uh, we have this uh demo here um, we have like you're researching a topic basically, and like you find a good paper, you don't really know what's going on here.
Um basically all you need to do is press command K and uh our Mac OS app realize shows up, it realizes like what's you don't need to do.
Takes the link automatically, sees it's a PDF document.
All you need to do is press press enter and uh it gets ingested into our uh your database.
And now what this allows it to to do is basically chat with the uh paper, ask anything about it.
And uh yeah, you can see on the right side here, uh all the sources it used to formulate that answer.
And uh this is like just a tiny part of our platform.

[44:23] Speaker 5: It's impossible to go out of the video and continue to the next slide.

[44:29] Speaker 5: There you go.
So uh, basically we spent go back.
[44:41] Speaker 5: How do I
[44:42 - 44:45] (Silence)
[44:45] Speaker 5: There we go.
We spent most of our time actually building like our ingest engine.
So you can basically integrate any any kind of app you have into it.
So basically you have the command K anywhere.
And for example, for YouTube, you're watching a good YouTube video, you want to save it.
All you need to do is press command K.
It realizes, oh, this is a YouTube video.
saves the link to your database.
Um embeds the whole transcript of the YouTube video, and you can then chat to it using our platform.
We also like allow you to organize your stuff into workspaces.
You can share your workspaces with like your friends, uh colleagues, so you can like they can use it however they want.
Um and you can work together on project as well.
[45:46] Speaker 6: So, there are a bunch of different solutions for this that we've already found, but what makes Ivoni unique?
Um, it's secure.
So, we have a platform that will allow you to take your data and keep your data because of the fact that we're using Tinyvec, which is a locally embedded uh vector database.
So it's always yours and you just have to download it.
Um offline flexibility because of this as well and it's scalable.
So, it's enterprise ready.
We have workspaces for people to collaborate and it's also swift and seamless, incredibly fast, again, thanks to Tinyvec and some of the LLM providers we're using.

[46:36] Speaker 4: So, you're probably wondering what's next for us.
Um, as mentioned, enabled by Tinyvec, we can expand both the depth and the breadth of the features that we have in Ivoni.
Um, things that we want to add immediately, we want to hook it up to an MCP server.
This will give your LLM or this will give any LLM or agent access to your knowledge base.
So you don't even have to go in there and do the manual work.
It'll do it for you.
Um, we want to do iOS and Android mobile applications, um, because then you can access and update your information on the go.
I don't know about you guys, like I think of stuff while I'm driving all the time that I would love to be able to just add like little facts and opinions to my knowledge base, um, especially about projects I'm working on.
And then lastly, there's a lot of potential here for secure enclaves.
Um, when you talk about like really, really sensitive data, being able to have it offline is incredibly important.
And so that's uh kind of another, you know, larger goal that we have for Ivoni down the road.
Um, but yeah, you can go to Ivoni XYZ Ivoni.XYZ right now if you want to try it out.
Thank you guys for your time.
[47:57] Speaker 3: Awesome.
Thank you guys.
All right, up next we have Alesha and John with The Last Ember.
[48:36] (Testing, testing)
[48:39] Speaker 7: Does this one work?

[48:48 - 48:53] (Silence)
[48:53] Speaker 7: Sure, let's stand over there.
[49:16 - 49:22] (Silence)
[49:22] Speaker 7: Sorry, just doing the sound.
All the way all the way.
All the way up.
Okay.
Hello everyone.
I'm John and this is Alesha and we're the team behind The Last Ember.
So, it's no secret that games are really expensive.
One of the most awaited games coming out, Grand Theft Auto 6 costs an estimated 2 billion dollars to make.
And another one, Elden Ring, Game of the Year, cost 200 million, which is considered cheap.
Uh, and we've seen how AI can meaningfully disrupt enterprise SAS.
That's kind of the definition of Gant AI where one or two person teams can actually compete with gigantic SAS companies.
And we've seen how AI can do that with games too on Twitter, but how about AAA games?
AI just can't really do realistic graphics, vast worlds, fast paced combat, or can it?
So for our capstone, we decided to see just how far we could push AI and if we could make a meaningful dent in the world of AAA gaming studios.
And we hope you enjoy.
[50:59] Narrator: Our world was broken by fire and scale.
We lost everything.
Even our hope from an act of defiance.
Our village burned.
All were erased.
He lost his home and his family.

--- Segment 6 (Starting at 50:00) ---

[50:00] Speaker 1: Years later,
the cycle returned.
But this time
no more cruelty,
no more destruction.
The cycle ends with him.

[50:36 - 50:41] (Silence)
[50:41] Speaker 2: We have adopted a lot of AI approaches during our game development from voices, videos, sounds, all aspects. For the trailers that you just saw, is we use Leonardo AI to generate images that fit with our game styles.
And then we feed those images to runway AI to give it camera control.
So, we can better prepare the player to embargan journey with us.
And if you see closely, you'll see someone familiar on the screen.
How we did it is we took a lot of photos of Austin's face and we used those images to mold the 3D model and then use the images to bake it on the model and put his head on the night.
So before, people have to go into professional studio to do a full body scan to have the 3D model. But now with the AI, we can just render this in less than two hours.
On top of that, we also use Blender MCP to help us troubleshooting because sometimes there are some problems that we just cannot solve by searching on internet, on YouTube, but the Blender MCP can help us look at the problems that we have, the settings and then help us troubleshoot.
And without further ado, let's see the game.
[53:16] Speaker 3: Yeah, there's not really much of a choice here.
We have to pick Austin.

[53:25] Speaker 3: Yeah, so welcome to the world that we've built.
Um just to tell you all, this is all built in pure JavaScript running on the browser.
This is not unity or unreal.
And the grass field that I'm running on, these are individually rendered.
Millions of grass blades.

And if I turn Austin around here and I zoom in.
And of course a lot of this code was written by AI as well.
So uh we've really tried to use AI to kind of tackle every aspect of game development here and I think to finish it off, I can show you some combat.
Do you guys want that?
So I think I have a pretty strong enemy.
might even be a boss.
Uh I should probably power up before I fight him.

That's modeled after Austin's voice by the way, using AI.

And yeah, this is complete pure JavaScript running on my MacBook Air.

[55:22] Speaker 4: Well done.
[55:24] Speaker 3: Yeah, so uh we think that in a few years, AAA Gaming Studios making millions uh could be run by one or two people and we hope after this presentation, you'll think the same.
Thank you.
[55:55] Speaker 4: Thank you.

[56:07] Speaker 5: All right, thank you.
I can't describe quite the feeling of what it is to see a demo and then see that they've rendered your face in 3D and your voice on on a video game character.
That's that's a first for me.
Um, all right, next we have Amir with Nor.

[57:26] Speaker 6: All right.
Hello everyone.
My name is Amir and I have Nor.
Picture this, you're back in college, sitting down in your lecture.
You're so frantically writing down every single word the professor says that you end up missing out on the entire lesson.
And this is just the reality.
70% of college students in US have reported that they are not able to engage in their lectures because they're so focused on taking down notes.
Traditional note taking causes college students to miss out on all the key insights throughout their classes.
This is because they're so focused on taking down notes that they end out end up missing out on the entire understanding.
And after class, they're juggling multiple tools, whether it's having their notes on Google Drive, having flash cards on Quizlet, or accessing random websites to take practice quizzes.
Everything is fragmented which leads to inefficient study sessions as well as wasted time.
This is where Nor steps in.
Nor is an AI driven study platform as well as a note taker that can transfer any type of content including PDFs, slides, recordings, or even YouTube links and generate detailed notes, flash cards, quizzes, and mind maps for those visual learners.
Nor is available on both iOS and web, which enhances a user experience allowing them to first start off by recording these lectures or processing them on their phone and then later up following up on their web to be able to review and study on a bigger screen.
By freeing students from manual note taking and allowing them to fully engage and absorb and absorbs all the information in their lectures, they will have a better understanding throughout their classes and have a fully set of study materials ready to go all centralized in one app.
Now this is a quick web demo of Nor.
Users can record their lectures, upload audio or documents or even process YouTube links.
They can go ahead and add a title, choose what class this lectures for and add tags to enhance searchability throughout those uh classes.
Then they can follow along with the processing to keep track of what's going on.
And then when it's ready, they have a personal dashboard created to be able to review their notes and then also their flash cards, mind maps, and their quizzes.
They also do have the feature to export these notes to PDF and generate a shareable link to class uh share with their classmates.
They also do have a calendar to be able to keep track of those lectures and know when they were recorded.

[1:01:15] Speaker 6: So far, we've gathered over 50 test users across three of the biggest college campuses in North Texas and here in Austin.
And we've overall received positive feedback as well as suggestions of new features to implement, one of those being the YouTube link processing.
We're planning to officially launch on April 1st and we're solely relying on user generated content to promote on our Tik Tok and Instagram as well as word of mouth and campus ambassadors to expand our user base.
If interested, you could visit us here at Nor.online or scan this QR code to download on test flight.
Thank you.

[1:02:18] Speaker 5: Awesome.
Thank you.
Next up, we have Amelia with Study B.

[1:03:05] Speaker 7: All right, hello everyone.
I'm Amelia and I'm very excited to share my project Study Be with you all.
So let's talk about a problem that a lot of students are facing right now.
It's traditional study methods.
A lot of students rely on these, but they can often lead to passive learning and

--- Segment 7 (Starting at 60:00) ---

[61:08] Speaker 1: limited, um, retention.
Students are just not aware of more creative approaches to studying.
I know when I was in high school, not even two years ago, I was unaware of um these more creative study methods.
So I felt trapped by these traditional methods such as flash cards or just rereading your notes.
Um, but more diverse and active study methods can be a lot more engaging and effective for students.

So the solution is Study Bee.
Study Bee provides students with a structured way to experience more creative methods to learn.
It doesn't only um provide these suggestions, but it will help the students um use them and walk them through these study methods.

So I've implemented six methods into my project Study Bee.
I'm not going to go over all of them right now, because I'll be showing some in the demo.

All right, I'm going to walk you through the user experience here.
So first you have your subjects.
So this student chose history and philosophy, you upload documents um that are relevant to these um subjects for you, and you can go to your calendar and create tests.
And with these tests, you can link your um documents that you uploaded and generate some suggestions for studying.
So um we have these for American Government, um study guide, and we can even create these um new events based on what the AI suggested, so just like that.
But the real material is going to be in the study materials page.

So we're going to see our active materials here, and it looks like we have one that is an SQR4 method.
Let's dive a little deeper into that.
So what is SQR4?
So the app is going to walk you through all of that.
So first you're going to survey your document.
So you're going to read the titles, headlines, look at the images, not go too deep into the information.
Um, and you're just going to take some brief notes and you are going to provide some questions that you have, because you haven't thoroughly read the document.
So you might have some questions on um, the information.
And then you're going to read the document super thoroughly, um and answer the questions that you ask yourself.
Take some extra notes.

And then you are going to recite, reflect, and review.
And then complete your study session, um and it'll be in your completed materials.
Now, let's create a new study material, and we're going to use the Feynman technique for this.
Um, this is going to be on an American Government study guide.
So this technique is really cool, because um the student kind of becomes the teacher in this, because we learn a lot better when we um can teach to other people.

And so the student becomes a teacher, and the AI becomes the student.
So the student's going to either record themselves doing a little lecture of the um information in the document they connected to this study method, or they're going to just type out some text.
So for example, there are three branches of the US government.
And they're going to submit their text or their audio recording, and the AI is going to ask them a question, such as what are the primary functions of each of the three branches of the US government?
And so the student's going to have a little conversation with the AI.
So they're going to provide an explanation, and then the AI is going to question them on that explanation, and it's going to be a good back and forth for the student to learn.
Another study method is um chunking your time.
So focusing 25 minutes on studying, and then five minutes, take a break.
25 minutes studying, um and repeat that process.
So we have a little timer up there to help the student um take care of that.
And these are just three of the six uh study methods I've implemented, but I'm working on more, and uh this is Study Bee.
Thank you.

[69:53 - 70:01] (Silence)
[70:01] Speaker 2: All right.
Up next, we have Arpan with Doctor Contract.

[70:08 - 70:16] (Silence)

[70:19 - 70:28] (Silence)

[70:31 - 70:53] (Silence)
[70:53] Speaker 3: Test, test.
All right.
So who's ready to talk about B2B SAS?
Let's go!
All right.
So this is Doctor Contracts.
Uh, I'm Arpan by the way.
Uh, this is Doctor Contract.
Doctor Contract is designed to save your organization hundreds of thousands if not millions of dollars.

So if you're a business owner, you probably have this issue.
You have tens or hundreds of contracts scattered across your organization, and it's a nightmare to keep track of renewal dates, of cancellation dates, of how much you're paying in total, of number of seats, all that stuff.
Doctor Contract handles all of this for you.

So the way we work is that we crawl through every single email inbox in your company in a privacy preserving way.
We analyze all of the contracts, all of the documents, we pull out key terms, important information, and then we upload this to our platform and present this to you in a beautiful dashboard.
So, let me show you how it works.

[72:09 - 72:12] (Silence)
[72:12] Speaker 3: So first we have our email crawler.
This runs on your desktop, so it is privacy preserving.
You drop in your Google Workspace credentials or Microsoft Outlook, whatever.
And then we will pull out every single email in your organization.
You can select which ones you want us to crawl.
If you don't want us to crawl an email, then you deselect it and we will not look at any of that, any of those emails.

And then we can go ahead and analyze them.
We will search by subject lines.
So you have complete control over what we look for, what subject lines we search for, what we're analyzing.
You can pull out subject lines if you don't like them or if you want to hide them from us.
All good.
And then we can start the crawl.
We go through every single email inbox, search for those subject lines, and pull out the contracts.

Or if you want, um we also have another option where you can drag and drop PDFs.
So if you have a bunch of PDFs, we'll analyze which ones are contracts and then we'll upload them to our platform.
I just dropped like 50 PDFs and we'll analyze them in under a minute.

And then we'll analyze them and store them in our contract.
All your contracts are encrypted, it's totally secure.
This is our dashboard.
You can see typical dashboard stuff.
And then you can go to our contracts page, where you will get a list of all of the contracts that we've analyzed and pulled out.
Um, you can see the vendor name, renewal dates.
You can choose what column headers you want.
So if you want to see the cancellation notice date, you can see that.
You can filter the contracts, maybe you only want to see contracts above $50,000 a year, you can do that.
Uh, you can sort them by whatever you want.
Uh, you can export this data as a CSV.

And then you can also click on an individual contract to get more information.

So we'll click on one.
And then you can see the details we pulled out, the contract summary, start date, yearly cost, billing cycle, whatever, number of seats you've purchased.
All of this we've pulled out from the contract with OCR, with LMs.
And then you can also see the PDF on the right-hand side, so you can get a sanity check, look deeper, whatever.
And then we also have a deep research feature.
So we will research all the terms in the contract, and then we'll tell you how is this vendor?
What's the ratings?
What are competitors?
Are you overpaying?
Are you underpaying?
And this is super useful information if you want to negotiate your contract.

And then we also pull out all the dates, all the important dates from all the contracts we analyze, and we plot them in a calendar, so you can keep track of your cancellation dates, of your renewal dates, things like that.
So you don't accidentally forget to cancel a contract and lose 50 grand.

And then we have a category breakdown.
This helps you visualize and understand your spend.
So we assign tags to all the contracts we analyze, consulting, security, infrastructure, DevOps, whatever.
And then you can see each contract and uh see how much you're spending on infra, or how much are you spending on DevOps and get a much much better understanding of how your company is spending money.
So we launched our beta last week and we are close to closing our first $20,000 in revenue.
Thank you.

[78:15 - 78:18] (Silence)
[78:18] Speaker 2: All right.
Thank you, Arpan.
Uh, now we've got Brett and Will with Echo AI.

--- Segment 8 (Starting at 70:00) ---

[70:00] Speaker 1: Hello?

[70:05] Speaker 1: All right, hi everyone.
My name is Brett and this is Will and we're not showing our screen yet, so let me try and get that going.
Let's give us a second here.

[70:48] Speaker 1: Suspense hangs in the air.

[70:53] Speaker 1: Oh, we're on.
Yeah.
Oh, we're good.
Okay.
Someone should have told me that.

[71:04] Speaker 1: All right, so I'm Brett and this is Will and this is Echo AI.
Um, so one of the things that I personally struggled with was where we had, um my wife and I with our newborn, we're watching the Australian Open and we had subtitles on to not wake him up and I just noticed how terrible the captioning was on that.
It was 15, 20 seconds late.
It was the players names were wrong.
It was just an utterly broken experience.
And that's something that we can fix through AI.
Um, so when you're watching watching live events, be it sports or news or the there's a long delay, the accuracy is typically like 13 percentage points below what it should be, uh, for domain specific terminology.
And uh the current ADA and FCC regulations are unmet and they're actually not enforced because the the current state of the art is so bad.

[72:28] Speaker 1: So our solution is to develop a AI first uh close captioning and transcription service that uh delivers transcriptions in 750 milliseconds.
That's 20 times faster than the the current state of the art in the industry.
And we have a native integration with all the professional audio and video workflows.
You know, some of the things that we're seeing here at the presentation and anything being done uh on your favorite cable news station.
Uh and we also are able to extend this technology through a plugin architecture where you can enable and disable and change the configuration on the fly of any of the plugins that are working through um through the system.

[73:28] Speaker 1: So in just two weeks I was able to have uh some very impressive conversations with some stakeholders at various companies such as the NBA, Twitch, Assist, and upcoming next week the um MLB Advanced Media.
Each of these conversations led to two additional leads and a referral onto uh investors into the company.

[73:59] Speaker 1: The total uh media and broadcasting and enterprise education, as you get into captioning and uh live production is about $80 billion.
Of that, the accessibility and close captioning is close to 30 billion and I think that we would be able to capture a third of the market within 18 months uh on being able to integrate all of these uh large contracts into us and and go down the long tail of all the second, third and and amateur type of sports and news productions.

[74:47] Speaker 2: Yeah, so on the technical side of things, just what makes us so much different than everyone else is it really does just come down to AI, both on the building our product.
You know, we're able to build it incredibly quickly due to using AI.
And also just we actually believe in the technology.
A lot of the competitors in the space, um, they're very set on keeping a human in the loop and that's where their 15 to 20 seconds comes from.
They don't see the present where it is working without a human in the loop and it's only going to get better from here.
Um, so that's our biggest technical differentiator.

[75:53] Speaker 2: Uh, but to introduce I'm Will, um, I've been a software developer for four or five years-ish, um, done some work at smaller companies, done some work at bigger companies and now excited to try this out.
Yeah, I think Will is definitely underselling himself.
He uh is quite an amazing partner to work with um and I'm proud to, you know, have him as a founding engineer on Echo AI.
Uh so I've worked uh I've got a few more years of experience of that and I've worked mostly for the last 12, 15 years in the uh VC gig economy space.
As you can see some of the things there.
Uh I also started a nonprofit for the Ukraine war and we raised $77 million in the first year.
Um, we have a few strate- one strategic advisor who uh is already on board and two more that are unannounced as of yet.

[76:59] Speaker 1: So the ask for us is uh to help us raise a seed round.
That would allow Will and I to work full-time on this and we believe we would have our first customers within weeks and months, uh not necessarily anything longer than that.
We are UE positive from the day one.
Our lean structure allows us to have uh superior gross margins where an average SAS company now maybe 75, 80%.
We think we're able to get closer to 90, 92% margins, uh which are just unreal.
Uh and that goes through the use of AI and our lower uh staffing and other costs.
Um, there are four different revenue streams that we're looking to capture.
Uh one obviously being the enterprise captioning and translation I've been talking about, but there are some future revenue streams that are already identified.
Uh license licencing fees from third party plugins that get added to the system, uh a subscription based analytics, both for existing customers and for external parties looking to understand what is being talked about and and what is the the sentiment around some of those conversations.
And then, um obviously the data brokering as you get into uh having petabytes of very interesting data, uh you definitely need to be able to monetize that.
So I would appreciate uh anyone checking out our website, uh echoai.studio and we have a QR code there and we look forward to uh helping you.
Thank you.
Thanks.

[78:46] Speaker 3: All right, thank you.
Uh next up, we have Kale with Primer.

[79:03 - 0:79:24] (Silence)

[79:24] Speaker 4: And yes.
Allow.
Let's go.
Wait.

[79:48 - 079:57] (Silence)
[79:57] Speaker 4: All right.
Uh, thank you for having me.
Um, so my name is Kale.
Um, and I am a big Neil Stevenson fan.
Um, I don't know uh how many people have read it, but he had a book in the 90s called the Diamond Age or the Young Ladies Illustrated Primer.
And it revolved around this girl from a very hard scrabble beginning who had come across this kind of AI powered book that would anticipate her needs and kind of teach her lessons through storytelling, through kind of videos and images and whatnot.
I thought that was a really cool idea uh and so I decided to build it.
Oh.
And then I decided to get rid of that.
All right, so how the primer works is there are adaptive stories.
The AI um will kind of anticipate what the child needs to learn at any given time.
Um, and it will teach them those lessons through um a blend of storytelling and images, videos, games.
Um, and the primer kind of grows up with the kid and um, yeah, it kind of it gets more complex, the questions, the lessons get harder as they go.
Um, and as they every there we go.
Sorry about that.
Um, so yeah, uh the primer will generate all of these kind of age appropriate stories and lessons and then it adapts and grows with them.
And this is why I'm a little bit flustered uh because I managed to break the app an hour ago uh by running get reset hard in the wrong terminal.
Um, so instead of throwing away the half hour of work that I had done trying to tweak the voice pipeline, I instead threw away three days worth of work on the app itself.
Um, so yeah, uh drop your commits early and often.
But anyways, unfortunately, uh I won't be able to demo it today, but um it's not going to take too much longer to fix.
Uh I just I I broke it an hour ago and and didn't have time to put it back together in time for the demo.
Anyways, um, so the features of it.
It has a very personalized um relationship with the user and it interacts with the user in a very um, just kind of natural and holistic way.
Um, it becomes their friend and it becomes their mentor.
Um, everything is generated in real time.
It's not pulling uh information from databases or what not.
It's it's kind of coming up with it on the fly as appropriate.
Uh, and yeah, it kind of seamlessly integrates text and images and video games.

--- Segment 9 (Starting at 80:00) ---

[80:01] Speaker 1: uh would be kind of the cool thing that I want to do next on it.
[80:10] Speaker 1: Um, and by doing this, so not only can you teach the children really well, but you make them love uh learning because of that connection that they have with the primer.
[80:31] Speaker 1: Uh it's a big part of the story and it was a big part of why I wanted to do it.
[80:42] Speaker 1: Um, yeah, so uh technical architecture. It uses PipeCat uh PipeCat Cloud uh for the voice pipeline. uh that does kind of
[81:00] Speaker 1: speech to text and text to speech and then there's an AI agent in the middle of it, um that can do all of the tool calls and whatnot that you want.
[81:12] Speaker 1: Uh, and it is quite cool.
[81:15] Speaker 1: So, my roadmap and next steps. Number one, fix it.
[81:20] Speaker 1: That would be a good idea.
[81:22] Speaker 1: Uh and then after that, um currently it just uses uh GPT's um history function to track the progress.
[81:35] Speaker 1: Uh I wanted to put in an actual vector database and then get it into the hands of kids.
[81:42] Speaker 1: Anyways, thank you very much.
[81:45] Speaker 1: That's all.
[81:47 - 82:34] (Silence)
[82:34] Speaker 2: Alright, thank you.
[82:37] Speaker 2: Uh next we have the feed up team consisting of David, Thomas, and Harmony.
[83:07 - 83:20] (Silence)
[83:20] Speaker 3: Okay.
[83:21] Speaker 4: All right.
[83:22] Speaker 5: Hello, I am Harmony Morris.
[83:26] Speaker 6: I'm Thomas Davidson.
[83:29] Speaker 7: I'm David Johnson and we've been working on Feed Up, which is an organizational feedback platform for big organizations.
[83:48] Speaker 7: So let us tell you about our problem in an anecdote.
[83:51] Speaker 7: Um, I worked at Target for about two years before I joined the Gauntlet just in a store and in all that time I was asked for feedback one time and I can pretty much guarantee you that nobody ever looked at it.
[84:08] Speaker 7: Um, there was definitely no action taken based on my feedback.
[84:14] Speaker 7: Uh, so what that means, um, given that this is not in any way a unique uh situation, viable insights just fall through the fingers of decision makers.
[84:26] Speaker 7: There's just too much data, there's not enough time to do anything with it.
[84:30] Speaker 7: And so no action gets taken.
[84:34] Speaker 7: And employees correctly identify that nobody's hearing them, and so they grow to resent um resent their employers.
[84:45] Speaker 7: And annual surveys just don't cut it anymore.
[84:48] Speaker 7: So our solution is to put gamified daily surveys that take 30 seconds or less into the hands of millions of employees across the country.
[85:00] Speaker 7: uh enabling their managers to produce viable action for them.
[85:04] Speaker 8: Um, so we tried to differentiate ourselves from previous solutions through two main ways, mostly by um consolidating the large amount of data into easily digestible, only the actual items uh request to send up to managers and uh we realized that mandates weren't quite enough to ensure compliance among these surveys.
[85:41] Speaker 8: Um, so the ease of use and gamification will be really number one in getting them there.
[85:48] Speaker 7: Absolutely.
[85:50] Speaker 7: And here is our uh just quick little demo of our mobile app.
[85:58] Speaker 7: Um, that would be for the respondents.
[86:01] Speaker 7: We have the reward system here.
[86:05] Speaker 7: Here you see a survey, nice and easy to take.
[86:10] Speaker 7: Animation looks good.
[86:13] Speaker 7: Um, and it's just a very simple process.
[86:19] Speaker 7: Um, you can view all of your old responses.
[86:24] Speaker 7: Um, and you can also um just yeah, you can view all of your old responses and then you see you have we have a streak function.
[86:38] Speaker 7: You can change your work days so that you're not um losing your streak just because uh you were off work that day.
[86:48] Speaker 7: And we also have just an always on feedback box type thing where you can tag uh create tags, choose to submit anonymously, just uh allowing more channel as many channels as we can for feedback.
[87:10] Speaker 7: And we also allow users to completely control the aesthetics of the app just because we think that'll help people use it more.
[87:18] Speaker 5: All right, here we have the manager's page.
[87:20] Speaker 5: We can see that there's a dashboard with a bunch of information taken from the respondent's survey responses.
[87:29] Speaker 5: There is a easy way to manage groups and users and we can easily create surveys to give out to your employees.
[87:40] Speaker 8: Cool.
[87:41] Speaker 8: And um how we built it, mostly it was to have the front ends interact with a uh single backend that is able to uh get all the data.
[87:57] Speaker 8: Um and also have a uh scheduling server so that uh we can provide the summaries uh very quickly to managers when they need them, but also um having reliable scheduling will allow for anonymized surveys to make sure that employees don't feel on the spot when they give answers.
[88:25] Speaker 5: All right, so how did we use uh AI in stage one to develop our planning docs and anthology.
[88:32] Speaker 5: We used Open AI's 01 and then stage two coding, we used cloud 3.7 and 3.5 and then from now and continuing, we'll use Open AI's 40 until there's a better newer model.
[88:54] Speaker 7: So what's next for us? uh we're hoping to find some pilot organizations and revolutionize their employee feedback collection and utilization pipeline and as well improve their employees's experience, retention and morale and we hope to expand to more and more types of organizations till we can get everybody's voice hurt.
[89:30] Speaker 7: Thank you all very much.
[89:31] Speaker 8: Thank you.
[89:33 - 90:39] (Silence)
[90:39] Speaker 2: All right, up next, we've got Drew with Sim Circuit.
[91:48] Speaker 9: Yeah, it's just not connecting.
[91:53] Speaker 9: Uh potentially.
[92:18] Speaker 9: Sorry for the technical delay.
[92:22] Speaker 9: Um my name is Drew Soterquist and I've built or I'm working on Sim Circuit, which is a educational mobile app um that is targeted to teach basic circuitry to K through eighth grade.
[92:50] Speaker 9: So what's the goal in Sim Circuit?
[92:54] Speaker 9: It is to bridge the gap in STEM education.
[92:59] Speaker 9: So I'm targeting K through 8 to learn the basics of circuitry.
[93:04] Speaker 9: I'm a bit biased, why is learning circuits important?
[93:08] Speaker 9: Well, as our world moves more and more into technology, um understanding the basic paths and flows and embedded technology is very important.
[93:22] Speaker 9: So my overall goal with Sim Circuit is to have 20 plus circuit components and three plus learning paths for the kids to grow and build their knowledge.
[93:37] Speaker 9: So the current challenge in circuit education is that there's no great one size fits all solution.
[93:46] Speaker 9: Nothing fully takes you from step zero to one on your journey.
[93:53] Speaker 9: There's a lot of abstract concepts in circuitry that I feel a lot of simulators or other apps do not touch on.
[94:02] Speaker 9: There's some age appropriate tools such as um online web app simulators that if you know what you're doing or you know the basics, you can go in, create your own components, your own circuits and see the flow.
[94:20] Speaker 9: There's limited hands-on learning, but there's a great kit.

--- Segment 10 (Starting at 90:00) ---

[90:00] Speaker 1: that I showed here of some snap on circuits. But the issue with these is accessibility. If you have this at a school, a couple kids in a classroom, these are very restrictive in the amount of components they have. If a kid loses a part, but how can this all be solved? My solution is SIM circuit.
So where I've built a custom drag and drop interface with intuitive design for wired connections, a component library and all of this with accessibility in mind.

So here's a quick demonstration of my app, starting first with the onboarding process of what you would see when you first download the application.

After signing in, we take you straight to the dashboard, which shows some gamification of a home screen with some basic levels going over um the intro or basics to circuitry. Right here, we clicked into level two, so where we can see the drag and drop design, adding in components such as a battery and an LED, connecting them with wires.

All of this with very intuitive design. And if you wish to delete a component, you can hold down, they shake, they get a little scared and you press the X to delete.

So as shown in the home screen there, my goal is to have three plus learning paths. The first three would be circuit basics, which I've completed so far. Next would be light and sound and energy and power.

So what's next for me? Um detailing a bit of my next development steps would be adding in real-time simulation, which I've started progress and working on, um which would show interactive voltage and current flow, and as well as multiple, or sorry, more educational content. So going down those other two learning paths that I've laid out. Thank you all for your time.

[93:24 - 93:27] (Silence)
[93:27] Speaker 2: All right, up next, we have Francisco with Autoplayer.

[94:03 - 94:49] (Silence - music plays)
[94:49] Speaker 3: Hi, uh my name is Francisco and uh I've made a TFT Autoplayer. I think after spending a lot of time uh building with AI, I would introduce my favorite game to it. So what exactly is TFT? It is uh basically uh eight player free for all game where everybody completes in a series of 1v1 rounds and it takes place on this grid-based board. Uh it's a little complicated with 63 units and 14 uh origin and 12 class synergies and there's also an economy component to it. Um but my real reason for making it is the grinding is incentivized in these rank games. There's a total of 31 different rank divisions and basically you start at the bottom and uh for me personally I belong like 19 divisions up, which takes about 20 hours of consistent perfect play, which is basically impossible.

So I wanted to create this thing to save me some time. Um so how does it work? Uh so I basically created an agent and uh gave it a bunch of tools. Uh the first one I fine tuned vision model, so it can recognize all of the characters. I uh gave it the game data so it could understand the basics of the game and I gave it tools to move uh buy units, sell units and place them on the board. And I shared the my strategies so it would play like I would. And so here's the first step where it scans the board, it takes a screenshot and it looks at the game state, it looks at the characters on the board and it decides the best moves to make. And then it translates it to a text text representation. uh just because it's not really a vision agent, it needs a text representation to understand what's happening. So I created this text format for it to understand and it uses that to uh make the moves from there. And then it makes the moves. Uh here's a little low-quality GIF, but it basically takes a screenshot, looks at what's going on and it buys from the shop and places them on the board as you would want them to. Um some optimizations that I made, uh since scanning takes about 2.3 seconds to um see the screenshot and analyze it, I did uh some manual maintaining and updating of the board state. Uh so that goes on behind the scenes and sometimes it'll chain moves without scanning again. And then from there waits for the next planning phase so it can uh continue playing the game. But uh yeah, that's the basics and uh so hopefully you can get where you belong without sinking too much time in. Thank you.

[99:26] Speaker 2: Good job.
[99:31] Speaker 2: All right. If you're just joining us for the Gauntlet AI Demo Day, please make sure that you go to demoday.gauntletai.com to see all of our challengers, all the projects that they have and to see what they're going to be presenting next. Next up, we have Gage McCoy with OTD. Your wardrobe just got smarter and he'll show you how.

[100:19 - 100:23] (Silence)
[100:23] Speaker 4: Uh I've spent more money purchasing clothes than I have on my college tuition. So I think that makes me the most qualified person in Gauntlet to say I'm the most fashionable person in Gauntlet. Um I'm Gage McCoy. I'm 19 years old and four weeks ago I met with a TikToker with 1.9 million followers. She pitched her idea for OTD or Outfit of the Day. And this is uh the app I'm working on for the past four weeks. I dropped everything to get to work. OTD is an all-in-one fashion app meant for building, sharing, finding and purchasing outfits powered by AI.

I only have a limited time with you today, so I'm just going to showcase the core features, starting off with the stylebot. Picture this. You're just asked a girl out and she said yes, crazy. You have to figure out what you want to wear for the date. You have three hours, so you snap a picture, say I'm going on a date in 3 hours. I need to know what to wear, just like in the picture. Um and your jaw drops as it immediately gets to work and our state-of-the-art uh stylebot will tell you what to wear. help you create a fresh, trendy outfit based off your style preferences and what's already in your wardrobe. This is personalization taken to the next level. It knows everything you wear and it uh will help you uh create the perfect outfit from that. The most the most interesting feature though for me to build was this uh clothing outfit analyst. So what it does is it will take any picture and it will scan it and find all clothing items inside of it and try to tag those. That way we can provide Amazon affiliate links for them. Initially, I tried using just one-shot classification and that did not work at all. My second attempt was to use Meta's SAM or Segment Anything model, and it was functional, but it was really, really slow and it was not sustainable. My third approach, I used uh MoveNet's pose detection to analyze the pose of the person. It finds key points like their shoulders, their waist, knees, and so on. and because of that, it's able to define what I call clothing regions. With the detected clothing regions, it's able to detect proper clothing items as you can see in the picture. This is fast. It's about 3 to 8 seconds per image and so it helps create a fast, smooth, and scalable experience for the user because waiting is so last season.

I'm excited to see uh where OTD goes. If you would like to reach out, uh talk to me about how OTD is going to change the way we style our lives, feel free to or if you just want to hear more about my personal story. Um thank you so much for your time.
[103:53] Speaker 2: One of the biggest unexpected parts about Gauntlet AI has been the emergence of so many game developers. The fact that we started this cohort and they hadn't built a game before,

--- Segment 11 (Starting at 100:00) ---

but now they can do it with AI.
So in that same way vein, I'd like to present Jack Mitchell and his game parkour run.

[101:15 - 101:25] (Silence)

[101:25] Alright. Hello everyone.
My name is Jack Mitchell, and for the past two weeks, what I've been working on is something that I've wanted to build for a long time, but I just haven't had the time to until now.
So I built an infinite open world parkour running game and, yeah, let's just get right into it.
So I've called my game parkour.run, and you can find it at prkr.run in any browser.
So let me just walk you through some of the core functionality.

So once you load in, you can run around, you can jump, you can slide, you've got some wall climbing, you've also got wall running for maintaining momentum and distance.
So that's all cool, but you've also got a grappling hook and double jump, which is super cool, they're unlockables.
So the core game loop is that you go around defeating enemies with your weapon, and then by doing that, you gain currency, which you can then use in shops around the world.
So that's all fine and dandy, but what I think sets apart parkour.run from the rest of the competition is the auto parkour and aimbot feature.
So it'll run around playing the game for you, basically, so then you can go do other things and then come back later and then upgrade your stuff.
So while that's going, I'd like to talk about my process.
So basically, in the past few weeks, I think there's been a massive paradigm shift in the way developers can develop games because with the release of new models like 3.7, Claude, it's become so much easier to code on a very high level.
So you no longer need to know necessarily all the low-level implementation details.
You can just say like, hey, Claude, can you add a gun to this parkour game?
And he'll just do it.
Like this took me 10 minutes to implement and now I have combat in my parkour game.
So point being that you can just like make stuff you want.
So I think at this point we've got some currency.
So the last thing I'd like to show you is the shop system.
So let's find a shop.
They spawn randomly around the world, and I think I see one over there.

So let's see if we get something cool.
Alright.
Looks like we can get a meteor tome, and it says it replaces the gun, so it's a new weapon.
Let's let's see what this does.
Let me go to a high vantage point to show you what it looks like.
And it looks like it just rains down meteors everywhere.
So if there's anything I want to leave with you from this presentation, it's that now that AI has gotten so good that you can just tell it to do stuff and it'll make stuff for you, you can create things that you used to only dream of building.
And so, yeah, I would encourage you to go out and build your dreams.
Thank you.

Alright.
Next up, we have the Lambert Brothers, Jefferson and Jared, who are bringing you the US Manufacturing Portal.
Their goal is to reinvigorate manufacturing here in the United States.
Let's welcome the two brothers.

[105:54] American manufacturing is falling way behind our foreign adversaries.
And um that is what Jared and I are here to change with US Manufacturing Portal.
So the issue is for decades we've been prioritizing low cost over manufacturing.
We've shipped all of our manufacturing capability overseas, and now we rely on China among other countries to manufacture parts that are critical for American companies and for our economy.
Our platform, uh, we hope will be the solution to a couple different problems in the United States.
We have a management platform, a marketplace, as well as instant quoting tied into all of that.
This is a little brief look into what our dashboard is looking like right now.
Uh, we want our software to be something you can drop into any machine shop in the United States and automatically start running their processes faster and better.
And the idea is that we'll have every shop API enabled.
We'll have a uh instant quoting process that'll allow us to open up a marketplace.
It'll be like the Alibaba of the United States for machine parts.
So uh right there you saw an image of what the marketplace might look like.
Somebody will be able to upload their part.
Our system will analyze it, find out how manufacturable it is, how many different operations it needs to be manufactured, and then generate a quote for the customer as well as suggesting improvements.
Uh, and like I said, this will be API enabled, so we'll have a um, you know, a marketplace where somebody uploads their part and we'll be able to get quotes from many different shops in the United States near them instantly.
So this, the manufacturing software industry is about $5 billion.
Um uh a $5 billion market.
Um of that portion of the market, we have about 16 to 17,000 different machine shops that would be able to use our software and take advantage of it.
And currently, uh with the current administration, lots of tariffs and things going on, manufacturing, there's a big push to bring it back to the United States and a lot of money has been invested into doing so.
For example, TSMC has just promised to invest $100 billion in bringing back chip manufacturing to the United States and lots of other investors are spending a lot of money, um, investing in larger uh, more automated machine shops.
Um, this is a picture of me with my father getting home from a deployment in 2008.
This is the whole reason why we're interested in this in the first place.
We care about our country.
We've been raised to care about the United States of America.
And we know that we're at a critical point right now where we have to be fixing the problem with our manufacturing and we think our our platform can be a part of the solution.
So with US Manufacturing Portal, we're going to speed up the process of manufacturing in the United States, cut quote times down to nothing.
And if you're an investor who wants to be part of that or a machine shop who's interested, please reach out.
Thank you.

Now, one of the biggest things that AI unlocks is the ability to create content, personalized content for all of you whenever you want it.
With that in mind, we have Joshua and Anthony, who are going to create AI-generated explainer videos on demand.

Hey, I'm Josh.
And I'm Anthony.
And we are Brunner, AI-generated explainer videos.
So, I imagine you've had the experience where you searched for something because you wanted a video to explain it and there wasn't really a good video that could explain it that well.
And I'm sure it would be nice if we had AI that could just generate explainer videos, but it's currently not that great.
And the reason is because making videos like that is hard.
And it's hard because there's tons of little decisions that are involved in making those videos.
Like just to give you a little ballpark, you're deciding what happens on every frame.
And for a 10-minute video at 24 frames a second, that's like 15,000 different decisions, which is a lot of time and energy.
And if you talk to maybe like an educational YouTuber, they'll tell you that it can take weeks to make videos like this.
And you might be thinking

--- Segment 12 (Starting at 110:00) ---

[110:00] Speaker 1: well, why don't we just use some of the new video generation models that are coming out.
[110:09] Speaker 1: And they're cool, like they're good at generating good graphics, but that's not the bottleneck.
[110:18] Speaker 1: We believe that the bottleneck is actually those thousands of little details that you have to get right.
[110:25] Speaker 1: For example, if you say chat GPT, make me an essay that's good that's on some subject.
[110:38] Speaker 1: Chances are the essay that it gives you won't be that good.
[110:43] Speaker 1: You have to actually get the prompt right, you have to work with it back and forth.
[110:52] Speaker 1: Eventually you'll get a good essay, but it'll take a little bit of work.
[110:58] Speaker 1: And it's the same thing with video models.
[111:03] Speaker 1: It might look cool, but with one shot it probably won't be that meaningful.
[111:09] Speaker 1: So, how do you get the details right and how do you do it quickly?
[111:16] Speaker 1: Enter Brenner, which is AI generated explainer videos.
[111:23] Speaker 1: So right now our project goes directly from prompt to video in one shot.
[111:33] Speaker 1: And in the future, we want to we want to make it into basically a pipeline.
[111:43] Speaker 1: We want it to give you like a like a like a little idea and tell you if it's going in the right direction, and if it's not, you want to correct it early and often.
[111:57] Speaker 1: So imagine like a like a discovery call with AI.
[112:03] Speaker 1: We also uh in the future are going to have it remember all of your preferences, so that way you can go from idea to video very quickly.
[112:15] Speaker 1: And to give you a little taste of that, we'll show you a little clip of what we can generate right now.
[112:24] Speaker 2: So we're going to show you here a quick 30-second clip that you can see.
[112:28] Speaker 2: This is just all text input and it's sped up a little bit for the presentation.

[112:39] Speaker 3: Welcome to our exploration of the quadratic formula.
[112:43] Speaker 3: A quadratic equation has the standard form X2 + bx + c equals 0, where A, B and C are constants and is not equal to zero.
[112:51] Speaker 3: The quadratic formula gives us the solutions.
[112:54] Speaker 3: Equals negative B plus or minus the square root of B2 - 4ac, all divided by 2a.
[113:07] Speaker 1: So that's a little taste.

[113:15] Speaker 2: So the big thing we were thinking about when we thought about this problem and solution is we thought about the learning pyramid.
[113:20] Speaker 2: A lot of you have probably seen this before.
[113:23] Speaker 2: What we realized is this is less of ranking methods of how we can actually learn, it's more of a workflow.
[113:34] Speaker 2: You I mean, you'll realize the bottom half of these, teaching others, practice, discussion, these are not ways to learn that initial content.
[113:44] Speaker 2: They're reinforcement learning.
[113:47] Speaker 2: So considering the initial content we look at, we see AI education as a amazing tool.
[113:55] Speaker 2: It's all stuck in the reading level though, 10%.
[113:59] Speaker 2: We see that if we can master AI educational videos, we can double that to 20% with audio visuals.
[114:09] Speaker 2: So looking at the progression of applicable AI modalities, as these each of these modalities come out, we see that as they reach production level quality, they expand exponentially.
[114:23] Speaker 2: Text, image and voice are all are all there getting there right now.
[114:29] Speaker 2: Um, what we're seeing is in the past, open source we didn't have open source models like Sesame, Corona and other ones as well.
[114:46] Speaker 2: And we're seeing that they are just getting the same quality too.
[114:51] Speaker 2: Um, and it's really impressive.
[114:54] Speaker 2: We're also seeing reasoning models as well, being a big help to be able to make long form content, like long form videos so they can be 5, 10 minutes and longer.
[115:06] Speaker 2: So when I was considering this this idea, the big question that I had was how much more useful is this in regular video?
[115:13] Speaker 2: Then if I just go on YouTube and I go search Khan Academy playlist.
[115:19] Speaker 2: And the more I thought about it, the more I realized that time and time again, we have seen these generalized incumbent providers switch to more individual personalized products.
[115:35] Speaker 2: And we believe the same thing is going to happen with with AI and video and audio visual education.
[115:43] Speaker 2: And so we're on the cutting edge of this.
[115:45] Speaker 2: We're ready to kind of exponentiate this and bring this everywhere and everywhere.
[115:52] Speaker 2: Thank you.
[116:11 - 116:21] (Silence)
[116:22] Speaker 4: Now, if you've ever coded with any AI coding tools, you'll know one of the hardest things to manage is versions of dependencies.
[116:33] Speaker 4: And so now for the next project, we'll have Joshua Jerbech, who's going to talk about anchoring, his solution to supercharging development with version specific um context.
[116:50] Speaker 4: Here's Joshua.
[117:05 - 117:34] (Silence)
[117:35] Speaker 5: Hello.
[117:39] Speaker 5: My name is Josh Cherbeck and I'm founding a company called anchoring.
[117:45] Speaker 5: Um it's both a desktop app and an API.
[117:51] Speaker 5: Um the current problems that AI faces with AI assisted development is that AI models uh are trained to generalize on a large corpus of data.
[118:06] Speaker 5: Um and they often fail when absolute specificity is required.
[118:13] Speaker 5: Um 66% of developers distrust AI code.
[118:19] Speaker 5: Only 10 to 30% of AI code matches correct library versions and 38% of AI generated code uses deprecated APIs.
[118:32] Speaker 5: Uh my solution to this is version specific documentation ecosystem, both with a desktop app and an API.
[118:43] Speaker 5: Um and we're going to revenue share with uh package maintainers so that they automatically sync uh up with our systems uh whenever a new version is released.
[119:01] Speaker 5: Um The business model uh AI platforms uh open AI, Anthropic pay us uh to whenever they're doing coding things, uh they request snippets based on the user's environment.
[119:19] Speaker 5: Uh so here's a little demo of the desktop app.
[119:23] Speaker 5: It's free and open source.
[119:26] Speaker 5: Uh it relies on uh web scraping versus actual documentation.
[119:36] Speaker 5: Um but you have anti patterns such as release, uh so that it doesn't recursively crawl uh other releases.
[119:48] Speaker 5: You're just going for the ones that you you want.
[119:51] Speaker 5: Um you version pin it.
[119:53] Speaker 5: You give it a a uh version um at the top left.
[120:02] Speaker 5: says Tori 2.40.
[120:07] Speaker 5: Um and here's the documentation snippets.
[120:10] Speaker 5: We do structured output.
[120:13] Speaker 5: So it gives it a title, uh description, list of concepts and the actual snippets itself all cleaned up um to give to an LLM when it's uh helping you develop.
[120:30] Speaker 5: And after this, we're going to show you a demo.
[120:35] Speaker 5: Um first without the MCP server.
[120:42] Speaker 5: Uh that's built into the desktop app.
[120:45] Speaker 5: Uh we're going to have it do a real time uh emission using Tori 2.0 uh which came out in 2022.
[120:58] Speaker 5: So I sped it up about 20X because uh rust compile times are a little slow.
[121:05] Speaker 5: Uh but it keeps trying to use deprecated syntax over and over and eventually gives up and tries to do something completely different using polling instead of real time emissions.
[121:17] Speaker 5: Then here we go with the MCP server LinkedIn and it's going to do the same exact prompt uh to give the uh it's going to do random words every few seconds uh on the React side, but it's emitting it from the rust side.
[121:41] Speaker 5: And there we go.
[121:43] Speaker 5: Uh no errors or anything.
[121:46] Speaker 5: It consistently looked up uh the documentation snippets based on the environment of the code base.
[121:54] Speaker 5: Thank you.

[122:01 - 122:10] (Silence)
[122:11] Speaker 4: All right, we're going to go back to our gaming revolution, our gamers here at Gauntlet AI and we're going to present Juan Pablo Herrera, who made Tussle Tubbies, a multiplayer online game that works on your browser.
[122:35 - 122:56] (Silence)
[122:57] Speaker 6: Okay, so the problem I'm tackling is that competitive shooters take a really long time to play.
[123:05] Speaker 6: Each game um takes like 30 minutes in Fortnite, and Valorant takes 40 minutes, and in Overwatch it takes 30 minutes.
[123:18] Speaker 6: So it just takes a really long time to play.
[123:22] Speaker 6: And people don't have time for this.
[123:25] Speaker 6: People want short content.
[123:29] Speaker 6: Like think YouTube versus Tik Tok.
[123:33] Speaker 6: Why watch one 40

--- Segment 13 (Starting at 120:00) ---

[120:04] Speaker 1: minute YouTube video when you can just watch many TikTok videos instead.
[120:12] Speaker 1: So, how nice would it be if there was a competitive shooter game that lasted that short amount of time, like three minutes.
[120:25] Speaker 1: So, that's what I want to build.
[120:30] Speaker 1: But surely there's a reason why all these competitive shooters take long, right?
[120:43] Speaker 1: Like but the reason is that um these competitive shooters build on previous games that were successful.
[120:57] Speaker 1: So think like Valorant and CS:GO.
[121:01] Speaker 1: Valorant took CS:GO and added characters like Overwatch and combined the two games.
[121:13] Speaker 1: So what if I take a game with a competitive scene and um has short games.
[121:26] Speaker 1: So that's what I did.
[121:28] Speaker 1: I took a mobile game called Brawl Stars and turned it into a competitive third-person shooter.
[121:41] Speaker 1: So, this is what it looks like but has three to five minute rounds, skill-based matchmaking, and it's a third-person shooter and you can play as different characters.
[121:54] Speaker 1: And I have a game mode called Gem Grab, which is specifically taken from the game.
[122:01] Speaker 1: So I can show a quick demo here of the matchmaking process.
[122:11] Speaker 1: So I have two different accounts here, I press find match and then it'll parry them up together.
[122:21] Speaker 1: And this is playing at 2X speed that you can see that the characters are grabbing the gems.
[122:34] Speaker 1: And the way this game works is that once one team reaches 10 gems, then a countdown starts.
[122:43] Speaker 1: And if the countdown finishes, then that team wins.
[122:48] Speaker 1: But if they die and lose those gems then the countdown resets.
[122:58] Speaker 1: So, as you can see here, I'm trying both different characters and you can see they're you can see they're moving on both of the browsers.
[123:16] Speaker 1: And the countdown has started, but then I'll kill them and the countdown resets.
[123:32] Speaker 1: So, this is made using 3GS.
[123:39] Speaker 1: I made the character models with um Blender MCP.
[123:48] Speaker 1: And I also rigged all the animations with Blender MCP.
[123:55] Speaker 1: And yeah, as you can see the blue team has won.
[124:01] Speaker 1: So yeah, that's my game.
[124:15 - 124:19] (Applause)
[124:23] Speaker 2: So, natural language to SQL presents a huge opportunity for anybody who wants to take it.
[124:31] Speaker 2: And this next group wants to take really hard to understand and get data from basketball stats and the NBA and put it in the hands of the exact people that need it, the analysts.
[124:46] Speaker 2: Presenting Cortex, Corey, Matt and Jordan.
[125:37] Speaker 3: Um, how's it going?
[125:39] Speaker 3: So for our project, we wanted to solve a real problem for real users.
[125:48] Speaker 3: So in that search, I was talking to a former colleague of mine who now works in NBA analytics and he expressed a common pain point that him and his peers experience, which is that for complex queries, they have to spend hours a day aggregating data across various sources and analyzing it just to answer a single query.
[126:13] Speaker 3: So out of that pain point, we decided to create Cortex, a natural language interface to advanced NBA data.
[126:25] Speaker 4: So we started by looking around in the marketplace to see if anyone was addressing this particular pain point and the best publicly available solution at the moment is Stat News.
[126:43] Speaker 4: You can see here, it's capable of answering a basic statistics question about LeBron from last year.
[126:53] Speaker 4: But this is something you could get off of Google or a quick chat GPT search.
[127:00] Speaker 4: If we use Cortex, we can bump up the complexity a little bit.
[127:07] Speaker 4: Ask something like what Warriors lineup had the highest plus minus over the 2022-23 regular season.
[127:31] Speaker 4: So while Cortex is thinking about the problem, I'll give a quick shout out to Daryl who owns the website PBP stats which services this niche community of data analysts.
[127:47] Speaker 4: He gave us read access to a legacy version of the database that has statistics going back to the year 2000.
[127:58] Speaker 4: Um, you can see that Cortex is having to think about the problem, process it, but on the other hand, even though latency is next to zero for Stat Muse, it doesn't understand even what we're asking.
[128:16] Speaker 4: Here we give back full response about the lineup, extra statistics just saved an analyst, you know, a couple hours out of their morning.
[128:29] Speaker 5: Uh, so that's the basic use case, but let's talk about design for just a moment.
[128:35] Speaker 5: Um, we naturally approached this problem from a very high level, uh, point of view.
[128:44] Speaker 5: We broke it down into three subsections: understanding the problem, and then, uh, uh, researching the problem in the database and answering the question.
[128:57] Speaker 5: Um, in the first step, we break the question down into smaller questions that could be found in a database.
[129:06] Speaker 5: Um, and then we pass it off to another agent that has the ability to go dig deep into the data to see how it's formatted, to figure out its internal relationships, um, and then to assert with certainty that it can find the data that we're looking for.
[129:28] Speaker 5: And then it passes all of the insights that it gained about the raw data to a separate code writing agent that's responsible for writing, um, executing and iterating on any possible things that can go wrong, any errors or implausible results.
[129:52] Speaker 5: Um, and then once it's able to justify its answer and gets a reasonable result, uh, it returns it to the user in natural language.
[130:02] Speaker 3: Yeah, so you can see we asked an even more complex question here, um, with a nod to expected value, which is a concept that analysts often need answers to.
[130:14] Speaker 3: And uh, Stat Muse has no concept of it at all.
[130:18] Speaker 3: It just gives a completely irrelevant response, uh, just listing the player from the season who had the most points, not the answer to the question.
[130:28] Speaker 3: Whereas Cortex, after it did some thinking, it found and provides an actually useful answer.
[130:37] Speaker 3: So, if we kind of zoom out, um, this is like a strong proof of concept that we think has good vertical scalability within the NBA world.
[130:48] Speaker 3: Um, NBA teams and analysts have use for this kind of thing.
[130:52] Speaker 3: But the interesting thing that emerged while we were building this is that the only reason while while we have some NBA uh and database specific logic in our pipeline, that was more of a function of the intense time constraint we were under than any capabilities.
[131:15] Speaker 3: And especially as AI capabilities increase, we see a clear path toward building a truly domain and database agnostic system that can just plug straight into any existing database and provide a natural language interface to answer complex queries about user data.
[131:41] Speaker 3: Thanks.
[131:43 - 131:45] (Applause)
[132:06] Speaker 2: So, if any of us have tried to read a research paper before, it's not that digestible.
[132:14] Speaker 2: Well, the next team has taken research and taken AI to make the insights coming out of a research paper something everybody can access and understand.
[132:26] Speaker 2: This is Danny and Lucas with Paper Mastery.
[132:31 - 132:34] (Applause)
[133:39] Speaker 6: Test.
[133:51] Speaker 7: Cool.
[133:52] Speaker 7: All right.
[133:53] Speaker 7: Uh, hi everyone.
[133:55] Speaker 7: I'm Danny, that's Lucas.
[133:58] Speaker 7: Uh today we'll be presenting Paper Mastery.
[134:02] Speaker 7: Uh Paper Mastery is a research learning platform that breaks down complex papers into digestible content and provides our users a path towards mastery.
[134:14] Speaker 7: Our goal is to make research approachable for everyone.
[134:21] Speaker 7: Picture this, you find a paper that with an interesting title but are left confused on what to do after reading the abstract section.
[134:33] Speaker 7: Wouldn't it be nice to have a

--- Segment 14 (Starting at 130:00) ---

[130:00] Speaker 1: tool to help guide you through this process.

[130:05] Speaker 1: Well, Paper Mastery does just that.
[130:10] Speaker 1: Uh, by utilizing various LLMs to create educational content and provide the user a starting point to their learning journey.

[130:22] Speaker 1: And if the learning material we've generated is not enough, we've even come up with solutions to help you go the extra mile.

[130:33] Speaker 1: Before I pass it off to Lucas to talk about that, a little bit of context on why we built this.
[130:41] Speaker 1: We have a special interest in this area from our own experience reading white papers and being frustrated when confirming whether or not we've achieved mastery on the subject.
[130:53] Speaker 1: We developed this tool with us in mind, but after scoping out the products in this space, we realized it could benefit far more people.

[131:10] Speaker 2: All right, so a little bit on the technical side.
[131:13] Speaker 2: Uh, you start by uploading a paper. You can do that through a PDF upload or you can do it through a journal link, archive link, or any other PDF URL.
[131:24] Speaker 2: After that, paper is going through our system.
[131:26] Speaker 2: We have a bunch of LLMs that generate a variety of learning content for you to actually understand the paper and test yourself on that.
[131:35] Speaker 2: And lastly, we've developed a pipeline that will reach out to paper authors and other experts in the field, uh, so that you can actually test with them whether or not you understand what you're talking about.

[131:50] Speaker 2: When we started looking into the space, uh, there were basically two existing products, Explain Paper and Scholarcy, both had really limited feature sets and paywalled very heavily.
[132:05] Speaker 2: Uh, so we were able to use much cheaper models and give away all of their features for basically free.
[132:13] Speaker 2: Uh, and the only thing that we might have to charge for is unlimited content regeneration and consultations with the researchers or experts in the network.

[132:43] Speaker 1: So why do we think this platform will work?
[132:47] Speaker 1: Uh, to start, it's mostly free.
[132:50] Speaker 1: And if that's not convincing enough, we think users will see our competitors are too expensive and lagging behind the latest AI developments.
[133:01] Speaker 1: We're also committed to matching our competitors bar for bar with any new features they might add in the future, as well as introducing our own.
[133:09] Speaker 1: For initial activity on the platform, in addition to hopefully everyone watching signing up, we tapped our network networks to provide users with high quality experts ready to help.
[133:22] Speaker 1: We're also targeting PhD students uh to start because as users, they need cheaper alternatives to our competitors.
[133:31] Speaker 1: And as experts on our platform, they'll have the opportunity to consult on research they already love and get paid for it.

[133:44] Speaker 1: Our platform doesn't just stop with PhD students, though.
[133:47] Speaker 1: We can expand to master students, undergrads, high school students, journal clubs, weekly digest, uh, pretty much virtually every corner that research touches.
[134:01] Speaker 1: Uh, we do this again with the goal of making research approachable for everyone.

[134:16] Speaker 2: All right, we're going to do a quick uh video demo.
[134:20] Speaker 2: Let's put this on full screen.
[134:22] Speaker 2: So when you log in, this is what the dashboard looks like.
[134:26] Speaker 2: You can see a graph of your research papers connected by various nodes that they both reference.
[134:32] Speaker 2: Paste in an archive link and we're going to pull the PDF, begin processing it.
[134:40] Speaker 2: We'll go into this page.
[134:41] Speaker 2: We're going to generate a bunch of different summaries at various complexity levels, so you can kind of ramp up on the paper smoothly.
[134:49] Speaker 2: And then once you finish each of these various learning sections, you mark them as done and we mark off some progress at the top of the page.
[134:56] Speaker 2: We uh generate queries and grab them grab YouTube videos that are going to be helpful for understanding things.
[135:05] Speaker 2: Make some quizzes so that you can test your knowledge immediately.

[135:13] Speaker 2: Flash cards are right here.
[135:15] Speaker 2: You can do them in the app or you can export to Anki and do your own uh space repetition.
[135:22] Speaker 2: If you're confused about anything in the paper, highlight it, hit explain or summarize, that'll send it to the chat interface and we'll explain it to you.

[135:37] Speaker 2: And then lastly is the consulting view.
[135:40] Speaker 2: So when you click book a session and you're a premium user, that'll kick off the pipeline and we'll try to find the researchers' uh contact info, reach out to them on your behalf.

[136:23] Speaker 3: There are millions of dollars every year that are given to researchers and professors to do research.
[136:31] Speaker 3: These are through grants that are provided by the government, but why is it so hard to apply to them?
[136:38] Speaker 3: So now we have Mark with Flumflow AI, who's going to make it easy for everyone to get money to do research.

[137:37] Speaker 4: All right, welcome to Flumflow AI.
[137:40] Speaker 4: Streamlining the grant application for maximum microphone.
[137:46] Speaker 4: Mark, Mark, use the mic.
[137:52] Speaker 4: Welcome to Flumflow AI, streamlining the grant application for maximum impact.
[138:06] Speaker 4: The total grant applications that were funded by the US government was just under 200 billion in the year 2024.
[138:16] Speaker 4: Grants enable critical research, essential services and educational innovation, driving advancement that would otherwise be unattainable.
[138:29] Speaker 4: They are vital for fueling progress.
[138:32] Speaker 4: The problem, grant writing, is time-consuming, sifting through guidelines, gathering data, and crafting a persuasive narrative demands significant effort.
[138:51] Speaker 4: Diverting resources from the core mission.
[138:56] Speaker 4: The solution is Flumflow AI, and the core of our application is a centralized document store that every applic every document that is uploaded to the grant application goes into it.
[139:09] Speaker 4: It's vectorized and available to all the assistance.
[139:14] Speaker 4: And also the shared thread.
[139:16] Speaker 4: That's a conversation thread that all the assistants use, so what you say to one assistant is available to the other assistants.
[139:26] Speaker 4: And then we have our three assistants.
[139:28] Speaker 4: The first research assistant will help you craft a stronger grant by asking you questions about your grant, and also answering questions about the grant.
[139:40] Speaker 4: And then we have the writing assistant that will write the grant, and the review assistant that will check it for spelling errors, logic errors, and other errors, and also check for compliance with the grant funding requirements.
[140:03] Speaker 4: How do we do this in 21 days?
[140:06] Speaker 4: We coded it with Claude and Cursor, uh, did 100% of the code base, did all the refactoring.
[140:13] Speaker 4: If we hit a problem that Claude couldn't solve, we use Gemini to get an answer and we were able to get past it.
[140:22] Speaker 4: And we use Grock to write all the documentation and the technical requirements.

[140:37] Speaker 4: So we'll go through a quick demo.

[140:57] Speaker 4: The first thing you do is you select your opportunity and enter in basic grant information, and then select your optional sections.
[141:11] Speaker 4: What you what it does when it generates the grant application is it will automatically attach to the grant application all the requirements that the funder has for that grant application.
[141:30] Speaker 4: Then what you can do is attach your own app uh documents.
[141:36] Speaker 4: In this case, I attached some research papers that support the thesis and I go out to the research assistant.
[141:44] Speaker 4: It asks me four questions, I give it answers and it will come back and ask more questions or answer the questions that I asked.
[141:54] Speaker 4: You can see on the right hand side that it has a history that you can review.
[142:00] Speaker 4: We will then generate our first pass of the grant and you can go in after that and review the sections uh where the text is.
[142:09] Speaker 4: Uh, we had a bit of an issue with the internet connection here, but it popped up uh and then if you don't like the content, you can edit it directly in the editor there or you can ask the AI to edit it.
[142:33] Speaker 4: And when it's done and you're satisfied with your application, you go out to the main page and you hit submit.
[142:43] Speaker 4: Or cancel.
[142:46] Speaker 4: In either case, uh submit will download your documents uh in word format to your to your drive, uh, so you can upload them to the grant application, uh, site on government grants.gov.
[143:06] Speaker 4: Uh, or cancel it and in either case, both your assistance, your thread, and your document store will be destroyed up on the open A server, Open AI servers so that it's not sitting there costing money.

[143:48] Speaker 5: What if I told you, what if I told you that you could write a book for just 15 minutes a day in just a year.
[143:59] Speaker 5: We think there's writers everywhere and with the next application unfold, you can just write your book in 15 minutes every day.
[144:12] Speaker 5: Here's Michael and Unfold.

--- Segment 15 (Starting at 140:00) ---

[140:00 - 141:45] (Silence)
[141:45] Speaker 1: All right. Hi, I'm Michael and I built Unfold. It's an AI writing assistant that uh doesn't do the writing for you, but it does help you complete all the non-writing parts of the process. And with this, you can complete a book by only working 15 minutes a day, so long as you can keep it up for a whole year.

The problem I'm trying to address here is that 80% of Americans have said at one point in their life that they'd like to write a book. They have a story in their head, they'd like to share it. But only 1% of those people will even attempt to try it, and of that 1%, only 3% will even finish their book.
Uh, my contention is, I don't think the best stories that uh people have heard are in the minds of professional authors. I think they might be in the heads of people who haven't even had a chance to try it yet. Uh, and so I built unfold to address this uh, basically the core impediments that are keeping people from putting pen on paper or hands on keyboard.

So, how Unfold solves this?

Uh
Yeah.
We uh basically the core issues keeping people from writing are uh issues with time. You get off work, you don't want to spend four hours writing a book. Motivation. I mean, once you uh once you start writing a book, how do you stay on it? Habits are notoriously hard to keep.
And then the structure. Like if you aren't a professional author, how would you even know how to start writing it to begin with? Uh, we address the time thing by sectioning off sessions into uh basically 15-minute writing sessions, much like how Duolingo would do their uh language learning process. Uh and we have AI in the background organizing everything for you.
Uh, motivation, we have scheduled notifications and uh progress tracking and achievements in app for all the work that you're doing. And then structurally, AI is in the background analyzing everything that uh you're feeding it so that uh it provides the guidance and uh doesn't do the writing for you, but will give you coaching and tips along the way.

So, how's this all work? Uh, basically, Unfold acts as our writing assistant. Uh, it keeps the state of the story in the background for you, uh uh in yeah, all of yeah, keeps track of all the associated entities that you're developing.
What you do is you dump in your brainstorms, your core ideas. Uh and what Unfold ends up doing is it takes all that content and forms an ontology of all of the story entities. Basically, the characters, the settings, and the plot points all get formed into models that uh get stored for the AI to be able to access efficiently in the future.
Uh next step, all of that information, those core entity models, are pulled in and and turned into outlines. Uh, that is outlines are structured and grow and grow and grow. Uh, in writing, you might call this the snowflake method.
Basically, this grows to the point uh to where all that's left is sort of the meat of the story, and this is the part that you write once your AI assistant has handled all of the background information for you. Uh, the meat of the story, you can call story beats. They're sort of the core entity of the story uh that you write. You might think of it as a paragraph, you might think of it as uh simply just a state uh any particular state change in the story.
Uh, Unfold basically sits in the background, provides you the background information that you've already fed it. It feeds you your ideas and then prompts you to write the next best part of the story.
Uh and then finally, it provides you uh basically a story map and progress updates so that you can keep track of everything you've been doing and keep you motivated to continue.

So, if you'd like to join Unfold and share your story, uh go ahead and scan the QR code here. I'd love to have you all and see what stories unfold.

I think everybody watching and everyone here at HQ has built a form before. And we all hate it. Drag and drop all these things, put it the form together. But the next team has a solution where AI does everything for you. This is Form Locker, Nicholas, Tim, and Wilson.

Bro.
Oops.
Test.
Okay, hey everyone. Uh my name is Nicholas and then we have Tim and Wilson. And the three of us we are Form Locker. So, Form Locker is an all-in-one uh form builder platform where we basically try to supercharge form building with AI.
So, Form Locker believes that forms suck. It's absolutely tedious for form creators to create forms. There's too much branching, too much conditionals and it's just way too much to control when forms are too big.
It's also boring for form responders because it's just too one dimensional. Like they don't want to answer too many questions, they don't want to go through five minutes just to answer a form.
81% of every responder actually drops out of the forms halfway through. So, why don't we just make AI do everything?
Why don't we just make an in between between a static form and a chatbot? Make it AI forms.
So, instead of making it too boring or make it too robotic like a chatbot, we thought of making it crafted personally for every responder using AI.
So, we think static forms is the basic level form, like a level zero. A more advanced one is the dynamic forms with the branching. We think the next one is actually AI assisted forms where where AI creates the questions for you, creates the flow for you. And finally, we have a product called Z forms.

And so with using Form Locker, building a traditional form is made super easy. With AI, a single prompt like this can turn into this. AI generate generates for you a pages pages of forms complete with input fields and relevant relevant information. Um, combine or what usually takes hours can now take seconds.
Combined with the drag and drop interface and configuration options, the forms are fully customizable. Um, you can add any input field like text, numbers, and signatures and files, whatever.

Yeah.
So, the second product that we offer is called Z forms. So, what is Z forms? Um, we believe that AI is better in humans at gathering user information. So, instead of creating the form questions yourselves, we allow you to define the exact information that you want to get from your users.
Then we let AI figure out how to best do that. So, the form creator only needs to provide some context and define the exact output schema that they want. And then the form respondent is able to enjoy a personalized and tailor-made form experience.
So, we have upcoming features like hybrid form models between AI assisting you creating questions and also the Z forms. We have more customization options coming up soon with like more casual tone or formal tone for AIs, analytics, multi-language support.

--- Segment 16 (Starting at 150:00) ---

[150:00] Speaker 1: and integration with other tools such as HubSpot, Google Sheets, etc. for your marketing needs.
[150:14] Speaker 1: So, why not we revolutionize forms with AI?
[150:20] Speaker 1: Come join Formlocker.
[150:24] Speaker 1: Thank you.

[150:33] Speaker 2: All right.
[150:36] Speaker 2: Next up, we have another gamer here at Gauntlet AI, who's been working on Monster Hunter and believes that the browser is the next big game engine.
[150:47] Speaker 2: This is Norman with Monster Hunter.
[150:53] (Applause)
[150:56] Speaker 3: Okay.
[150:57] Speaker 3: Okay, okay, right there.
[151:00] Speaker 3: Okay, okay.
[151:03] Speaker 3: Yeah, yeah, okay.

[151:11] Speaker 3: Connecting people all around the world, okay?
[151:16] Speaker 3: 7 billion people in the world, bring them all online and getting them together.
[151:24] Speaker 3: When I talk about this and ask people to think about a company that's associated with this slogan, the first thing that people think about, oh, is Facebook, right?
[151:35] Speaker 3: Bringing everyone together, connecting the world's people.
[151:43 - 151:48] (Silence)
[151:48] Speaker 3: When I think about connecting the world's people, I think about a game that I used to play when I was a little kid, okay?
[152:02] Speaker 3: When I think about connecting the world's people, I think about Club Penguin, okay?
[152:08] Speaker 3: A game that I used to love.

[152:12] Speaker 3: In the year 2025, nobody is on Facebook because they want to be on Facebook.
[152:20] Speaker 3: You're on Facebook because that's the way things have always been done, because you are afraid to miss someone's messages and because you're just simply trapped there.
[152:32] Speaker 3: Uh, nobody wants to be on a platform and hang out with, uh, people who might be robots, right?
[152:40] Speaker 3: People, uh, who might be hijacking your attention for malicious intent and a place that's frankly a wasteland that's filled with advertisements.
[152:54] Speaker 3: Believe it or not, people used to want to be on games like Club Penguin and Maple Story and World of Warcraft, okay?
[153:08] Speaker 3: Kids would come home and they would, every day they would come home and they would look forward to logging on and hanging out with their friends and doing cool things together with their friends online.
[153:22] Speaker 3: It was a big part of many people's lives, and it was a big part of my life too.
[153:26] Speaker 3: In fact, many people even found their husbands and wives on games like this, okay?
[153:34] Speaker 3: On games as humble, as you might think, as Club Penguin.
[153:40] Speaker 3: And the players loved the game so much that whenever one of them was shut down, bought out by a company or canceled, they loved the game so much that they would spin up their own private servers just so they could get the chance to play, okay?
[153:56] Speaker 3: And so I have a minute left.
[153:59] Speaker 3: And so, you all today are invited to uh, do an interactive demo.
[154:06] Speaker 3: So monsterhunter.io is the game that I'm releasing today.
[154:13] Speaker 3: Um, so everyone can log on and play together.
[154:20] Speaker 3: Uh, So this is an authentic, uh, real-time multiplayer game that runs in the browser, okay?

[154:43] Speaker 3: And, uh, there you go.
[154:47] Speaker 3: So here's two different players and everyone is uh invited to log on and play.
[154:56] Speaker 3: And so this is something that's been on my mind for a long time.
[155:01] Speaker 3: But we're also going to make it successful.
[155:05] Speaker 3: And the way to make it successful is the fact that this thing runs in the browser, everybody.
[155:11] Speaker 3: When you're in the browser, you have built in distribution and you can build it once and run it on iOS.
[155:19] Speaker 3: You can run it on Android, you can run it on Windows, you can run it on MacOS.
[155:24] Speaker 3: We're going to integrate Monster Hunter everywhere you can run a browser.
[155:28] Speaker 3: You're going to be able to play Monster Hunter and that's how we're going to build, right?
[155:33] Speaker 3: Something that people are actually going to love.
[155:37] Speaker 3: A place that people are going to want to be in and hang out with their friends with and do cool things online.
[155:45] Speaker 3: All right, Monster Hunter, everybody.
[155:48] Speaker 3: Thank you.

[156:01] Speaker 2: Next up, we have Peter Chen with Assemble.
[156:07] Speaker 2: Have you tried to build any deep hardware and had no idea where to start or what parts to buy?
[156:14] Speaker 2: Well, Peter's solving that problem with AI and his new app Assemble.
[156:21] (Applause)
[156:43] Speaker 4: Thanks Ash.
[156:45] Speaker 4: Uh, yeah, so I'm here to uh demo Assemble.
[156:51] Speaker 4: So Assemble is a uh AI powered web app that helps you select parts and create block diagrams for electronics.
[157:01] Speaker 4: Um, so a little bit of background.
[157:05] Speaker 4: There's been a few times, uh, in my life where I've wanted to create some hardware, something I thought was really cool.
[157:12] Speaker 4: Um, but I'd always get bogged down by like trying to look through the thousands and thousands of different components that are available to use.
[157:24] Speaker 4: And uh, yeah, it just made me really uncertain and uh kind of killed my momentum.
[157:32] Speaker 4: And um, after talking with some friends I've been in the industry for a pretty long time, um they say that it it can take a while even for experienced engineers and uh it can also get really expensive if you make a mistake.
[157:46] Speaker 4: Um, so with Assemble, hopefully uh the process can become much faster and much less error prone.
[157:55] Speaker 4: Um, yeah, so let's dive into the demo.

[158:00] Speaker 4: Um, so I'll just create a new project.
[158:12] Speaker 4: Yeah, so when you first open Assemble, it'll ask you uh what you want to build.
[158:19] Speaker 4: Uh, and then so I'll just put, let's say we want to build a smartwatch.
[158:30] Speaker 4: After you put that in, it'll use AI to generate additional follow up questions to really dial in what you're looking for.
[158:38] Speaker 4: Um, just for the sake of time, I'm not going to go through this whole flow.
[158:42] Speaker 4: Um, I'm going to skip it for now.
[158:45] Speaker 4: Um, and then you land in kind of the main interface.
[158:50] Speaker 4: Um, so you can manually add a part, uh, like a microcontroller.
[158:59] Speaker 4: It is much harder to type with one hand.
[159:02] Speaker 4: Uh, and you can just drag and drop your component onto your block diagram, kind of like lucid chart.
[159:09] Speaker 4: Um, but you can also ask AI to help you out.
[159:12] Speaker 4: So I can ask it to find me
[159:34] Speaker 4: Uh and then it'll start responding and it'll start looking stuff up.
[159:39] Speaker 4: Uh, And for the sake of time, I'm just going to jump over to uh another project where this conversation has already played out.
[159:51] Speaker 4: Um, I'm going to kind of show you what the AI can do.
[159:54] Speaker 4: Um, so it'll look up different parts for you.
[159:58] Speaker 4: Um, it'll give you the specs, it'll give you uh the link to the product page so you can look at it with more detail if you wanted to.
[160:08] Speaker 4: Um, out of the all the products that it looks up, uh, it'll make suggestions and pick the best one for uh what you're trying to build.
[160:17] Speaker 4: Uh, it'll allow you to add these products um with all this data, uh, into your projects.

[160:33] Speaker 4: And it'll also update your block diagram to actually contain these and it'll make the connections between the different interfaces that uh need to be connected.
[160:45] Speaker 4: Um, and you can always, you can also use it just like any other AI agent.
[160:49] Speaker 4: You can ask it questions, um, and you can, you know, ask it for updates as well.
[160:55] Speaker 4: Say like the microcontroller is too expensive, you want something cheaper.
[161:00] Speaker 4: It'll look up more stuff for you and it'll generate a table.
[161:15] Speaker 4: Um, and then it'll it can update your parts again.
[161:22] Speaker 4: Um, and then after you're done with all that, it'll give you a nice summary of uh everything that you're using, the price and the availability.
[161:30] Speaker 4: So you can go ahead and just go to the vendor, uh distributor in this case Digi Key and go buy it.
[161:38] Speaker 4: Um, and yeah, that's uh Assemble for now.
[161:42] Speaker 4: Uh it's still in the early stages.
[161:46] Speaker 4: Still a lot of features that we have planned such as exporting this, uh, so that you can import it into CAD software and you can build on top of it.
[161:56] Speaker 4: Uh, and yeah, and here's a QR code for the wait list and we'll let you know when it's ready to launch.
[162:05] (Applause)

[162:31] Speaker 2: All right, so over 13,000 people have joined us on this stream throughout the entire day.
[162:38] Speaker 2: But if you haven't gone to demo day.gauntletai.com and checked out all of our challengers, all of the projects that they've worked on so far or even a little bit of a recap, feel free to go there right now and make an account.
[162:54] Speaker 2: For our next team, we have Hero Hurder.
[162:59] Speaker 2: We have old school game engines like Unity being combined with new school tools and AI.
[163:08] Speaker 2: This is Brian and Riley with Hero Hurder.

--- Segment 17 (Starting at 160:00) ---

[160:00 - 161:45] (Silence)
[161:45] Speaker 1: Hello.
[161:46] Speaker 1: My name is Raley Bird.
[161:50] Speaker 1: My partner here.
[161:51] Speaker 2: I'm Brian Jensen.
[161:55] Speaker 1: And what we what we can is is a small experiment to see how older systems such as Unity might still work with newer systems such as Cursor.
[162:08] Speaker 1: So that they together can make game development perhaps reach the proverbial 10 times faster.
[162:20] Speaker 1: Uh, we decided that it'd be best if we just tried to make a game with them.
[162:38] Speaker 1: Originally, the idea was perhaps a little smaller than now.
[162:46] Speaker 1: But we looked around, there are plenty of powerful, well-established game engines, but few are meaningfully integrated with with with their AI tools that are available nowadays.
[163:03] Speaker 1: Some of them, such as Unity, have a handful, uh, but often they're not a direct part of the engine and you have to go hunt them down.
[163:18] Speaker 1: While some progress is being made, it often feels like the innovation of the AI tools is lagging behind their potential.
[163:34] Speaker 1: Part of the issue right now is that AI tools are often siloed with no standard way to connect them to engines or pipelines.
[163:47] Speaker 1: Teams must rely on work arounds and custom solutions which slows progress and limits creative potential.
[163:55] Speaker 1: Many game companies aren't even taking the time to learn how to make the new tools work with their old, simply because they have so many custom ones that that are all in-house and they can't be bothered to take the time because that would slow down development of their next big project and take too much money.
[164:17] Speaker 1: So we took matter into our own hands.
[164:19] Speaker 1: We said to test how modern AI could enhance classic engines by building a game with them.
[164:26] Speaker 1: As mentioned, Unity was the engine we decided to make the game in.
[164:31] Speaker 1: Chat GPT helped us brainstorm.
[164:34] Speaker 1: Cursor helped us to code, debug and streamline collaboration.
[164:40] Speaker 1: Dolly rapidly generate visual concepts and replacements for any assets we couldn't find online.
[164:48] Speaker 1: Combining the old tools with the new ones, created momentum.
[164:52] Speaker 1: Development was faster, creative decisions came easier, and pretty much tasks were reduced.
[165:00] Speaker 1: Giving the AI tools the context of our of our unity setup and our game design dock helped them to suggest actionable steps for implementation in the spots where they weren't themselves able to make the code or the items or the objects, et cetera.
[165:25] Speaker 1: The result, a working game that shows that when you stop waiting and start building, you too can make things.
[165:34] Speaker 1: This wasn't just about making a game, it was a test whose results show that AI could supercharge even legacy systems.
[165:44] Speaker 2: So as mentioned our nice title screen here was initially designed by Dolly and then cleaned up by other AI tools to get all those fancy artifacts that it loves putting in everything out.
[166:00] Speaker 2: Um, this background was initially generated by Grock and then fed into a Pixel diffusion engine.
[166:12] Speaker 2: And uh, a lot of our assets here like that nice sky background and these adventurer sprites all came from a free uh, free to use collection made by this very nice Japanese guy who just does that all day I suppose.
[166:40] Speaker 2: And um, yeah in the in the in the age of AI the value of free assets has really gone up quite a bit as the cost of software development is rapidly dropping.
[167:00] Speaker 2: And there's our game where you possibly get burned by a dragon.
[167:05] Speaker 1: The game itself, uh, is Hero Herder.
[167:10] Speaker 1: Play as a as the guild manager of a new found adventures guild and your job is to rebuild the guild, hire new adventures, make the buildings to give you more abilities and a possible build a team that can in fact slay the dragon lest it do the most terrible thing imaginable, come down and burn you to a crisp.
[167:45] Speaker 1: Again, my name is Raley Bird.
[167:47] Speaker 2: I'm Brian Jensen.
[167:48] Speaker 1: And we made Hero Herder.
[168:02 - 168:38] (Silence)
[168:38] Speaker 3: Now, before when we wanted to make a game, we needed a bunch of developers, we needed a bunch of money.
[168:51] Speaker 3: We needed all these other things than just our imagination.
[168:56] Speaker 3: But now with dreamup.gg, you can make a game in just one minute and bring your imagination to life.
[169:07] Speaker 3: Presenting Sebastian, Matt, Robert, Andre, and Chris.
[169:16] Speaker 3: This is dream up.
[169:20] Audience: Woo!
[169:20 - 172:29] (Silence)
[172:29] Speaker 4: Check, check.
[172:34] Speaker 4: All right, hello everyone.
[172:37] Speaker 4: We're the team behind dream up, the Replit for games.
[172:43] Speaker 4: We call ourselves the dream team.
[172:47] Speaker 5: I'm Rob.
[172:51] Speaker 6: I'm Matt.
[172:53] Speaker 7: Sebastian.
[172:58] Speaker 8: I'm Chris.
[173:00] Speaker 9: I'm Andre and I was going to build a live game for you guys, but actually after this lunch's speed build, I'm going to build two live games.
[173:19] Speaker 4: Okay.
[173:21] Speaker 4: All right, so everyone has an idea for a game.
[173:25] Speaker 4: Uh as kind of went through this, but I'll go through it again.
[173:35] Speaker 4: Um, but in the past, you couldn't just go and build it.
[173:40] Speaker 4: You had to spend all kinds of time uh learning skills like game development, uh going to school for years, or hiring expensive teams.
[173:55] Speaker 4: But now all you need is your imagination.
[174:02] Speaker 4: So we built dream up to allow you to go onto our site, put in your wildest crazy idea for a game, and watch as AI builds it for you in a minute or two.
[174:19] Speaker 6: So once you describe your game in text or voice, our game development agent will build it for you.
[174:26] Speaker 6: Uh it will plan out the rules if there's any ambiguity, pull in relevant assets and write the game code for you.
[174:34] Speaker 6: Uh if the result is not what you envisioned, you can edit your game with further requests and our game agent is based on the latest Cloud 3.7 Sonnet, which makes far more impressive games than any model that came before it.
[174:51] Speaker 9: As we instead of just showing things or describing things in abstract, we decided to have this to demo live.
[175:04] Speaker 9: I can create the game by typing.
[175:11] Speaker 9: And while you wait, we have some featured games to to pass time.
[175:18] Speaker 9: The other day, I thought, you know what?
[175:21] Speaker 9: I want a retro game for educational content.
[175:27] Speaker 9: So we created fraction invaders.
[175:32] Speaker 9: And you can play around.
[175:36] Speaker 9: The cool thing here is once you played and you thought this was nice, maybe you can share with a friend and you can keep editing it.
[175:50] Speaker 9: So I have a bunch of edits.
[175:53] Speaker 9: I shared with my friend Matt.
[175:56] Speaker 9: He decided to play a little more and make it a little nicer looking.
[176:04] Speaker 9: So he went and did a couple edits and had a better ship.
[176:18] Speaker 9: Now, one of the things we can do is click here, share with my friend Chris as well.
[176:25] Speaker 9: So Chris, why don't you play it out?
[176:29] Speaker 8: All right, thanks Andre.
[176:32] Speaker 8: And with his link, I'm able to jump right in to play the game and I could play and edit the game's himself, but I already know how to reduce fractions.
[176:46] Speaker 8: So I wanted to make something like a cross up between Golden Eye 007 but with ninjas.
[176:57] Speaker 8: So I was able to come up with that and get a pretty good game that I uh I was pretty happy with right off the bat, but I had some suggestions, uh tweaks, improvements that I want to make.
[177:15] Speaker 8: Uh, so I was able to keep chatting.
[177:20] Speaker 8: Uh, and the game editing ontology we use is to send up some of the previous conversation, the last version of the game code and the user's new query.
[177:36] Speaker 8: And that goes up to our agent, which is able to pull out outside libraries, like in this instance, 3js uh for 3D rendering for a 3D shooter, cannon JS for physics.
[177:52] Speaker 8: And but I didn't need to know any of that.
[177:54] Speaker 8: I just needed to keep chatting in order to create the game that I envisioned.
[178:00] Speaker 10: Since our launching, we have more than 100 weekly active users and more more than 1,000 games were developed in our platform.
[178:13] Speaker 10: Now, we're launching our premium features, which is for $29 a month, you can have more prompts, you can have better assets, and we develop a AI agent that transform any single player game to a multiplayer game.
[178:33] Speaker 10: So Chris, if you mind, can we play Golden Eye together?
[178:39] Speaker 8: Yeah, absolutely absolutely.
[178:42] Speaker 8: Let's jump in.
[179:01] Speaker 7: Sebastian, where are you at?
[179:09] Speaker 7: Oh, I see you.
[179:12] Speaker 7: Oh you do?
[179:13] Speaker 7: Where are you?
[179:14] Speaker 7: Are you shooting me?
[179:16] Speaker 7: I'm behind you.
[179:19] Speaker 7: Ah, there you are.

--- Segment 18 (Starting at 170:00) ---

[170:00 - 170:03] (Silence)
[170:03] Speaker 1: Cool.
[170:05] Speaker 1: And our games are ready.
[170:11] Speaker 1: So, enchanted flight you can choose an owl, a butterfly, a dragon, or a parrot and start flying.
[170:21] Speaker 1: And if you guys want to play cars against friends, you can just go to dreamhub.gg or scan this QR code.
[170:33] Speaker 1: Thank you.
[170:33] Speaker 2: Thank you.
[170:39 - 170:47] (Silence)
[170:47] Speaker 3: For our next challenger.
[170:51] Speaker 3: He believes that software is being democratized.
[170:55] Speaker 3: If there's a SAS out there, you can just rebuild it with AI.
[170:59] Speaker 3: That's why he's created Assassins and he's going after Docusign.
[171:03] Speaker 3: This is Robert Cardwell.
[171:07 - 171:13] (Silence)
[171:13] Speaker 4: Uh, no.
[171:14 - 171:17] (Silence)
[171:17] Speaker 4: Yeah.
[171:17] Speaker 4: Uh, please work.
[171:23] Speaker 4: Uh, do we have signal?
[171:32 - 171:35] (Silence)
[171:35] Speaker 4: Um, there we go.
[171:36] Speaker 4: Okay.
[171:37] Speaker 4: Uh, very good.
[171:38] Speaker 4: So, uh, the assassins, I'm going to kill DocuSign and uh, and others and you're going to make a killing.
[171:54] Speaker 4: So, this is what the world would look like if people had never decided that you could charge $10 a month simply to sign some documents online.
[172:06] Speaker 4: Um, AI has destroyed everyone's Moat and this is not just a pitch.
[172:17] Speaker 4: This is also a, uh, financial, not financial advice.
[172:22] Speaker 4: So, people are not pricing in Gauntlet.
[172:28] Speaker 4: Consider how often people are surprised when Austin comes over and he does his demos and everyone's like, oh, I want to hire gauntlet students.
[172:39] Speaker 4: So the market has figured out that AI is a thing.
[172:43] Speaker 4: It has not figured out how it is a thing.
[172:47] Speaker 4: Um, Now there, DocuSign is a particular example because a, their product has way too many people, and b, it sucks.
[173:01] Speaker 4: So, I'm not the only one who's doing this, but uh, this is actually a good thing because it's essentially more, think of it as more shots on goal.
[173:14] Speaker 4: The more free products there are, the more people are going to be like, oh, the price that I should be paying is zero dollars, and they're not going to pay for signing their documents.
[173:32] Speaker 4: And my plan is essentially to make a big, splashy public launch with this, um, ideally get investment, which I can use to advertise because advertising is is fierce, um, post to post to Reddit, post to Twitter, uh, Wall Street bets, basically just really get people thinking like, oh, the price for signing a document should be zero dollars.
[174:09] Speaker 4: Um, Turns out it's actually really easy to build DocuSign.
[174:16] Speaker 4: This is what it looks like after about 2 hours.
[174:19] Speaker 4: You have the basics.
[174:21] Speaker 4: Um, this is what it looks like after about five days.
[174:28] Speaker 4: We have a little bit more than the basics, um, but still like a nice clean interface.
[174:36] Speaker 4: And this is what it looks like after 10 years of zero interest rates.
[174:49] Speaker 4: The DocuSign, if you go to click, like the very first thing you see is the login screen.
[174:57] Speaker 4: It's bad.
[174:58] Speaker 4: I don't need to explain more how it's bad.
[175:00] Speaker 4: Um, so my quick search found like three different people who have made perfectly good products, all of which are pretty similar.
[175:12] Speaker 4: Um, mostly they're not getting a huge amount of attention.
[175:16] Speaker 4: This is not something, this is not the first um thing that you're going to go after when you have the power of AI.
[175:26] Speaker 4: Like, first you're going to build something that makes you money and then well, everything will have been have been built already, so you make something that'll make you a little bit less money.
[175:40] Speaker 4: Um, Essentially what I'm looking for is investment or people who wanted to coordinate this with me, uh, essentially publicity blitz, you can short the stock.
[175:55] Speaker 4: Um, you can get people very scared because it's a new thesis and then you can cover.
[176:03] Speaker 4: Uh I leave you with this meme and I leave you with my uh contact info and where you can actually go and see the demo website I have up.
[176:17] Speaker 4: All right.
[176:17] Speaker 5: Yeah.
[176:23 - 176:27] (Silence)
[176:27] Speaker 3: Now, if you want to start playing Dota, there was a steep learning curve.
[176:33] Speaker 3: But with the new Dota 2 draft assistant and Ryan's new application, you can start learning now and get better at drafting.
[176:41] Speaker 3: This is Ryan Harron.
[176:43 - 177:06] (Silence)
[177:06] Speaker 6: Howdy.
[177:08] Speaker 6: I am a degenerate gamer and I play Dota 2.
[177:15] Speaker 6: That's my favorite game.
[177:17] Speaker 6: Dota 2 is one of the most popular games right now.
[177:21] Speaker 6: It's got about 600,000 average viewers a day, uh, players, excuse me.
[177:29] Speaker 6: It's got several tens of millions of active players throughout a month.
[177:35] Speaker 6: Uh, but the unfortunate thing about Dota is that it's very complicated.
[177:39] Speaker 6: It's got a very strict rule set.
[177:42] Speaker 6: You can do a lot of things in it and there's 126 unique characters that all interact with each other in a rock paper scissor sort of format, which means that one hero will counter another while in turn countered by a third.
[178:02] Speaker 6: When I started playing Dota, I got very quickly into the competitive aspect of the game, which involves drafting at the start of every match where you would pick and ban heroes to create your perfect team against the enemy.
[178:20] Speaker 6: You do this with knowledge of what the enemy plays and what you play, so they would also know what you play and it would complicate things in terms of mains, counters, synergies, and so on and so forth.
[178:35] Speaker 6: And I sucked at drafting.
[178:40] Speaker 6: I was terrible at it because I didn't know anything about the game and you have to know far too much.
[178:51] Speaker 6: When I wanted to practice my drafting, I had no one to practice with.
[178:56] Speaker 6: Other captains didn't want to do it because they would be teaching me things that would help me to beat them.
[179:05] Speaker 6: So why not make a robot that does it better than them anyway and practice against that.
[179:11] Speaker 6: I have here the Dota 2 Draft assistant.
[179:17] Speaker 6: As you can see, you can start a draft at any point in time and you will be recommended picks and bans as well as explanations for why you should pick and ban certain heroes based on the meta of the professional games of Dota 2.
[179:37] Speaker 6: So the top level games.
[179:39] Speaker 6: You can also choose to select the opponent's level, which means that you can choose to pull in additional information from a certain rank of the, uh, opponent to better focus against who you'd be playing against.
[179:59] Speaker 6: So you still have the professional meta, but now you're playing against people from your own rank and you're drafting against them too.
[180:13] Speaker 6: The other things that this app includes are a player profile to track your games.
[180:25] Speaker 6: It will track them in real time, updating once every hour, as well as a live assistant, which is currently in development.
[180:41] Speaker 6: With this app, I hope to level up the uh, the drafting game of both professional and amateur teams as well as to teach players how to draft using the recommended picks and bans and analyses.
[181:06] Speaker 6: Thank you.
[181:11 - 181:16] (Silence)
[181:16] Speaker 3: All right, next up we have Shahiir Ahmad, who has his email agent and uses browser use envision for task management.
[181:25] Speaker 3: Here's Shahiir.

--- Segment 19 (Starting at 180:00) ---

[180:00-180:02] (Silence)
[180:02] Speaker 1: Hey there everyone, my name's here.
[180:04] Speaker 2: Thank you. I'll I will use the microphone.
[180:08] Speaker 1: Sorry.
[180:09] Speaker 2: Hey there, my name is here, Ahmed, and this is email agent.
[180:14] Speaker 1: So, currently, a lot of LLMs are, you know, they're trained on a ton of internet data, right?
[180:20] They have the entire corpus of the internet, a ton of text.
[180:23] Next token prediction has allowed them to make huge strides in intelligence and break through the scaling wall, right?
[180:29] Um, however, when it comes to actually making, you know, action taking actions within the real world, they may not necessarily have the context or knowledge of the abstractions that we take for granted on a day-to-day basis.
[180:42] For example, when you're training an LLM, um, something like GPT 3.5 or four, it's not looking at images of the of a website, it's looking at just the code of the website.
[180:52] So then when you're trying to have it access the web, it's not going to have nearly the same performance as when you ask it to write a PhD paper, for example.
[181:00] And so if we truly want agents to participate in the global economy, they need to interface at least for some period of time with the same systems that we do on a day-to-day basis.
[181:09] An email agent is the perfect way to start testing out this application because we can have our agent take actions for us and actually provide value.
[181:22] So, over here, I decided to use browser use an open source library and have it take actions on my email account, so it could send emails, summarize content from emails, and also unsubscribe from emails.
[181:34] So, as you can see over here, I did have to give it my login information, so if you're not running this on local host, you might have to think about your privacy concerns, but as you can see over here, it's accessing the Google login page.
[181:50] It puts in the information and then it's going to look through my email.
[181:56] Um, as you can see, right, it's has to scan through the page and get a better understanding, hey, what's going on here.
[182:03] And then it's looking for keywords to understand what to do.
[182:08] And so, as you can see here, I'm asking my friend uh about that thing I sent him.
[182:14] Um, There are also multiple other examples.
[182:18] Uh, for example, there's this summarization process where it has to look through my email and then um look uh look for certain senders and then summarize the information.
[182:32] Right here.
[182:33] So, this would be perfect, let's say if you if you have a ton of articles from substack in your inbox and you want to get a better understanding of what you're missing out.
[182:43] And then this is the one one of my favorite examples is unsubscribing from emails, right?
[182:50] So, currently, you know, what do you call it?
[182:52] This can be a huge concern for a lot of people.
[182:55] You know, we have a ton of spam, right?
[182:58] And let's say you're tired of getting Groupon emails, you could have your agent do all the work of unsubscribing for you.
[183:07] Um, And yeah, as you can see here, it finds the unsubscribe button and is able to unsubscribe.
[183:16] So, if we want agents to participate in the economy, right, and actually take actions outside of like an MCP circle MCP server with predefined, you know, context and actions, then they need to start interfacing with the web and, you know, maybe even use some of the tools that we're creating here at Coled AI.
[183:36] And thank you.
[183:37] That's all I got.
[183:46-184:04] (Silence)
[184:04] Speaker 3: All right.
[184:06] Next up, we have a team that is changing the way that we test software engineering.
[184:13] Now, with the advent of AI, our traditional software engineering tests aren't going to work anymore.
[184:21] People use AI to solve all those algorithms instantly.
[184:25] So this is Vibe Decoder with Spencer and Tim.
[184:34-184:38] (Silence)
[184:38] Speaker 4: Use this.
[184:38] Speaker 5: Sure.
[184:39] Speaker 4: Use this.
[184:45-184:55] (Silence)
[184:55] Speaker 6: Yo, y'all, my name is Tim.
[184:57] This is my beautiful partner, Spencer here, and coming at you live from Austin, Texas.
[185:02] We're here to introduce our solution, Vibe Decoder.
[185:07] But before we talk about our solution, we need to address the problem in the room.
[185:12] And the problem is twofold.
[185:16] The current technical evaluation process for hiring a software engineer is both broken and obsolete.
[185:24] Well, why is it broken?
[185:27] Well, currently, companies hiring software engineers typically give home a technical aptitude test that involves either a take-home exam or a hacker rank challenge.
[185:39] Both prohibit the usage of AI.
[185:42] Inevitably, people cheat on those exams by either memorizing the algorithm for hacker rank or using AI on the side.
[185:50] And why is it obsolete?
[185:52] Well, I think we're all an attestation to the fact that building software today looks radically different than it did a year ago.
[186:03] And we believe, our thesis is, the individual's ability to orchestrate a result or set of results using one or more AI tools is going to be the predominant um skill that's going to be very important in hiring software engineers.
[186:18] With that being said, we're happy to introduce Vibe Decoder, which is a way that we reimagine what the technical hiring process for an engineer looks like.
[186:27] Instead of discouraging or prohibiting AI usage, we allow you to do it however you see fit.
[186:35] The participant opens up the the practice exam.
[186:43] They're given a problem and we collect video and we collect audio and we have a live AI voice agent that not only passes down the requirements of the assignment, but also ask engaging questions because fundamentally we believe that collecting and extracting how the user thinks while they're orchestrating a result with AI tools is more just as important as them successfully completing the exam itself.
[187:14] We want to extract their mental model.
[187:17] We want to identify their brain their decision-making tree while they make decisions.
[187:21] What tools do they use?
[187:23] We believe that this all helps us collect data to make a candidate viable for hiring partners.
[187:29] And with that, I pass it off to my beautiful partner, Spencer.
[187:34] Speaker 7: Thank you.
[187:36] Next slide.
[187:40] Here's what it looks like for the candidate or the test taker.
[187:44] The first time they log into the test, it asks you to share your screen and share your audio.
[187:51] That's because it's very different from other coding tests.
[187:56] We're going to use AI to analyze your entire screen.
[188:04] Once you're into the test, you see the rules, you're allowed to do pretty much anything you want.
[188:09] You can use AI, you can use documentation.
[188:13] The only thing you cannot do is talk to other humans.
[188:17] And then we give you the task and this is the task that was given to people.
[188:26] Now, here's what it looks like on the hiring manager side.
[188:29] We gave this test to 10 Galet litterers to see who is the best at using AI.
[188:37] We have the results scored from top to bottom and you also see that we have a transcript at the bottom.
[188:45] If the hiring manager would like to see the entire video, that option is available as well.
[188:54] Now, if we click the drop down for AI usage, this shows you why the score was assigned and also evidence supporting that score.
[189:04] We wanted to make sure that we were assigning accurate scores, so this is critical information for the hiring manager.
[189:11] They need to know the why and they need to know the evidence.
[189:15] That can give them the confidence that the score is correct.
[189:21] So, how did we get these scores?
[189:23] And how are we using AI in this application?
[189:27] The first step is that we pass the video to Gemini and generate that entire transcript that you saw on the last slide.
[189:36] The second step is that we have more prompts to actually assign the grade.
[189:41] Each company may have their own preferences or their own ontology of what the grade should be, so we allow them to customize the rubric.
[189:53] With all that being said, we hope that this app helps company helps companies decode who are the best vibe coders.
[189:59] That's it.
[190:00] Yeehaw, baby.
[190:01] Galet 2025.
[190:04-190:15] (Silence)
[190:15] Speaker 8: All right, AI gives us the opportunity to create personalized learning aids and education for all of our children around the world.
[190:26] What if we could supercharge reading fluency with AI?
[190:31] That's exactly what Suman built.
[190:33] This is Reading Beast.
[190:36-190:39] (Silence)
[190:39] Speaker 9: You should have to take out the Speaker 9: Uh screen.
[190:58] Speaker 10: your screen.

--- Segment 20 (Starting at 190:00) ---

[190:00] Speaker 1: Okay, this is what the child sees.
[190:05] Speaker 1: Sorry guys, see?
[190:10] Speaker 1: The audio is not working.
[190:13] Speaker 1: So this is audio.
[190:15] Speaker 1: Here is the library which consists of a collection of AI generated short stories.
[190:22] Speaker 1: Just want to make sure the audio is working.
[190:25] Speaker 1: These short stories can be filtered by various levels and then you have to change it to the speakers in which we're going to dial.
[190:30] Speaker 1: The child then picks the one that would like to read.
[190:33] Speaker 1: Where is it at?
[190:35] Speaker 1: The task is to read the passage aloud one minute.
[190:38] Speaker 1: Is it working?
[190:39] Speaker 1: And AI will analyze the reading.
[190:40] Speaker 1: Okay, wait a second.
[190:42] Speaker 1: Thank you.
[190:42] Speaker 2: Space Adventure.
[190:44] Speaker 2: Maya wanted to be an astronaut.

[190:57] Speaker 1: Sorry just you can use a mirror now instead.
[191:18] Speaker 1: He's right here.
[191:20] Speaker 1: I don't see it there.
[191:22] Speaker 1: I'll put a new one right there.
[191:27] Speaker 1: Okay, hi everyone.
[191:29] Speaker 1: This is Suman here and I want to introduce you to reading Beast.
[191:34] Speaker 1: Reading Beast is an AI tool that helps your children to improve in their reading fluency.

[191:40] Speaker 1: So this is the problem.
[191:42] Speaker 1: One recent study showed that 32% of fourth graders are reading below their reading proficiency.
[191:52] Speaker 1: This means that you know, they have problems, the poor reading skills have problems like academic setbacks, social isolation, and an emotional challenges.
[192:07] Speaker 1: So I built a solution, which is reading Beast and this is uses AI, advanced AI to assess reading abilities and it's gamified in such a way that the children find the practice as not as a chore, but as a fun quest to obtain badges and points.
[192:32] Speaker 1: So I prepared a pre-recorded demo for you so that it's all going to be easy for you.
[192:38] Speaker 1: I hope the audio works.

[192:56] Speaker 1: Okay, this is what the child sees after they log in.
[193:01] Speaker 1: And here is the library which consists of a collection of AI generated short stories.
[193:14] Speaker 1: These short stories can be filtered by various levels.
[193:24] Speaker 1: The child then picks the one that he would like to read.
[193:27] Speaker 1: Let's pick this one.
[193:35] Speaker 1: The task is to read the passage aloud for one minute and AI will analyze the reading.
[193:45] Speaker 2: Space Adventure.
[193:47] Speaker 2: Maya wanted to be an astronaut.
[193:50] Speaker 2: She read books about planets and stars every night.
[193:56] Speaker 2: Her room was decorated with pictures of space.
[194:02] Speaker 1: This is my son reading the passage.
[194:11] Speaker 2: We couldn't wait to explore space someday.
[194:20] Speaker 1: And here are his results.
[194:24] Speaker 1: So here are the results generated by the AI.
[194:29] Speaker 1: On the main screen, you get to see the number of words that are correct per minute, reading accuracy, areas of strength, and areas that needs some work along with the errors.
[194:46] Speaker 1: In the next step, you see a more detailed analysis, including fluency, prosity, confidence.
[194:58] Speaker 1: And on this step the user going to click on the words to practice.
[195:05] Speaker 2: Museum.
[195:09] Speaker 2: Exercise.
[195:17] Speaker 1: They can also practice interactively with the AI.
[195:30] Speaker 1: So the child is encouraged to practice, we take the test and improve the score.
[195:40] Speaker 1: The child's reading fluency is tracked over time here.
[195:48] Speaker 1: By month, all time, this week, the profiles tab show all the badges that the child has collected over time and what is still left to be unlocked.

[196:15] Speaker 1: Okay, so finishing off this product has a immense market opportunity with 90,000 elementary schools in the United States and 24 million K through 5 students and an edtech market that is rapidly growing.
[196:41] Speaker 1: So just to finish off, I would like you to invite to join and use reading Beast and to transform the reading experience for children.
[196:52] Speaker 1: Thank you very much.

[197:08] Speaker 1: There are thousands of people every day using chat, GPT and cloud to combat shyness and nervousness.
[197:18] Speaker 1: This next challenger built shy chat, which lets you practice and get better at public speaking and different events that might make you shy.
[197:28] Speaker 1: This is Tom Jacob.

[198:45 - 198:56] (Silence)

[199:13] Speaker 1: Oh, uh, it's okay.
[199:15] Speaker 1: Uh, I can just drag it in.
[199:22] Speaker 1: Yeah.
[199:30] Speaker 1: Oh, it's live.
[199:32] Speaker 1: Okay.
[199:33] Speaker 1: Yeah.
[199:35] Speaker 1: Slide show.
[199:40] Speaker 1: Oh, nice.
[199:43] Speaker 1: Oh, okay.
[199:44] Speaker 1: Uh, yeah, let's see.
[199:46] Speaker 1: I don't know how.
[199:49] Speaker 1: Ah.
[199:50] Speaker 1: Okay, let's see.
[199:52] Speaker 1: Yeah.

[199:56] Speaker 1: Uh, yeah, I see it.
[199:57] Speaker 1: All right.
[199:58] Speaker 1: Thank you, thank you.

[200:11] Speaker 1: I got you.

[200:37] Speaker 1: Yeah.
[200:39] Speaker 1: Okay.
[200:40] Speaker 1: Hello everyone.
[200:42] Speaker 1: Uh, thank you for coming today.
[200:45] Speaker 1: All right.
[200:45] Speaker 1: So, have you ever felt nervous or unsure about how to start a conversation or found yourself creating awkward pauses?
[200:54] Speaker 1: You're not alone.
[200:55] Speaker 1: Today, many teens and young adults are having trouble being social, they're becoming less social.
[201:01] Speaker 1: They're experiencing higher levels of social anxiety than ever before.
[201:06] Speaker 1: So, you can see from this graph here, right?
[201:10] Speaker 1: Teens are becoming less social, they're hanging out with friends less and less, right?
[201:14] Speaker 1: They're alone, they're on their devices.
[201:18] Speaker 1: You can see the outcome as well.
[201:20] Speaker 1: Anxiety is going up and it's the highest among the youngest generation.
[201:25] Speaker 1: And what are the causes?
[201:26] Speaker 1: Likely, people are just alone, right?
[201:28] Speaker 1: They're not hanging out, they're not talking to people.
[201:31] Speaker 1: So what's the outcome of that?
[201:33] Speaker 1: A lot of shy people, right?
[201:35] Speaker 1: People are afraid of going out, talking to people.
[201:39] Speaker 1: So I've noticed this problem myself, right?
[201:41] Speaker 1: After COVID especially, and so I decided to work on my social skills.

--- Segment 21 (Starting at 200:00) ---

[200:00] Speaker 1: Dios.
[200:01] Speaker 1: Um, so I found a online course, a social skills course, and honestly, it wasn't that good, right?
[200:13] Speaker 1: Um, I talked to people who were there for a long time and they didn't seem to be getting a whole lot out of it.
[200:20] Speaker 1: I talked to someone who's there for about a year and they're still taking the course.
[200:25] Speaker 1: And uh, I I don't know.
[200:27] Speaker 1: It didn't seem to be working for them.
[200:30] Speaker 1: And um, the problem that I found was that they're using a lot of information, they're giving you scripts on what to say, and they're not enough give not spending enough time actually socializing.
[200:46] Speaker 1: There's only about 15 minutes of a of a a Zoom lesson at the end where you actually end up talking to people.
[200:55] Speaker 1: So my best understanding is that the circuitry is already in your brain to learn how to socialize.
[201:02] Speaker 1: And so, the best way to learn how to socialize and learn how to talk to people is just get the reps in and talk to people.
[201:11] Speaker 1: So that's how I came up with Shy Chat.
[201:15] Speaker 1: The idea here is, I want people to be able to build conversation skills naturally.
[201:24] Speaker 1: All right?
[201:25] Speaker 1: So, my app provides an easy, low stakes way to learn socialization and conversational skills.
[201:36] Speaker 1: It uses Chat Rulette style matching to allow you to talk to as many people as possible and allow you to get your reps in.
[201:46] Speaker 1: So you get more exposure and decrease your social anxiety over time.
[201:54] Speaker 1: And we have the added benefit of um, the scripts from before, the information, right?
[202:01] Speaker 1: So you have the best of both worlds.
[202:03] Speaker 1: You can talk to people with essentially uh a guy in your ear guiding you if necessary.
[202:11] Speaker 1: And this is done via tool tip and it'll give you conversational cues based on uh the sh- of the call transcript.
[202:22] Speaker 1: So we're giving the transcript to AI and then we're getting uh information about what you might be able to say next, guiding you throughout the conversation, giving you essentially training wheels as you talk to people.
[202:39] Speaker 1: So here we have a screenshot of the app.
[202:42] Speaker 1: And you can see uh conversational tips uh that are shown up for this conversation.
[202:51] Speaker 1: You have uh um, what are you up to for the rest of the day?
[202:58] Speaker 1: What is it about bicycling that you like?
[203:02] Speaker 1: Speaking of bicycling, what's your favorite bike route in Austin?
[203:06] Speaker 1: So obviously the conversational partner was talking about bicycling and uh we have these uh tips available.
[203:15] Speaker 1: And so the idea here is, we go from uh nervous and afraid of talking to people to uh confident, comfortable and eager to engage in uh conversations.
[203:33] Speaker 1: Thank you.
[203:34] Speaker 1: It's been Shy Chat.
[203:39 - 203:49] (Applause)
[203:55 - 204:15] (Silence)
[204:15] Speaker 2: Over 15,000 individuals have joined us live on X. And if you want to apply to be a hiring partner, go to Golet AI.com, and you can also apply to be a challenger for our next cohort.
[204:40] Speaker 2: Up next, we have Tree, who's created a 3D multiplayer game called tensu.io.
[204:50] Speaker 2: Let's welcome Tree.
[204:52 - 205:10] (Applause)
[205:10] Speaker 3: So we have one mic for you, Cornel.
[205:13] Speaker 3: Hold it.
[205:14] Speaker 3: And then face him.
[205:14 - 206:10] (Silence)
[206:10] Speaker 4: Okay, hello everyone.
[206:13] Speaker 4: Uh I realized that there have been a lot of game presentations here and I personally think that they're pretty cool, but if you have gotten tired of seeing them, I apologize because I made another game.
[206:35] Speaker 4: My game is called tensu.io and it's an online multiplayer game where you play as a slime character, a magical slime character.
[206:51] Speaker 4: Originally, I wanted to create an open world game uh because it's always been my dream to create an open world game.
[207:00] Speaker 4: But I realized that with the time frame that we were given, I didn't have enough time to do the project.
[207:08] Speaker 4: It was too ambitious.
[207:10] Speaker 4: So I decided to do an online multiplayer game instead.
[207:15] Speaker 4: I decided to create an IO game because historically, they've been pretty popular and what's made them popular is that they're browser based, which makes them very accessible and uh they have short search session links, meaning that you can play a game without taking up too much of your busy schedule.
[207:45] Speaker 4: And this has led to them uh receiving a lot of traffic over the years.
[207:55] Speaker 4: Before I uh show my demo, I would like to say that tensu.io is available right now.
[208:04] Speaker 4: If you go to tensu.io or if you scan the QR code, it works on both desktop and laptop sorry, desktop and mobile.
[208:14] Speaker 4: So, I'll just give everyone a few seconds to scan it if you want to.
[208:25 - 208:38] (Silence)
[208:38] Speaker 4: Okay.
[208:40] Speaker 4: Let me show the demo now.
[208:46] Speaker 4: So like I said, tensu.io is a a play game where you play as a slime character.
[208:57] Speaker 4: And the objective is you're placed in a world with other players and you have to farm these crystals, those blue things that you see, gain XP to level up, uh battle other players and eventually make it to the top of the leader board on the top right.
[209:20] Speaker 4: Um, this game was made using 3js for the graphics engine, uh Rapier for the physics engine and for ne the network model, I used uh client authenti client authority with service side validation in order to uh have smooth movement and combat.
[209:45] Speaker 4: And as you can see for a browser game, it does pretty well.
[209:53] Speaker 4: And instead of going into the details of my game, I would like to share my opinions about the game in gaming development and AI and how AI can play a role in it.
[210:08] Speaker 4: Game development has been uh historically difficult because it requires expertise in so many different creative uh fields, from music, sound effects, uh 3D modeling and art.
[210:30] Speaker 4: just to name a few and it'd be really difficult for a small indie team to pull all these off by themselves.
[210:40] Speaker 4: But I think that with the recent development in AI tooling, these barriers are becoming lower and lower and eventually I think that almost anyone will be able to turn their ideas into reality.
[210:58] Speaker 4: And I think that uh on like contrary to a lot of people's opinions, I rather than AI hindering uh human creativity, I think that AI will actually enable human creativity because it makes it so much easier for anyone just to express their ideas and that's kind of like my spicy POV, but um I think that we should take advantage of this going forward and be as creative as we can.
[211:44] Speaker 4: Thank you.
[211:45 - 211:48] (Applause)
[211:48 - 212:00] (Silence)
[212:00] Speaker 2: All right, next up we have U and Lamar with their breakthrough coach.
[212:12] Speaker 2: It's a coach that can lead you through personal development and walk you through it using video chat.
[212:27 - 214:46] (Silence)
[214:46] Speaker 5: Okay.
[214:47] Speaker 5: Oh, okay, I got you.
[214:48] Speaker 5: I got you now.
[214:49 - 215:23] (Silence)
[215:23] Speaker 5: It's all the way.
[215:25] Speaker 5: Good?
[215:26] Speaker 5: All right, cool.
[215:28 - 215:37] (Silence)
[215:37 - 215:50] (Music)
[215:50 - 215:56] (Silence)
[215:56 - 216:03] (Music)
[216:03] Speaker 5: All right, it works.
[216:04] Speaker 5: Good.
[216:07] Speaker 6: All right, let's go.
[216:09] Speaker 6: Um, so basically, um everyone here, we all use chat GPT, Claude and other AI chat tools to increase our productivity.
[216:26] Speaker 6: So I did a survey of my friends to ask them, my non-technical friends, to ask them how they use chat GPT.
[216:37] Speaker 6: Because I was curious what they're using it for?
[216:40] Speaker 6: Are you using it to be more productive at work?
[216:44] Speaker 6: Are you using it to, um, shop for groceries or find out things that you should do in life?
[216:57] Speaker 6: And what I found out is a high percentage of my friends were using AI to develop themselves.
[217:07] Speaker 6: They're having chats about relationships, chats about boosting their confidence, chats about eliminating fear and different things like that.
[217:20] Speaker 6: So we decided to create the breakthrough coach.
[217:26] Speaker 6: So the technology to create this has only been uh available for a few weeks.
[217:36] Speaker 6: So as soon as we saw that te

--- Segment 22 (Starting at 210:00) ---

[210:03] Speaker 1: technology, we began creating the breakthrough coach and we created the breakthrough coach and we actually put it on this robot.
[210:19] Speaker 1: Um, so someone could like come up to the robot and chat with the robot, um, with the breakthrough coach.
[210:29] Speaker 1: So, Uzair will tell you guys how he created, uh, how he hacked this robot, because it was really hard to do that, to make that work.
[210:40] Speaker 2: All right, so uh, with the advent of vision LLMs and speech to text, right now if there's any like COVID era telepresence equipment, you can repurpose it to chat with AI.
[211:02] Speaker 2: Um, with the camera, the LLM can see whatever is around it, with audio to speech, that's like a natural interface, and then any other device it has like wheels or hands or whatever can be two calls.
[211:22] Speaker 2: And so the idea of having the breakthrough coach on this was that you can just stand and talk more naturally to it.
[211:30] Speaker 2: It can see you, it can also comment on your posture, if your like body language is good, on top of like you what you're saying.
[211:48] Speaker 1: All right, perfect.
[211:49] Speaker 1: So, basically we did a confidence uh, breakthrough coach session.
[211:58] Speaker 1: Um, so we did a so for this demo, we actually recorded the demo of a session so that it wouldn't take so long so you get the point.
[212:09] Speaker 1: And also because like it's really sensitive to sound because it's um, made for one on one communication.
[212:18] Speaker 1: Um, so let's um read that.
[212:26] Speaker 1: And then um, so cool, so this is an example of a conversation.
[212:36] Speaker 3: Uh, I think the main uh challenge that I'm trying to tackle is I'm doing a presentation.
[212:42] Speaker 3: Um, and I want to make sure that I'm super confident um, during that presentation.
[212:57] Speaker 4: I notice you have a great energy about you and your light green hoodie really makes you stand out.
[213:04] Speaker 4: Now, let's talk about your presentation.
[213:09] Speaker 4: What's the main topic you'll be discussing?
[213:12] Speaker 4: And what specifically makes you feel less confident about it?
[213:17] Speaker 3: Uh, I feel confident.
[213:20] Speaker 3: If you maybe if you could just give me like one tip of how to be confident during the presentation, uh, that'll be great.
[213:31] Speaker 4: One tip that can greatly help is to focus on your message rather than your self-image.
[213:37] Speaker 4: Remember, you're speaking to share information, not to impress with your speaking skills.
[213:44] Speaker 4: What do you think is the most important point you want your audience to take away from your presentation?
[213:51] Speaker 3: Uh, I just want them to see how cool the technology is.
[213:56] Speaker 3: And basically, I think you helped me out really good and um, I think that's perfect.
[214:00] Speaker 3: So thank you.
[214:05] Speaker 4: That's great to hear that I could help.
[214:08] Speaker 4: And I'm glad you're excited about the technology.
[214:14] Speaker 4: To summarize, we discuss building confidence for your presentation and I offered a tip to focus on your message.
[214:25] Speaker 4: As a next step, practice your presentation with that mindset and you'll see a boost in your confidence.
[214:31] Speaker 4: Keep pushing forward.
[214:32] Speaker 4: You've got this and I'm rooting for your success.
[214:36] Speaker 3: All right, thank you.
[214:38] Speaker 1: All right, cool.
[214:39] Speaker 1: So, this breakthrough coach is um, so the technology that we use to do that, um, that that agent can actually see me.
[214:53] Speaker 1: And the first thing it did was comment.
[214:56] Speaker 1: I'm wearing a light green uh, hoodie.
[214:59] Speaker 1: And so the the first thing it did was give me a comment.
[215:02] Speaker 1: says my energy was good and it liked that my light light green hoodie.
[215:09] Speaker 1: So it's more realistic, the avatar looks realistic and it responds in real time.
[215:15] Speaker 1: So it gives people a better interface to chat with rather than just using uh, a chat box.
[215:23] Speaker 1: And so, that's our project and if you guys want to contact us, there we go.
[215:43 - 215:49] (Silence)
[215:49] Speaker 1: Good job.
[216:09] Speaker 5: Now, what if we could invest in virality and videos that creators make?
[216:18] Speaker 5: What if you could support your favorite creators, so they can make more videos?
[216:26] Speaker 5: That's what the next app is trying to do.
[216:30] Speaker 5: This is Hype by Christian Cataneo.
[216:34 - 217:07] (Silence)
[217:12] Speaker 6: Testing.
[217:24 - 217:27] (Silence)
[217:27] Speaker 6: Beautiful.
[217:30] Speaker 6: Can everyone hear me?
[217:33] Speaker 7: Yes.
[217:34] Speaker 6: Amazing.
[217:39] Speaker 6: We made it this far.
[217:41] Speaker 6: Thank you everyone for coming.
[217:46] Speaker 6: Let's give ourselves a round of applause.
[217:56] Speaker 6: So today I will be presenting hype.
[218:05] Speaker 6: For a very long time since basically getting addicted to social media, I've wanted the ability to capitalize on my knowledge of social trends.
[218:26] Speaker 6: And it's been immensely frustrating not being able to instantly invest in a viral video or a creator that I think is going to blow up.
[218:43] Speaker 6: And after speaking with my friends, I learned that this is actually something that other young people are interested in and it also applies to music and sports.
[219:03] Speaker 6: And so, I present to you Hype, which is a way to invest in posts that generate money by promoting products and services for brands.
[219:22] Speaker 6: So that is the basis of the revenue.
[219:29] Speaker 6: And the idea is that you can scroll and micro invest instead of liking.
[219:41] Speaker 6: Now, before Gauntlet, I didn't have any mobile app experience and so this project felt like uh, an immense kind of obstacle ahead of me and I didn't know where to begin, but I was able to build this out in the past two weeks.
[220:00] Speaker 6: And it's got some kinks left before it can be, you know, on the app store or whatever.
[220:05] Speaker 6: But you can invest and you can short and even the comments are investible.
[220:19] Speaker 6: And so you can also invest in creators because you can represent accounts as ETFs of the monetized content.
[220:40] Speaker 6: And then when you invest in an account, you get shares of all of their future posts.
[220:47] Speaker 6: So this way you can track their success.
[220:50] Speaker 6: Now, personally, I've found YouTubers and TikTokers and Instagram, um, influencers or whatever, um, when they had a very low follower count.
[221:02] Speaker 6: And so this is a way that we can capitalize on our knowledge of social trends.
[221:11] Speaker 6: And there's different ways that revenue can be generated, whether creators are making an individual post to promote something or they're replying and getting filtered by follower accounts and maybe they have partnerships with brands.
[221:39] Speaker 6: And so any content that they make is uh, actually monetizable and they're getting paid.
[221:46] Speaker 6: And this way, users, consumers, investors can uh, capitalize off of the growth of emerging creators.
[221:58] Speaker 6: And there's a social status side of it, where since you can rank top creators, um, already successful influencers might be drawn to the application.
[222:13] Speaker 6: And you can also track who knows the most about social trends and maybe copy the portfolios of uh, YouTubers, for example, that know who's on the come up or whatever that may be.
[222:44] Speaker 6: And you can see your uh, your investments here.
[222:48] Speaker 6: You can also see who your tribe is, which are the people that you invest in that also invest in you.
[223:01] Speaker 6: Um, and yeah, it's basically a combination of Robin Hood and Instagram.
[223:11] Speaker 6: So thank you so much for listening and um, Gauntlet helped me create this, so thank you.
[223:20 - 223:30] (Silence)
[223:30] Speaker 5: Now, if any of you are like me, you've thought of a trade that you want to do and you just wish you could just sort of make the transaction right away and make the trade.
[223:43] Speaker 5: Well, K just built that and it's called Yolo.
[223:47] Speaker 5: This is K with his app for automatic agent-based trading.
[223:48 - 224:00] (Silence)
[224:00] Speaker 8: Alright, K.
[224:02] Speaker 8: Oh, used that one.
[224:03 - 226:12] (Silence)
[226:12] Speaker 9: All right.
[226:14] Speaker 9: Hey everyone, my name is K and I have a question for you.
[226:19] Speaker 9: Have you ever had the idea that something is definitely going to happen in the markets, but you did not know how to profit off that idea?
[226:28] Speaker 9: You got to take a position, long, shorts, you got to buy an option, calls, puts, delta,

--- Segment 23 (Starting at 220:00) ---

[220:00] Speaker 1: all does, yuck.
[220:02] Speaker 1: Now, in the age of AI, you don't have to do any of that.
[220:09] Speaker 1: Introducing YOLO, your AI investment buddy.
[220:15] Speaker 1: With YOLO, you tell the AI what you think is going to happen in natural language.
[220:23] Speaker 1: It will go through all the information I'm going to describe in a minute, find the optimal trade for you, and then execute it on your behalf.
[220:35] Speaker 1: So here is the app itself, where you can see the portfolio, all the positions I'm currently in.
[220:45] Speaker 1: And let's say I think for some reason or other that inflation is going to be really high in the next quarter.
[220:55] Speaker 1: I only tell the AI that inflation is going to be high and it's going to do the rest.
[221:07 - 221:12] (Silence)
[221:12] Speaker 1: Okay, one sec.
[221:16 - 221:33] (Silence)
[221:33] Speaker 1: headphones.
[221:37] Speaker 1: it doesn't work.
[221:39 - 221:48] (Silence)
[221:48] Speaker 1: Oh, yeah.
[221:51 - 222:00] (Silence)
[222:00] Speaker 1: I think inflation is going to be higher.
[222:04] Speaker 1: Higher inflation could lead to increased interest rates.
[222:12] Speaker 1: I recommend selling government bonds or investing in commodities or inflation protected securities.
[222:21] Speaker 1: A confirmation dialogue will appear for the recommended trade.
[222:28] Speaker 1: Analyzing now.
[222:30] Speaker 1: Please confirm the trade to buy inflation protected securities.
[222:40] Speaker 1: Trade executed.
[222:44 - 222:51] (Silence)
[222:51] Speaker 1: And that is literally it.
[222:54] Speaker 1: You tell the AI what you think is going to happen and the AI will automatically prepare the trade for you and then execute it.
[223:01] Speaker 1: So what is going on in the background?
[223:03] Speaker 1: What is the ontology of the app if you will?
[223:07] Speaker 1: Well, for one thing, when you talk in natural language to AI, it will use regular expressions to find what you want.
[223:16] Speaker 1: And then it will go through OpenBB, which is an open source Bloomberg terminal.
[223:21] Speaker 1: It will go through Yahoo News and other financial APIs and then it will use the Alpaca API to actually execute the trade for you.
[223:31] Speaker 1: So if we continue, you will see right now that we are actually in a long position on TIP, which is inflation protected securities.
[223:44] Speaker 1: And if we go to Alpaca and refresh, we will see that it has automatically executed the trade and taken a position on TIP.
[223:56] Speaker 1: Okay, so that's good enough.
[223:59] Speaker 1: But let's say we do not know what's going to happen in the market and we want a recommendation from the AI itself.
[224:06] Speaker 1: Well, the AI will go through the information that I just described again and then it will recommend a stock or a position for you to be in.
[224:15] Speaker 1: So in this case, the AI is recommending for us to buy Apple and you can swipe literally like Tinder, whether you want to be in a position or not.
[224:26] Speaker 1: So, in this case, we're not going to be in Apple and Tesla it says hold, I don't know why.
[224:35] Speaker 1: But we are going to actually buy Tesla.
[224:41] Speaker 1: Okay, that's good enough.
[224:44] Speaker 1: But let's say I wanted to do something even more complex.
[224:47] Speaker 1: Let's say I want to use a trading platform that does not have an API or I want some financial research to be done and I want a report.
[224:57] Speaker 1: Well, in this case, we've given YOLO its own virtual machine running Ubuntu and we've used Open AI computer use agent to execute whatever you like in a fully functional computer.
[225:15] Speaker 1: So in this case, we're just simply telling it to click on Firefox, enter Alpaca markets and retrieve the state of the market and prepare a financial report.
[225:26] Speaker 1: The AI will automatically do that autonomously, you don't have to sit through it and then it will come up with a report and then you can take a position in that trade.
[225:36] Speaker 1: Thank you.
[225:39 - 225:51] (Applause)
[225:52 - 226:14] (Silence)
[226:14] Speaker 2: Now, all of us love consuming content.
[226:19] Speaker 2: But what if AI could curate the best content and make it available to you on demand?
[226:28] Speaker 2: That's exactly what MineSurf is doing.
[226:31] Speaker 2: This is Silas with MineSurf.
[226:34 - 226:38] (Applause)
[226:39 - 226:43] (Silence)
[226:44 - 226:57] (Silence)
[226:57] Speaker 3: All right, test test.
[226:59] Speaker 3: Can you hear me?
[227:01] Speaker 3: All right.
[227:03] Speaker 3: So I'm here to talk to you about MineSurf today.
[227:06] Speaker 3: My name is Saterry, the creator of MineSurf.
[227:10] Speaker 3: And so let's get down to the problem.
[227:12] Speaker 3: Let's think about how does the internet exist right now today.
[227:17] Speaker 3: And it exists in these silos on each platform across the internet.
[227:23] Speaker 3: And so there's this massive amount of information that we can access, but it's all stiled across different platforms and each company has their own platform that they, you know, host their information on, right?
[227:37] Speaker 3: So MindSurf hopes to fix this by being able to curate from a large a large group of uh different platforms where the information already exists and to curate it in one place in an infinite scrolling feed that's addictive and really easy to access the information that you want to find, right?
[227:59] Speaker 3: So the way it works is you give it just a tidbit of information.
[228:03] Speaker 3: It could be a tweet, a stock ticker, an article or just a simple thought.
[228:10] Speaker 3: MindSurf creates an infinite list of supplemental information based on that idea, right?
[228:16] Speaker 3: And so you can explore that infinite feed based on just your original idea.
[228:22] Speaker 3: So the way the easiest way to think about it is perplexity plus X, right?
[228:27] Speaker 3: Perplexity is really good at giving you granular detailed information that is closely related to your initial query.
[228:36] Speaker 3: And X is really good because it contains a very broad depth of topics, right?
[228:42] Speaker 3: But neither one of them fully allow you to investigate ideas the way that you want them to, right?
[228:53] Speaker 3: So MindServe hopes to fix this by providing both the breadth and the depth of information on a topic.
[229:01] Speaker 3: So how's it work?
[229:02] Speaker 3: It creates an agent in the background that curates the feed and the agent has access to a subset of tools, whether it's Wikipedia, Archive, Medpub, scientific journals, Reddit, social media posts, substack, Twitter.
[229:22] Speaker 3: And so it find the agent decides in the background what to append to the feed based on your feedback, what it knows about you and what information it's already shown you.
[229:35] Speaker 3: So we can hop into a quick demo of the UI really quick.
[229:38] Speaker 3: And so the entire idea of the UI was that it's this infinitely scrolling feed where you can just swipe, right?
[229:46] Speaker 3: But I've also built it so that it allows power users to be able to navigate the application quickly.
[229:53] Speaker 3: And so everything can be navigated from the arrow keys.
[229:56] Speaker 3: If you want to jump in and see the entire text of a white paper, you hit right click, you know, right arrow key, right?
[230:04] Speaker 3: You hit down on the arrow key, you're back in the feed.
[230:07] Speaker 3: If you want to go to say settings, we can we can change the way that things look, change our, you know, styling and everything, but everything can be navigated from the arrow keys.
[230:17] Speaker 3: So by hitting left on the arrow key, you can pin this, you can copy the contents and we have a number of export options that we hope to integrate based on that.
[230:27] Speaker 3: Along with that, we can also upvote, downvote or regenerate, which gives the agent information that it needs to be able to curate a better feed for you, right?
[230:39] Speaker 3: And so really, MineServe is just a wrapper around an agent that curates the feed for you.
[230:47] Speaker 3: And so it becomes stronger with the tools that we give it, whether it's a second brain like notion or webflowy or obsidian or just your notes app, or whether it's, you know, we want to access Medpub or nature or science direct.
[231:03] Speaker 3: We can curate social feeds if we give it social tools.
[231:06] Speaker 3: And so it really allows this infinitely expandable list of information to be curated just based on what tools we provide it and we can select what preferences we want, whether it's more social based or more technical, right?
[231:21] Speaker 3: And so we have the ability to expand this and constantly refine and tailor what we're showing the user based on what tools we provide the agent.
[231:33] Speaker 3: So, I would say ride the swell, discover MineServe, start with a basic piece of information, just a tidbit and then find this infinite list of information that you can learn from.
[231:47] Speaker 3: So that's it.
[231:48 - 231:52] (Applause)
[231:53 - 231:58] (Silence)
[231:58] Speaker 4: Now, when we're deciding what to watch, most of the time we don't know what to pick.
[232:06] Speaker 4: But what if we could use AI to understand our preferences and psychology and let it decide what to watch.
[232:16] Speaker 4: That's exactly what Tennyson built and it's called any sage.
[232:21 - 232:36] (Silence)

--- Segment 24 (Starting at 230:00) ---

[230:01 - 231:47] (Silence)
[231:47] Speaker 1: Test test, can you hear me?
[231:50] Speaker 1: I don't know. I'm I'm just talking into it. Can anybody hear me out there?
[231:55] Speaker 1: All right, good.
[231:56] Speaker 1: Good.
[231:56] Speaker 1: So, uh, let's get started. My name is Tennyson. I created Annie Sage and uh, I'm just here to present it. So friends, uh Romans, countrymen, lend me your dollars. Uh let's go. This is your personalized anime recommendation engine.
[232:30] Speaker 1: So the problem that we're trying to solve immediately, what to watch?
[232:36] Speaker 1: There's literally a million things out there to watch, uh, anything, even if you narrow it down to just anime, even if you narrow it down to just one streaming service, you're going to have a problem with trying to find what to watch.
[232:54] Speaker 1: I call it analysis paralysis, you call it the paradox of choice, you're going to end up spending more time searching for things to watch than you're actually going to be watching things. Uh, so what do we do about this problem?
[233:12] Speaker 1: Solution, Annie Sage.
[233:17] Speaker 1: Now, steps one, two, three, answer some questions, build your profile, get recommendations.
[233:27] Speaker 1: This is the simple way to go.
[233:30] Speaker 1: So let's actually take a look at it.
[233:45] Speaker 1: So this is going to be pretty quick.
[233:47] Speaker 1: So you see, we're going through some questions here, we're going to be picking things that are oblique, not just asking you like, hey, what genre do you want to watch, or what kind of things have you seen before?
[233:58] Speaker 1: It's really just going to go on build a profile based on questions that you answer.
[234:06] Speaker 1: So just straight psychological things here.
[234:13] Speaker 1: So it's getting to the end, that was the 10th one and you can see coming up with these really nicely uh done images, posters of everything that you might want to watch that we've figured out.
[234:28] Speaker 1: So good psychological component and then finally a good little component.
[234:32] Speaker 1: You can see the profile there very quickly.
[234:38] Speaker 1: And of course, you can see on the end here, you can watch the trailer, which we're not going to put in because you get a copyright strike.
[234:53] Speaker 1: So, some key benefits here.
[234:58] Speaker 1: Uh does, as I was saying, psychological profiling without direct questions.
[235:05] Speaker 1: Uh, it will do advanced multi-dimensional recommendation, so it's actually building quite a few things with each one of these uh question steps here.
[235:14] Speaker 1: There's a lot of places this could possibly go.
[235:20] Speaker 1: Uh, there's comprehensive data integration.
[235:23] Speaker 1: I've put together lots of different uh APIs here.
[235:27] Speaker 1: So from My Anime List and Andy List, the movie database, uh the YouTube recommendations here, building all that, putting it together.
[235:41] Speaker 1: And you can find your next favorite anime here.
[235:46] Speaker 1: This is just the beginning of the Sage system.
[235:50] Speaker 1: So we've decided to go with Annie Sage as the very first thing.
[235:56] Speaker 1: Start your journey, experience smarter discovery and join the future.
[236:02] Speaker 1: Thank you.
[236:05 - 236:43] (Silence)
[236:43] Speaker 2: All right, for the next app, the next app is changing how healthcare brands talk with their customers.
[236:54] Speaker 2: The Corbit engine listens, speaks and understands each of its customers with precision.
[237:00] Speaker 2: This is Corbit with Yavuz.
[237:05 - 238:19] (Silence)
[238:19] Speaker 3: All right.
[238:20 - 238:22] (Silence)
[238:22] Speaker 3: All right, Corbit.
[238:25] Speaker 3: So 6.3 trillion uh is spent annually on health and wellness products and personalization is still missing.
[238:45] Speaker 3: It's uh based on guesswork and yeah, messaging is generic and superficial.
[238:57] Speaker 3: So the problem is targeting messages messaging is hard.
[239:03] Speaker 3: Brands rely on big buckets like skin type, age, total spend and it doesn't work.
[239:16] Speaker 3: Customers feel ignored and brands lose loyalty.
[239:25] Speaker 3: So, what if we could fix this?
[239:28] Speaker 3: What if there was a way to give the correct messaging to every customer, every time?
[239:36] Speaker 3: And that's why we're building Corbit engine and so here is how it works.
[239:52] Speaker 3: So this is on an e-commerce site of a brand X. And you put in your email address, you press this and you give a selfie.
[240:46] Speaker 3: And then the engine analyzes my skin, not just dry, like oily, but dives more in depth like visual clues like acnes, wrinkles, redness, those kind of stuff to do a deep analysis also on the location and the environment. And so under the roof we use data from sales, reviews, clinical studies, social media, competitors to create potential customer profiles for each product.
[241:33] Speaker 3: And now I get a detailed profile created for me.
[241:46] Speaker 3: Right here, conditions and location info. And I get recommended products.
[242:04] Speaker 3: I can also click here to schedule a visit with a brand partner and their input will improve our models. And this is the first step and the vision is to build a system that creates a year's marketing on email, SMS, website, crafted scheduled and updated real time for every single customer in minutes.
[242:55] Speaker 3: So imagine this brand is sending me an SMS about a moisturizer, not because it's on sale, but because uh because they see a need in my skin and it's timely like based on the local weather here in Austin right now.
[243:25] Speaker 3: So yeah, we're kicking this off with medical grade skin care brands and it's a highly growing market that craves this.
[243:38] Speaker 3: And even a slight uptick can optic in their lifetime value can make a huge difference.
[243:47] Speaker 3: And this is just the start.
[243:49] Speaker 3: Think cosmetics, pharmaceuticals, nutrition brands, the entire health and wellness market.
[244:02] Speaker 3: And yeah, this is not a marketing tool, we're changing how brands communicate with their customers.
[244:11] Speaker 3: So that's Corbit.
[244:12] Speaker 3: Thank you.
[244:15 - 245:01] (Silence)
[245:01] Speaker 2: Now, all of us have heard of vibe coding, but have we heard of Stark coding?
[245:10] Speaker 2: The next group is Rishab and Paul with special guest Uer and they're going to tell you all about enterprise grade Stark coding.

--- Segment 25 (Starting at 240:00) ---

[240:00 - 240:08] (Silence)
[240:08] Speaker 1: 
HDMI?
[240:14] Speaker 2: Oh yeah, that's HDMI.
[240:26] Speaker 3: Hi guys. My name is Rishab Kumar, and he's my friend Paul, and together we present to you Stark coding.
[240:55] Speaker 2: right here.
[240:56] Speaker 1: Audio I know
[240:57 - 241:01] (Silence)
[241:01] Speaker 4: Headphone headphones?
[241:04] Speaker 5: yeah
[241:05 - 241:08] (Silence)
[241:08] Speaker 6: and up. Ash, help me. You're our only hope.
[241:17] Speaker 6: No, it's.
[241:21] Speaker 7: on the headphones.
[241:22] Speaker 6: Good.
[241:27] Speaker 7: back to the audio device.
[241:36] Speaker 7: Try this.
[241:38] Speaker 6: Black?
[241:41] Speaker 7: Not.
[241:44] Speaker 6: It says Oculus.
[241:50 - 241:52] (Silence)
[241:52] Speaker 6: Okay.
[241:52] Speaker 7: Yeah.
[242:00] Speaker 6: Sorry.
[242:02] Speaker 7: Move your hand.
[242:15] Speaker 6: try.
[242:21] Speaker 7: It's not
[242:25] Speaker 6: Move your hand, please.
[242:30] Speaker 7: There it is.
[242:31] Speaker 6: Okay.
[242:38] Speaker 7: Ready?
[242:43] Speaker 8: Vive coding, everyone knows it. Everyone loves it.
[242:51] Speaker 9: Vive coding gets a bad rap for inefficiency. Slowly writing bad prompts then walking away from the computer while it generates garbage.
[243:04] Speaker 3: We know better than that. We use more than one coding agent at a time. By leveraging voice and hand gestures, we can rapidly speed up the coding process. No keyboard or mouse required. Virtual monitors with a VR headset makes it easy to monitor multitasking. AI is known to generate slop. It's an intern. Never blindly accept your intern's pull request. Reviewing code doesn't have to be purely manual.
Where rules files fall short, dedicated agents step in.
[243:54] Speaker 8: We have created multiple code review agents that analyze code bases for architecture and clean code patterns. These agents review every file for compliance and has been tested on projects with over a million lines of code.
[244:13] Speaker 3: Voice assistance are wonderful for brainstorming sessions. Even Tony Stark had Jarvis. A voice assistant is also a great entry point into calling a team of agents. No one likes tedious work, so why not have agents do the repeatable task? Hey Jarvis, I have an idea for a new project called Stark Coding. Can you check to see if the domain is available?
[244:40] Speaker 10: Yes, it appears the domain is available.
[244:42] Speaker 3: Okay, register it for me.
[244:45] Speaker 10: Right away, sir.
[244:47] Speaker 3: Let's make it a simple static website using HTML, CSS and TypeScript. I expect very little traffic, but make sure it can handle spikes after it goes viral in the USA. Make a new repo for it, set up ISC, use my normal GitHub bot template and get servers set up.
[245:11] Speaker 10: Yes, sir. I shall inform the DevOps engineer and cloud solution architect right away. Would you also like me to have the UX designer to create a placeholder page?
[245:21] Speaker 8: We have an army of agents, 100% coded and deployed by AI.
[245:27] Speaker 3: If your vive code isn't ready for production, sounds like a skill issue. Join Gauntlet AI to vibe with the best.
[245:41 - 245:42] (Silence)
[245:42] Speaker 8: Thank you
[245:42] Speaker 9: So this entire gauntlet, I have found ways to procrastinate doing my homework by doing probably 20 times as much work, creating agents to do my homework for me. Stark coding is trying to get it to a Microsoft level enterprise where I am can't tell a difference between, did I code it? Did I set up this project or did it?
So, my actual standard was sending all most of my projects snippets to my old coworkers and saying, hey, one of these was written by me, one is AI, which one is it? And they couldn't tell the difference.
[246:41] Speaker 10: I worked at Microsoft.
[246:48] Speaker 9: Thank you.
[247:10] Speaker 11: All right. Last but not least, the next presenter is creating Google for your computer. This is Peter with Phathom.
[247:32] Speaker 12: All right. Do both of these mics work? Okay. Let's talk about your computer. If you're like me, the first thing you probably open is a web browser because so much of our lives happens online, right? Whether it's email or streaming or Twitter, flamewars. But at the same time hard drives are getting bigger. Like Apple will happily sell you a 1, 2, 4, 8 terabyte hard drive. So people are definitely using it for something, right? They're not cheap, but people are using it. And it turns out that actually a lot of our lives still happens offline, still happens on our computer, whether you're a privacy advocate or work with sensitive information in industry or, you know, government intelligence or where you just have to be off like sometimes completely, you know, airgapped from the world. So a lot of our lives still happens in the computer.
But with that scale, like our ways of searching it is still pretty primitive. It hasn't really changed for the past 25 or 30 years. You say you're looking for cats, you have to type C and A and T and hope that you had everything everything you have actually has that exact string in it. It's very brittle. You can't talk about leopards or Jaguars or Puma, right? So, it's very narrow. It's like ironically, our local computer is like the last frontier. It's sitting right in front of us, but we can't really search it, right? And AI kind of makes this easier. Open AI just came out with a file search API that lets you kind of um do not just string matching but semantic search, right? What what's the meaning inside the text?
Um but it's still early. It costs like a 10 cents per gig a day and that works if you have a small project, but if you have like say even a 1 terabyte hard drive, that's 3,000 bucks a month. That's more than what I paid for an apartment in San Francisco and that's just storage. That's no embedding, that's no vectorizing, that's no searching. That's just holding the index. So there's an opportunity here. There's how can we leverage that technology for the scale of a local disc. And that's where fandom comes in. The idea is basically just a search engine for your own computer. It's fast because it's local and if you wear a tin foil hat, it's completely offline. So, it's pretty great. I mean, the secret sauce is pretty simple. It's the files are just already on your computer like in the sense that the files are in the computer. You don't have to store it again. It's already stored. So it's it's free. Um but

--- Segment 26 (Starting at 250:00) ---

[250:00 - 250:11] (Silence)
[250:11] Speaker 1: but we still have a size problem.
[250:14] Speaker 1: a terabyte's still a lot and we don't want to index all of that right away, right?
[250:22] Speaker 1: we don't want a table of contents to be as long as the rest of the book, right?
[250:31] Speaker 1: So we have to be smart.
[250:33] Speaker 1: We have to be smart about how we search.
[250:37] Speaker 1: We have to maybe have a a coarse way of covering the whole breath of something before we kind of drill down and that's, you know, when you have software engineers or mathematicians, it that sounds like a hierarchy in terms of searching, right?
[250:53] Speaker 1: And we have tools out of the box for that.
[250:57] Speaker 1: Mathwords, K means clustering, dominant set approximation, math.
[251:03] Speaker 1: Um, but here's kind of a brief dive into it.
[251:08] Speaker 1: You start with your massive amount of original text, whether it's PDFs, documents, uh or even non-text that we can turn into text and kind of cull uh the meaning out of.
[251:18] Speaker 1: And we break it up into a millions of pieces.
[251:22] Speaker 1: Sentence fragments, sentences, paragraphs, and then turn that into a numerical representation through language models.
[251:31] Speaker 1: So basically, into just straight up decimal numbers and that represents the meaning of those fragments.
[251:40] Speaker 1: Now, whereas most of the time we just throw all of that into one massive index and just query against it, we're going to do take one extra step.
[251:49] Speaker 1: We're going to take all those fragments and try to lump them up together based on what they mean, like topic and clump them, make these clusters.
[251:59] Speaker 1: And that's what came in clustering does.
[252:02] Speaker 1: And then in each cluster, we find the one fragment inside that is closest to the middle in semantic space, not physical space, but like that kind of contains the gist that is closest to the rest of that clump.
[252:19] Speaker 1: And instead of vectorizing everything and indexing everything, we index only the little middles.
[252:29] Speaker 1: So, how this works when it comes, you have a question about everything on your computer, you type in a query text and it gets fragmented and vectorized like before, but instead of being mashed against everything on your computer, it matches against the most middles and it finds the best middles.
[252:51] Speaker 1: From those middles, we can just look up the original text because it's already sitting on your computer.
[253:00] Speaker 1: The payoff, 2% index footprint.
[253:06] Speaker 1: So, if you have 10 megabytes, 100 megabytes of uh text, that's two mags of index.
[253:16] Speaker 1: It's zero cost to store because it's your computer.
[253:21] Speaker 1: It's zero cost to embed because the language model lives on your computer.
[253:27] Speaker 1: And it's zero to query for the same reason because it's just your computer.
[253:33] Speaker 1: Thanks.
[253:41 - 253:43] (Applause)
[253:43] Speaker 1: Am I done up?
[253:44] Speaker 2: All right.
[253:47 - 254:06] (Silence)
[254:06] Speaker 3: All right, I wanted to take a second to thank a couple of people who made today's event possible.
[254:14] Speaker 3: Starting with our head of ops, Rebecca Meders.
[254:21 - 254:24] (Applause)
[254:24] Speaker 3: And our other
[254:27] Speaker 3: head of Ops, Alicia DeMatteo.
[254:31 - 254:35] (Applause)
[254:35] Speaker 3: I wanted to thank the AV team, and I want to thank all the challenges for presenting their projects.
[254:43] Speaker 3: Rebecca, a few words.
[254:45 - 254:47] (Applause)
[254:51] Speaker 4: Now, y'all know I don't need a mic, right?
[254:57] Speaker 4: Uh also, I'm not getting on stage, so hopefully everybody can still see me.
[255:01] Speaker 4: Am I can I still be seen?
[255:03] Speaker 4: All right, great.
[255:05] Speaker 4: How do you guys feel?
[255:09 - 255:11] (Audience response)
[255:13] Speaker 4: Hell yeah.
[255:16] Speaker 4: You guys just submitted and it was great.
[255:22] Speaker 4: Y'all did an excellent, excellent job.
[255:25] Speaker 4: I'm so proud.
[255:27] Speaker 4: If you saw my uh X post, since I can't say tweet anymore, earlier today, you know that I feel like this is watching the Eiffel Tower being built from the beam.
[255:42] Speaker 4: Like that is what this feels like and you guys are the architects of all of this.
[255:46] Speaker 4: So I am incredibly proud to be a part of this organization.
[255:51] Speaker 4: Let's do a little bit of housekeeping.
[255:53] Speaker 4: All right.
[255:55] Speaker 4: Gauntlet staff, uh challenges, you guys kind of got marching orders from me, ask the people around you, so I don't have to say that out loud.
[256:04] Speaker 4: For the guests, we do have a great.
[256:07 - 256:12] (Inaudible)
[256:13] Speaker 5: You are so right.
[256:17] Speaker 4: Introducing the Gauntlet Champions.
[256:25 - 256:27] (Applause)
[256:30] Speaker 4: You're absolutely right.
[256:32] Speaker 4: So my champions, you got marching orders earlier today or earlier about 10 minutes ago about what happens next.
[256:41] Speaker 4: For our guests, we are going to serve you a great dinner.
[256:45] Speaker 4: Uh we like you to stay, talk to all of the challenges, just enjoy the celebration with us, but we want to get the tables a little bit more conversational.
[256:57] Speaker 4: So if you guys could just allow us a few minutes before we release dinner so we can do that.
[257:02] Speaker 4: Is everybody cool with that?
[257:06] Speaker 4: All right, now I'm going to put you back in the hands of the one, the only, the greatest MC to ever do it, Ash.
[257:14 - 257:17] (Applause)
[257:20] Speaker 6: So, this is 1,200 hours of work for every single champion back there.
[257:28] Speaker 6: So, I want to take a second to introduce the first graduating class of Gauntlet AI Winter 2025, all of the champions in the back over there.
[257:44] Speaker 6: Let's give it up.
[257:45 - 257:51] (Applause)
[257:56] Speaker 6: Each one of you put in well over 1200 hours of work, the the number of uh 24-hour sessions just coding, the amount of projects, the amount of challenges, you guys got through all of it and you are now the first graduating class.
[258:33] Speaker 6: It's important to acknowledge the ontology of projects that we have here at Gauntlet AI.
[258:35 - 258:37] (Applause)
[258:42] Speaker 6: It doesn't matter if it's gaming, it doesn't matter if it's finance, it doesn't matter if it's coding.
[258:47] Speaker 6: We have built product that is going to transform the way we build product over the next decade.
[258:55] Speaker 6: Each one of you has shown the power of AI and the power of picking up AI right now and using it to transform your workflows.
[259:07] Speaker 6: In the future, each one of you is going to become the individual that is going to take AI to the next level and bring coding its new workflow.
[259:22] Speaker 6: So, once again, I want to thank everybody who joined us today.
[259:26] Speaker 6: Over 15,000 people on X saw our live stream, over 15,000 people saw all of your projects.
[259:40] Speaker 6: If you haven't gone to demoday.gauntletai.com, please go and see all the projects, see all the challenges.
[259:51] Speaker 6: If you want to be a challenger, go to gauntletai.com and click apply now.
[259:59] Speaker 6: And if you want to hire these amazing graduates, click become a hiring partner on Gauntlet AI right now.
[260:14] Speaker 6: Thank you everybody for joining us.
[260:17] Speaker 6: Thank you Austin, thank you Rebecca, thank you Alicia, and thank you to the champions for making this the day that it was.
[260:26] Speaker 6: Thank you.
[260:28 - 260:30] (Applause)
[260:33] Speaker 7: I totally wigged that.
[260:36] Speaker 7: I got the feels like, oh my god, it's so worth it.
[260:39] Speaker 7: Gonna miss Ash, going to miss Rebecca.
[260:42] Speaker 7: My feels is coming in.
[260:44] Speaker 7: I can't handle it.
[260:46] Speaker 8: That was so good.
[260:47 - 261:00] (Audience sounds)